# AURA: IncorporaciÃ³n de deliberaciÃ³n cognitiva en la semÃ¡ntica de lenguajes de programaciÃ³n

## Un reporte tÃ©cnico sobre Agent-Unified Runtime Architecture

---

**Resumen.** Presentamos AURA (Agent-Unified Runtime Architecture), un lenguaje de programaciÃ³n cuya mÃ¡quina virtual redefine quÃ© significa ejecutar un programa: en AURA, un programa no es una funciÃ³n que mapea entradas a salidas, sino la especificaciÃ³n de un *espacio de trayectorias de ejecuciÃ³n vÃ¡lidas*, restringido por goals e invariantes declarados por el desarrollador, donde un agente cognitivo (respaldado por un LLM) actÃºa como orÃ¡culo que selecciona trayectorias viables cuando la ejecuciÃ³n determinista no puede continuar. A diferencia de los enfoques existentes donde los LLMs operan como herramientas externas de generaciÃ³n de cÃ³digo (GitHub Copilot, ChatRepair), como orquestadores de agentes (LangChain, DSPy), o donde la auto-reparaciÃ³n opera a nivel de sistemas (MAPE-K, Rainbow), AURA incorpora la deliberaciÃ³n cognitiva directamente en su semÃ¡ntica operacional: la VM puede suspender la evaluaciÃ³n en puntos arbitrarios, reificar el contexto completo de ejecuciÃ³n (variables, goals, invariantes, historial de observaciones, checkpoints), despachar a un LLM para deliberaciÃ³n, y resumir con una de cinco intervenciones estructuralmente tipadas---continuar, inyecciÃ³n de valor, parcheo de cÃ³digo, backtracking basado en checkpoints con ajustes, o detenciÃ³n. El backtracking con ajustes es la intervenciÃ³n central: convierte al programa en un *grafo navegable de estados* donde la ejecuciÃ³n puede retroceder y explorar trayectorias alternativas, acercando AURA a la semÃ¡ntica de bÃºsqueda de Prolog, los handlers de efectos algebraicos, y el sistema de condiciones/restarts de Common Lisp---pero con un orÃ¡culo generativo en lugar de handlers estÃ¡ticos. Hasta donde sabemos, AURA es el primer lenguaje de programaciÃ³n donde (1) la deliberaciÃ³n cognitiva es parte de la semÃ¡ntica operacional, no una capa externa, (2) la ejecuciÃ³n se modela como selecciÃ³n de trayectoria en un espacio restringido por intenciones declaradas, y (3) un LLM participa como componente del runtime que selecciona entre continuaciones semÃ¡nticamente vÃ¡lidas, no como herramienta heurÃ­stica arbitraria.

---

### Abreviaturas

| Abreviatura | Significado |
|---|---|
| 2APL | 2nd Agent Programming Language |
| ACT-R | Adaptive Control of Thoughtâ€”Rational |
| AOPL | Agent-Oriented Programming Language |
| APR | Automatic Program Repair |
| AST | Abstract Syntax Tree |
| AURA | Agent-Unified Runtime Architecture |
| BDI | Belief-Desire-Intention |
| CLOS MOP | Common Lisp Object System Meta-Object Protocol |
| CTL | Computation Tree Logic |
| DSL | Domain-Specific Language |
| FORMS | Formal Reference Model for Self-adaptation |
| HTN | Hierarchical Task Network |
| IoT | Internet of Things |
| LIDA | Learning Intelligent Distribution Agent |
| LLM | Large Language Model |
| LMQL | Language Model Query Language |
| LTL | Linear Temporal Logic |
| MAPE-K | Monitor-Analyze-Plan-Execute over Knowledge |
| RSS | Resident Set Size |
| SARL | SARL Agent-Oriented Programming Language |
| Soar | State, Operator, And Result |
| STM | Software Transactional Memory |
| VM | Virtual Machine |

---

## 1. IntroducciÃ³n

### 1.1 La brecha paradigmÃ¡tica

Tres comunidades de investigaciÃ³n han desarrollado independientemente soluciones al problema de construir software que se adapte a condiciones inesperadas:

**ProgramaciÃ³n orientada a agentes** (Shoham 1993; Rao 1996; Bordini et al. 2007) introdujo actitudes mentales---creencias, deseos, intenciones---como primitivas de programaciÃ³n. Lenguajes como AgentSpeak/Jason, GOAL y 2APL implementan la arquitectura BDI (Belief-Desire-Intention) con razonamiento explÃ­cito sobre objetivos y manejo de fallos en planes. Sin embargo, estos lenguajes son anteriores a la era de los LLM: su "razonamiento" es bÃºsqueda en biblioteca de planes, no deliberaciÃ³n abierta.

**ReparaciÃ³n automÃ¡tica de programas** (Le Goues et al. 2012; Xia & Zhang 2023; Long & Rinard 2016) desarrollÃ³ tÃ©cnicas para corregir errores automÃ¡ticamente, desde reparaciÃ³n basada en bÃºsqueda (GenProg) hasta reparaciÃ³n conversacional con LLM (ChatRepair). Estos sistemas logran resultados impresionantes en benchmarks, pero todos operan *post-mortem*: el programa primero debe fallar, producir un fallo de test o mensaje de error, y luego una herramienta externa propone un parche. Ninguna herramienta APR tiene acceso al estado de ejecuciÃ³n en vivo.

**Sistemas auto-adaptativos** (Kephart & Chess 2003; Garlan et al. 2004; Weyns et al. 2012) formalizaron el ciclo MAPE-K (Monitorear-Analizar-Planificar-Ejecutar sobre Conocimiento compartido) para computaciÃ³n autÃ³noma. Sistemas como Rainbow detectan violaciones de restricciones arquitectÃ³nicas y aplican estrategias de reparaciÃ³n predefinidas. Estos operan a nivel de infraestructura, no a nivel de lenguaje de programaciÃ³n.

A pesar de dÃ©cadas de progreso en cada comunidad, persiste una brecha fundamental: **ningÃºn lenguaje de programaciÃ³n existente integra deliberaciÃ³n cognitiva---la capacidad de pausar la ejecuciÃ³n, razonar sobre el estado actual contra intenciones declaradas, y elegir entre intervenciones estructuralmente diversas---en su semÃ¡ntica de ejecuciÃ³n.**

### 1.2 La sÃ­ntesis

AURA cierra esta brecha sintetizando ideas de las tres tradiciones en un solo diseÃ±o de lenguaje:

| Origen | Concepto | RealizaciÃ³n en AURA |
|--------|---------|------------------|
| Arquitecturas BDI | Goals como actitudes mentales de primera clase | `goal "descripciÃ³n" check expr` --- goals con expresiones de verificaciÃ³n evaluadas en runtime |
| VerificaciÃ³n en runtime | Monitoreo continuo de propiedades | `observe variable` --- declara puntos de monitoreo en runtime |
| DiseÃ±o por contrato | Precondiciones e invariantes | `invariant expr` --- restricciones que acotan todas las adaptaciones |
| Ciclo MAPE-K | Ciclo Monitorear-Analizar-Planificar-Ejecutar | `observe` -> `deliberate()` -> `CognitiveDecision` -> aplicar |
| Checkpoint/rollback | GestiÃ³n transaccional de estado | `CheckpointManager` --- snapshots nombrados con restauraciÃ³n y ajustes |
| Frameworks de agentes LLM | Razonamiento potenciado por LLM | `reason "pregunta"` --- deliberaciÃ³n explÃ­cita con inyecciÃ³n de valores |

El resultado es un lenguaje donde el modelo de ejecuciÃ³n difiere sustancialmente del modelo convencional:

**Modelo v1 â€” ejecuciÃ³n tradicional:**

```mermaid
graph LR
    A1[parsear] --> B1[ejecutar] --> C1[fallar] --> D1[reparar] --> E1[re-ejecutar]
```

**Modelo v2 â€” ejecuciÃ³n cognitiva:**

```mermaid
graph LR
    A2[parsear] --> B2[ejecutar] --> C2[observar] --> D2[razonar] --> E2[ajustar] --> F2[continuar]
    F2 -.-> C2
```

### 1.3 El salto conceptual

Sin embargo, describir AURA como "un lenguaje con primitivas de agentes" serÃ­a reduccionista. Lo que AURA implementa es algo cualitativamente distinto:

> **AURA no es un lenguaje que permite deliberaciÃ³n. AURA es una mÃ¡quina abstracta donde la continuidad del programa es negociada.**

En un lenguaje convencional, un programa define *el* comportamiento: dada una entrada, la semÃ¡ntica del lenguaje determina una Ãºnica traza de ejecuciÃ³n (o falla). En AURA, un programa define el *espacio permitido* de comportamientos. Los goals y los invariantes restringen ese espacio. La ejecuciÃ³n es entonces **bÃºsqueda guiada en un espacio de trayectorias vÃ¡lidas**, donde el agente cognitivo actÃºa como orÃ¡culo de selecciÃ³n.

Esto cambia de manera fundamental el estatus del programa:

| Aspecto | Lenguaje convencional | AURA |
|---------|----------------------|------|
| El programa define | El comportamiento | El espacio de comportamientos permitidos |
| La ejecuciÃ³n es | EvaluaciÃ³n determinista | BÃºsqueda de trayectoria vÃ¡lida |
| Un error es | Un crash | Un punto de bifurcaciÃ³n |
| El estado es | Una secuencia | Un grafo navegable |
| La continuaciÃ³n es | Determinada por la semÃ¡ntica | Negociada con un orÃ¡culo |

Este reencuadre tiene consecuencias teÃ³ricas importantes. Conecta a AURA no con los agent frameworks (LangChain, DSPy), sino con:

- **Efectos algebraicos** (Plotkin & Pretnar 2009): la deliberaciÃ³n es un efecto cedido a un handler, pero el handler es generativo (LLM) en lugar de estÃ¡tico.
- **Condiciones/restarts de Common Lisp**: el programa seÃ±ala una condiciÃ³n; el orÃ¡culo elige un restart---pero los restarts no estÃ¡n predefinidos sino generados dinÃ¡micamente.
- **Arquitecturas cognitivas tipo Soar** (Laird et al. 1987): un impasse activa sub-goalificaciÃ³n automÃ¡tica. En AURA, un error o expect fallido activa deliberaciÃ³n cognitiva.
- **BÃºsqueda con backtracking de Prolog**: la ejecuciÃ³n explora alternativas cuando un camino falla, pero las alternativas son propuestas por un orÃ¡culo en lugar de enumeradas estÃ¡ticamente.

Este posicionamiento teÃ³rico se desarrolla formalmente en la SecciÃ³n 4.

### 1.4 Contribuciones

Este reporte hace las siguientes afirmaciones, cada una respaldada por evidencia de implementaciÃ³n y posicionada contra la literatura relevada:

1. **EjecuciÃ³n como selecciÃ³n de trayectoria restringida** (SecciÃ³n 4). Proponemos un modelo operacional donde ejecutar un programa no es evaluar una funciÃ³n, sino buscar una trayectoria vÃ¡lida en un espacio de estados restringido por goals e invariantes. El LLM actÃºa como orÃ¡culo de selecciÃ³n, no como herramienta heurÃ­stica. Esta formalizaciÃ³n eleva a AURA de "sistema interesante" a "modelo de programaciÃ³n", conectÃ¡ndolo con semÃ¡nticas operacionales no deterministas, planificaciÃ³n, y model checking.

2. **DeliberaciÃ³n cognitiva como semÃ¡ntica del lenguaje** (SecciÃ³n 3.2). NingÃºn lenguaje existente define la deliberaciÃ³n como una operaciÃ³n semÃ¡ntica que puede modificar el estado de ejecuciÃ³n, reescribir cÃ³digo, o hacer backtrack con ajustes. El trait `CognitiveRuntime` (`observe`, `deliberate`, `check_goals`, `is_active`) es invocado por la VM durante la evaluaciÃ³n de expresiones, no como una capa de monitoreo externa.

3. **Backtracking con ajustes como primitiva semÃ¡ntica** (SecciÃ³n 3.3). La intervenciÃ³n `Backtrack{checkpoint, adjustments}` convierte al programa en un grafo navegable de estados donde la ejecuciÃ³n puede retroceder y explorar trayectorias alternativas. Esto no es una feature---es la semÃ¡ntica. Combina backtracking cronolÃ³gico (Prolog), memoria transaccional (STM), y re-planificaciÃ³n BDI, pero con un orÃ¡culo generativo que propone ajustes.

4. **Goals como expresiones de runtime evaluadas continuamente** (SecciÃ³n 3.1). NingÃºn lenguaje BDI existente trata los goals como expresiones en el lenguaje anfitriÃ³n evaluadas durante la ejecuciÃ³n. El `GoalDef.check: Option<Expr>` de AURA permite monitoreo continuo de goals a granularidad arbitraria, distinto de los Ã¡tomos simbÃ³licos de AgentSpeak, las fÃ³rmulas lÃ³gicas de GOAL, y los maintain goals basados en callbacks de Jadex.

5. **Ãlgebra de intervenciÃ³n de cinco modos** (SecciÃ³n 3.3). El enum `CognitiveDecision` define cinco intervenciones estructuralmente tipadas (`Continue`, `Override(Value)`, `Fix{new_code, explanation}`, `Backtrack{checkpoint, adjustments}`, `Halt(error)`), proporcionando un espacio de intervenciÃ³n mÃ¡s rico que cualquier sistema de auto-reparaciÃ³n existente.

6. **AdaptaciÃ³n acotada por invariantes** (SecciÃ³n 3.4). Los invariantes y goals declarados por el desarrollador restringen todas las modificaciones generadas por el LLM. La funciÃ³n `validate_fix()` verifica que los fixes sean parseables, respeten lÃ­mites de tamaÃ±o, preserven todos los goals declarados, y no introduzcan goals nuevos.

7. **AbstracciÃ³n cognitiva de cero overhead** (SecciÃ³n 3.5). Cuando `is_active()` retorna `false` (el `NullCognitiveRuntime`), todas las verificaciones cognitivas son no-ops. Los programas sin caracterÃ­sticas cognitivas se ejecutan con rendimiento idÃ©ntico al de un runtime no cognitivo.

---

## 2. Trabajo relacionado

### 2.1 Lenguajes de programaciÃ³n orientados a agentes

**AgentSpeak(L)** (Rao 1996) introdujo el modelo de programaciÃ³n BDI dominante: los agentes tienen creencias (hechos tipo Prolog), eventos disparadores activan planes de una biblioteca de planes, y las intenciones son pilas de planes parcialmente ejecutados. **Jason** (Bordini et al. 2007) es la implementaciÃ³n mÃ¡s completa, aÃ±adiendo actos de habla, entornos y abstracciones organizacionales. Los goals en AgentSpeak son Ã¡tomos simbÃ³licos (`!achieve_goal`) que disparan selecciÃ³n de planes; el fallo causa abandono de intenciÃ³n o re-planificaciÃ³n dentro de la biblioteca de planes.

**GOAL** (Hindriks 2009) usa goals declarativos expresados como fÃ³rmulas lÃ³gicas. La base de goals de un agente se actualiza mediante un ciclo de deliberaciÃ³n que evalÃºa goals contra creencias. GOAL es el trabajo previo mÃ¡s cercano al modelo de goals activos de AURA, pero sus goals son fÃ³rmulas lÃ³gicas en un lenguaje de consulta de creencias separado, no expresiones en el lenguaje anfitriÃ³n.

**2APL** (Dastani 2008) introduce *reglas de razonamiento prÃ¡ctico* (PR-rules) que revisan planes cuando las condiciones cambian. Cuando un plan falla, las PR-rules hacen matching con el contexto de fallo y generan planes revisados. Este es el mecanismo de re-planificaciÃ³n mÃ¡s sofisticado en la literatura AOPL, pero opera sobre mapeos regla-plan predefinidos, no deliberaciÃ³n abierta con LLM.

**Jadex** (Pokahr et al. 2005) aÃ±ade *maintain goals* al modelo BDI: condiciones que deben permanecer verdaderas, con re-activaciÃ³n automÃ¡tica de planes cuando se violan. Esto es estructuralmente similar al `goal ... check expr` de AURA, pero las condiciones maintain de Jadex son predicados Java registrados como callbacks, no expresiones en el lenguaje del agente mismo.

**SARL** (Rodriguez et al. 2014) introduce un modelo de capacidad/habilidad donde los agentes declaran capacidades requeridas y vinculan implementaciones en runtime. Esto es arquitectÃ³nicamente similar al sistema de capacidades de AURA (`+http`, `+json`, `+db`).

**La brecha.** NingÃºn lenguaje BDI existente trata los goals como expresiones evaluadas continuamente en el sistema de expresiones del lenguaje anfitriÃ³n. La Tabla 1 resume la distinciÃ³n:

*Tabla 1: RepresentaciÃ³n de goals a travÃ©s de lenguajes orientados a agentes*

| Lenguaje | RepresentaciÃ³n del goal | Momento de evaluaciÃ³n | Respuesta ante fallo |
|----------|-------------------|-------------------|-----------------|
| AgentSpeak | Ãtomo simbÃ³lico (`!g`) | Al dispararse | Abandonar intenciÃ³n |
| GOAL | FÃ³rmula lÃ³gica | Por ciclo de deliberaciÃ³n | Re-seleccionar plan |
| Jadex | Predicado Java (callback) | Al callback | Re-activar plan |
| 2APL | FÃ³rmula lÃ³gica | Por ciclo, PR-rules | RevisiÃ³n basada en reglas |
| **AURA** | **ExpresiÃ³n del lenguaje anfitriÃ³n** | **Continua, por paso** | **DeliberaciÃ³n cognitiva + backtrack** |

### 2.2 ReparaciÃ³n automÃ¡tica de programas

**GenProg** (Le Goues et al. 2012) fue pionero en la reparaciÃ³n automatizada de programas basada en bÃºsqueda usando programaciÃ³n genÃ©tica para evolucionar parches. **SemFix** (Nguyen et al. 2013) y **Angelix** (Mechtaev et al. 2016) introdujeron reparaciÃ³n a nivel semÃ¡ntico usando ejecuciÃ³n simbÃ³lica y resoluciÃ³n de restricciones. **Prophet** (Long & Rinard 2016) aprendiÃ³ modelos de correcciÃ³n de cÃ³digo a partir de parches humanos para rankear candidatos.

La era de los LLM transformÃ³ el campo. **ChatRepair** (Xia & Zhang 2023) usa interacciÃ³n conversacional con LLM para corregir 162/337 bugs de Defects4J a ~$0.42 por bug. **RepairLLaMA** (Silva et al. 2023) hace fine-tuning de LLMs open-source con adaptadores LoRA para reparaciÃ³n. **AlphaRepair** (Xia & Zhang 2022) demostrÃ³ que modelos de cÃ³digo pre-entrenados pueden realizar reparaciÃ³n zero-shot tratando cÃ³digo con errores como un problema de modelo de lenguaje enmascarado.

**La limitaciÃ³n post-mortem.** Todas las herramientas APR---clÃ¡sicas y basadas en LLM---comparten una arquitectura fundamental:

```
[Programa falla] â†’ [Extraer cÃ³digo + error] â†’ [Enviar a herramienta] â†’ [Obtener parche] â†’ [Aplicar] â†’ [Re-ejecutar]
```

Ninguna tiene acceso al estado de ejecuciÃ³n en vivo. Ninguna puede inyectar valores a mitad de ejecuciÃ³n. Ninguna puede hacer backtrack a un checkpoint con ajustes. La herramienta de reparaciÃ³n nunca ve quÃ© variables tenÃ­an quÃ© valores en el momento del fallo, quÃ© goals pretendÃ­a el desarrollador (mÃ¡s allÃ¡ de aserciones de test), o el camino de ejecuciÃ³n que llevÃ³ al error.

### 2.3 Sistemas auto-adaptativos

**ComputaciÃ³n autÃ³noma** (Kephart & Chess 2003) propuso la arquitectura de referencia MAPE-K: Monitorear (recolectar datos vÃ­a sensores), Analizar (determinar si se necesita adaptaciÃ³n), Planificar (seleccionar estrategia), Ejecutar (aplicar vÃ­a efectores), sobre Conocimiento compartido. **Rainbow** (Garlan et al. 2004) implementa MAPE-K a nivel arquitectÃ³nico, monitoreando sistemas en ejecuciÃ³n contra restricciones y aplicando estrategias de reparaciÃ³n predefinidas.

**FORMS** (Weyns et al. 2012) proporciona un modelo de referencia formal para sistemas auto-adaptativos con semÃ¡ntica rigurosa para el sistema gestionado, entorno, goals de adaptaciÃ³n, y ciclo de retroalimentaciÃ³n.

**La limitaciÃ³n de capa externa.** Todas las implementaciones MAPE-K aÃ±aden monitoreo y adaptaciÃ³n como una capa arquitectÃ³nica externa. El sistema gestionado es una caja negra observada a travÃ©s de sondas. Las estrategias de adaptaciÃ³n son configuraciones predefinidas, no modificaciones de cÃ³digo generadas en runtime. La lÃ³gica de adaptaciÃ³n estÃ¡ separada de la lÃ³gica del programa.

### 2.4 Arquitecturas cognitivas

**Soar** (Laird et al. 1987; Newell 1990) implementa un sistema de producciÃ³n con sub-goalificaciÃ³n universal: cuando ninguna producciÃ³n se dispara, un *impasse* activa la creaciÃ³n automÃ¡tica de sub-goals. El mecanismo de *chunking* de Soar aprende nuevas producciones a partir de la resoluciÃ³n de sub-goals, creando un ciclo de aprendizaje. **ACT-R** (Anderson & Lebiere 1998; Anderson et al. 2004) modela la cogniciÃ³n como la interacciÃ³n de buffers modulares (visual, motor, memoria declarativa, buffer de goals) mediados por reglas de producciÃ³n. **CLARION** (Sun 2016) modela explÃ­citamente la interacciÃ³n entre conocimiento implÃ­cito (subsimbÃ³lico) y explÃ­cito (simbÃ³lico). **LIDA** (Franklin et al. 2014) implementa la TeorÃ­a del Espacio de Trabajo Global con un mecanismo de difusiÃ³n similar a la consciencia.

**La relevancia.** El runtime cognitivo de AURA implementa un ciclo que mapea directamente a componentes de arquitecturas cognitivas:

| Componente cognitivo | ImplementaciÃ³n en AURA |
|---|---|
| PercepciÃ³n | `observe()` --- detecciÃ³n de eventos durante la ejecuciÃ³n |
| Memoria de trabajo | Buffer de observaciones + contexto de ejecuciÃ³n actual |
| DeliberaciÃ³n | `deliberate()` --- invocaciÃ³n del LLM con contexto empaquetado |
| DecisiÃ³n | Enum `CognitiveDecision` --- cinco tipos de intervenciÃ³n |
| AcciÃ³n | Hot reload, inyecciÃ³n de valor, restauraciÃ³n de checkpoint |
| Aprendizaje | Traza de `ReasoningEpisode` + persistencia en `HealingMemory` |
| MetacogniciÃ³n | `CognitiveSafetyConfig` --- lÃ­mites de seguridad sobre el comportamiento de razonamiento |

Argumentamos que esto convierte al runtime de AURA en una arquitectura cognitiva en sÃ­ misma, en lugar de un lenguaje usado para *implementar* una arquitectura cognitiva---una distinciÃ³n que, hasta donde sabemos, no tiene precedente directo en la literatura.

### 2.5 Arquitecturas reflectivas y de meta-nivel

**3-Lisp de Smith** (Smith 1984) introdujo la reflexiÃ³n computacional: un programa que puede inspeccionar y modificar su propia ejecuciÃ³n. **CLOS MOP** (Kiczales et al. 1991) proporcionÃ³ un protocolo de meta-objetos que permite a los programas personalizar su propio sistema de clases. **ProgramaciÃ³n orientada a aspectos** (Kiczales et al. 1997) introdujo puntos de uniÃ³n donde preocupaciones transversales pueden interceptar la ejecuciÃ³n.

**Efectos algebraicos** (Plotkin & Pretnar 2009; Bauer & Pretnar 2015) proporcionan el modelo formal mÃ¡s cercano: las computaciones pueden "ceder" efectos a handlers que los inspeccionan y reanudan. El puente cognitivo de AURA puede formalizarse como un handler de efectos algebraicos donde el efecto es "necesito asistencia cognitiva" y el handler es el LLM. La diferencia clave: los handlers de efectos algebraicos se definen estÃ¡ticamente; el "handler" de AURA genera respuestas novedosas dinÃ¡micamente.

**El sistema de condiciones/restarts de Common Lisp** es el precedente clÃ¡sico mÃ¡s cercano a la intervenciÃ³n a mitad de ejecuciÃ³n de AURA. Cuando un error seÃ±ala una condiciÃ³n, los handlers pueden elegir entre restarts predefinidos (ej., `use-value`, `store-value`, `abort`). AURA generaliza esto: en lugar de restarts definidos por el programador, el LLM genera intervenciones novedosas informadas por el contexto de runtime, goals e invariantes.

### 2.6 Sistemas de programaciÃ³n integrados con LLM

**LMQL** (Beurer-Kellner et al. 2023) es la comparaciÃ³n mÃ¡s relevante como lenguaje de programaciÃ³n real (publicado en PLDI) que extiende Python con generaciÃ³n restringida de LLM. LMQL compila a mÃ¡scaras a nivel de token para decodificaciÃ³n restringida. Sin embargo, se enfoca en restricciones en tiempo de generaciÃ³n, no en razonamiento de agentes---no tiene goals, observaciÃ³n, auto-reparaciÃ³n, ni runtime cognitivo.

**DSPy** (Khattab et al. 2023) introduce especificaciones declarativas de programas LLM con optimizaciÃ³n automÃ¡tica de prompts. **SGLang** (Zheng et al. 2024) optimiza la ejecuciÃ³n de programas LLM estructurados con RadixAttention. Ambos estÃ¡n embebidos en Python y se enfocan en la eficiencia de llamadas al LLM, no en adaptaciÃ³n en runtime.

**ReAct** (Yao et al. 2023) y **Reflexion** (Shinn et al. 2023) implementan ciclos observar-razonar-actuar en agentes LLM, pero como patrones de prompt, no como semÃ¡ntica del lenguaje.

*Tabla 2: Sistemas de programaciÃ³n integrados con LLM*

| Sistema | Â¿Es un lenguaje? | Â¿LLM como primitiva? | Â¿Goals? | Â¿Auto-reparaciÃ³n? | Â¿Ciclo en runtime? |
|--------|---------------|-------------------|--------|---------------|---------------|
| LMQL | **SÃ­** | SÃ­ (generaciÃ³n restringida) | No | No | No |
| DSPy | Parcial (DSL en Python) | SÃ­ (signatures) | No | OptimizaciÃ³n de prompt | No |
| SGLang | Parcial (DSL en Python) | SÃ­ (primitivas) | No | No | No |
| LangChain | No (biblioteca) | No (llamada a funciÃ³n) | No | No | No |
| ReAct | No (patrÃ³n de prompt) | SÃ­ (en-prompt) | No | No | SÃ­ (ad hoc) |
| **AURA** | **SÃ­** | **SÃ­** (`reason`) | **SÃ­** (`goal check`) | **SÃ­** (a nivel de lenguaje) | **SÃ­** (integrado en la VM) |

---

## 3. DiseÃ±o e implementaciÃ³n

La siguiente figura muestra la arquitectura general del runtime cognitivo de AURA, desde el cÃ³digo fuente hasta la intervenciÃ³n del LLM:

```mermaid
graph TB
    subgraph "CÃ³digo fuente AURA"
        SRC["goal, observe, expect,<br/>invariant, reason"]
    end

    subgraph "CompilaciÃ³n"
        LEX[Lexer<br/>logos] --> PAR[Parser] --> AST[AST<br/>nodos cognitivos]
    end

    subgraph "MÃ¡quina virtual"
        VM[VM<br/>eval + step_count]
        CP[CheckpointManager<br/>snapshots nombrados]
        OBS[observed_vars<br/>HashSet]
        PF[pending_fixes<br/>Vec]
    end

    subgraph "Runtime cognitivo"
        CR["trait CognitiveRuntime<br/>observe() Â· deliberate()<br/>check_goals() Â· is_active()"]
        NULL[NullCognitiveRuntime<br/>cero overhead]
        AGENT["AgentCognitiveRuntime&lt;P&gt;<br/>buffer Â· traza Â· safety"]
    end

    subgraph "Proveedores LLM"
        MOCK[MockProvider]
        CLAUDE[ClaudeProvider]
        OLLAMA[OllamaProvider]
    end

    SRC --> LEX
    AST --> VM
    VM <--> CR
    VM <--> CP
    VM <--> OBS
    VM <--> PF
    CR --- NULL
    CR --- AGENT
    AGENT <--> MOCK
    AGENT <--> CLAUDE
    AGENT <--> OLLAMA
```

### 3.1 Primitivas cognitivas

AURA introduce seis construcciones que forman su vocabulario cognitivo. Estas se parsean en nodos AST---son parte de la gramÃ¡tica del lenguaje, no funciones de biblioteca. En la SecciÃ³n 4.10 demostramos que estas primitivas no son features independientes sino **consecuencias necesarias** del modelo de ejecuciÃ³n: cada una corresponde a un componente formal sin el cual el modelo degenera a algo que ya existe.

#### 3.1.1 `goal`

```
goal "procesar datos de usuario correctamente"
goal "todos los usuarios deben tener nombres vÃ¡lidos" check usuarios != nil
```

Los goals son declaraciones de nivel superior (`Definition::Goal(GoalDef)`) con una expresiÃ³n `check` opcional. La estructura `GoalDef`:

```rust
pub struct GoalDef {
    pub description: String,
    pub check: Option<Expr>,  // El elemento novedoso
    pub span: Span,
}
```

Cuando `check` estÃ¡ presente, el goal es *activo*: la VM evalÃºa la expresiÃ³n check despuÃ©s de cambios en variables observadas, despuÃ©s de retornos de funciones, y en intervalos de pasos configurables. Si la verificaciÃ³n evalÃºa a falso, se eleva un `DeliberationTrigger::GoalMisalignment`, invocando el runtime cognitivo.

La palabra clave `check` se parsea como un *soft keyword* (`Ident("check")`), no como un token reservado---preservando compatibilidad hacia atrÃ¡s con programas que usan "check" como identificador.

#### 3.1.2 `observe`

```
observe usuarios
observe respuesta.estado
observe datos where valido == true
```

`observe` declara un punto de monitoreo en runtime (`Expr::Observe`). Cuando una variable observada cambia de valor, la VM:
1. Crea un checkpoint implÃ­cito (vÃ­a `CheckpointManager`)
2. Notifica al runtime cognitivo vÃ­a `observe(ObservationEvent::ValueChanged{...})`
3. Dispara la evaluaciÃ³n de goals activos

Sin un runtime cognitivo, `observe` es un no-op que retorna nil.

#### 3.1.3 `expect`

```
expect len(usuarios) > 0 "deberÃ­a haber usuarios"
```

`expect` es verificaciÃ³n de intenciÃ³n (`Expr::Expect`). A diferencia de las aserciones que crashean ante un fallo, los expects se registran como `ExpectationFailure` y, cuando un runtime cognitivo estÃ¡ activo, disparan `DeliberationTrigger::ExpectFailed`. El runtime puede entonces decidir continuar, sobreescribir el resultado, generar un fix, o hacer backtrack.

#### 3.1.4 `invariant`

```
invariant len(usuarios) > 0
```

Los invariantes (`Definition::Invariant(Expr)`) declaran restricciones que ninguna adaptaciÃ³n puede violar. Sirven como la frontera de seguridad del desarrollador: la funciÃ³n `validate_fix()` verifica que los fixes propuestos por el LLM no rompan invariantes antes de ser aplicados.

#### 3.1.5 `reason`

```
estrategia = reason "tenemos {len(usuarios)} usuarios, procesamos todos o filtramos?"
```

`reason` es un punto de deliberaciÃ³n explÃ­cito (`Expr::Reason`). La ejecuciÃ³n pausa, la pregunta y las observaciones recientes se envÃ­an al runtime cognitivo, y la decisiÃ³n del LLM se convierte en el valor de la expresiÃ³n. Esto permite *inyecciÃ³n de valores*: el LLM puede retornar un valor que se vincula a una variable y se usa en la computaciÃ³n subsiguiente.

Sin un runtime cognitivo, `reason` retorna nil.

#### 3.1.6 `@self_heal`

```
@self_heal(max_attempts: 5, mode: "semantic")
procesar_datos(datos) = { ... }
```

AnotaciÃ³n a nivel de funciÃ³n (`SelfHealConfig`) que marca funciones individuales para reparaciÃ³n automÃ¡tica. Configurable con `max_attempts` y `mode` (technical, semantic, auto).

### 3.2 El trait CognitiveRuntime

El trait `CognitiveRuntime` define la interfaz entre la VM y el agente cognitivo:

```rust
pub trait CognitiveRuntime: Send {
    fn observe(&mut self, event: ObservationEvent);
    fn deliberate(&mut self, trigger: DeliberationTrigger) -> CognitiveDecision;
    fn check_goals(&mut self) -> Vec<CognitiveDecision>;
    fn is_active(&self) -> bool;
    fn set_available_checkpoints(&mut self, checkpoints: Vec<String>) {}
}
```

**Eventos de observaciÃ³n** (`ObservationEvent`) incluyen `ValueChanged`, `ExpectEvaluated`, `FunctionReturned`, y `CheckpointCreated`. Estos proporcionan al LLM contexto de runtime rico que ninguna herramienta de reparaciÃ³n post-mortem puede acceder.

**Disparadores de deliberaciÃ³n** (`DeliberationTrigger`) clasifican quÃ© provocÃ³ la deliberaciÃ³n: `ExpectFailed`, `ExplicitReason`, `TechnicalError`, o `GoalMisalignment`. Esta clasificaciÃ³n ayuda al LLM a entender la naturaleza del problema.

El `NullCognitiveRuntime` implementa todas las operaciones como no-ops con `is_active() = false`, proporcionando cero overhead para ejecuciÃ³n no cognitiva.

El siguiente diagrama muestra cÃ³mo la VM interactÃºa con el trait durante la evaluaciÃ³n de expresiones:

```mermaid
sequenceDiagram
    participant VM as VM (eval)
    participant CR as CognitiveRuntime
    participant LLM as Proveedor LLM

    VM->>VM: eval(Expr::Let) â€” variable observada
    VM->>CR: observe(ValueChanged)
    VM->>CR: check_goals()
    CR-->>VM: [Continue]

    VM->>VM: eval(Expr::Expect) â€” falla
    VM->>CR: observe(ExpectEvaluated)
    VM->>CR: deliberate(ExpectFailed)
    CR->>LLM: request con contexto completo
    LLM-->>CR: respuesta
    CR-->>VM: Override(valor) | Fix{...} | Backtrack{...}

    VM->>VM: eval(Expr::Reason)
    VM->>CR: deliberate(ExplicitReason)
    CR->>LLM: pregunta + observaciones + goals
    LLM-->>CR: decisiÃ³n
    CR-->>VM: Override(valor_inyectado)
```

### 3.3 El Ã¡lgebra de intervenciÃ³n de cinco modos

`CognitiveDecision` define cinco intervenciones estructuralmente tipadas:

```rust
pub enum CognitiveDecision {
    Continue,
    Override(Value),
    Fix { new_code: String, explanation: String },
    Backtrack { checkpoint: String, adjustments: Vec<(String, Value)> },
    Halt(RuntimeError),
}
```

```mermaid
graph TD
    T[Disparador de deliberaciÃ³n] --> LLM[LLM delibera]
    LLM --> C{DecisiÃ³n}

    C -->|"sin cambios"| CONT[Continue<br/>La ejecuciÃ³n sigue normalmente]
    C -->|"inyectar valor"| OVR["Override(Value)<br/>Reemplaza el resultado de la expresiÃ³n<br/>con un valor elegido por el LLM"]
    C -->|"parchear cÃ³digo"| FIX["Fix{new_code, explanation}<br/>Reescribe el fuente y re-ejecuta<br/>desde el inicio"]
    C -->|"retroceder"| BT["Backtrack{checkpoint, adjustments}<br/>Restaura la VM al checkpoint nombrado<br/>con ajustes en variables"]
    C -->|"detener"| HALT["Halt(error)<br/>Detiene la ejecuciÃ³n<br/>con explicaciÃ³n"]

    style CONT fill:#d4edda
    style OVR fill:#cce5ff
    style FIX fill:#fff3cd
    style BT fill:#f8d7da
    style HALT fill:#e2e3e5
```

ComparaciÃ³n con modelos de intervenciÃ³n existentes:

| IntervenciÃ³n | SemÃ¡ntica | Precedente |
|---|---|---|
| `Continue` | Proceder normalmente | ComÃºn (todos los sistemas) |
| `Override(Value)` | Inyectar un valor de reemplazo en el flujo de ejecuciÃ³n | Restart `use-value` de Common Lisp, pero elegido por LLM |
| `Fix{new_code, explanation}` | Reescribir cÃ³digo fuente; re-ejecutar desde el inicio | Herramientas APR (GenProg, ChatRepair), pero con contexto de runtime |
| `Backtrack{checkpoint, adjustments}` | Restaurar la VM al checkpoint nombrado, aplicar ajustes a variables, continuar desde ese punto | **Sin precedente directo**, hasta donde sabemos, en APR o frameworks LLM |
| `Halt(error)` | Detener ejecuciÃ³n con explicaciÃ³n | ComÃºn (todos los sistemas) |

#### Backtrack como primitiva semÃ¡ntica central

La intervenciÃ³n `Backtrack` no es una feature auxiliar---es la pieza que transforma la semÃ¡ntica de AURA. Con backtracking con ajustes, **el programa deja de ser una secuencia y se convierte en un grafo navegable en runtime:**

```mermaid
graph LR
    S0["Estadoâ‚€<br/>inicio"] --> S1["Estadoâ‚<br/>checkpoint A"]
    S1 --> S2["Estadoâ‚‚<br/>checkpoint B"]
    S2 --> S3["Estadoâ‚ƒ<br/>error/fallo"]
    S3 -.->|"Backtrack(A, ajustesâ‚)"| S1
    S1 --> S4["Estadoâ‚„<br/>trayectoria alternativa"]
    S4 --> S5["Estadoâ‚…<br/>Ã©xito"]
    S3 -.->|"Backtrack(B, ajustesâ‚‚)"| S2
    S2 --> S6["Estadoâ‚†<br/>otra alternativa"]

    style S3 fill:#f8d7da
    style S5 fill:#d4edda
```

A diferencia de las herramientas APR que deben re-ejecutar desde cero, y a diferencia de la supervisiÃ³n de Erlang que reinicia desde el estado inicial, AURA puede restaurar a cualquier checkpoint nombrado *con ajustes*---el LLM especifica quÃ© variables modificar antes de reanudar. Esto permite re-ejecuciÃ³n parcial con correcciones informadas.

La consecuencia semÃ¡ntica central es que la ejecuciÃ³n ya no avanza monÃ³tonamente. El programa puede:
- **Retroceder** a un estado anterior (como Prolog)
- **Ajustar variables** antes de reanudar (a diferencia de Prolog, que solo hace backtracking puro)
- **Elegir a quÃ© checkpoint retroceder** (navegaciÃ³n en el grafo, no solo backtracking cronolÃ³gico)

Esto es mÃ¡s cercano a *reversible computing* parcial y a planificaciÃ³n online que a los modelos de ejecuciÃ³n tradicionales. La implicaciÃ³n teÃ³rica se desarrolla en la SecciÃ³n 4.

### 3.4 Seguridad: adaptaciÃ³n acotada por invariantes

La funciÃ³n `validate_fix()` impone restricciones de seguridad antes de que cualquier modificaciÃ³n propuesta por el LLM sea aplicada:

1. **RestricciÃ³n de tamaÃ±o**: los fixes que exceden `max_fix_lines` (por defecto: 50) son rechazados, previniendo reescrituras completas del programa.
2. **Validez sintÃ¡ctica**: cada fix propuesto debe tokenizarse y parsearse como AURA vÃ¡lido.
3. **Inmutabilidad de goals**: el fix debe preservar todos los goals declarados---sin adiciones, sin eliminaciones, sin modificaciones. Los goals son dominio exclusivo del desarrollador.
4. **Profundidad de backtrack**: `max_backtrack_depth` (por defecto: 5) previene ciclos infinitos de backtrack.
5. **Seguimiento de progreso**: `max_deliberations_without_progress` (por defecto: 3) detiene el razonamiento descontrolado.

```rust
pub struct CognitiveSafetyConfig {
    pub max_fix_lines: usize,
    pub max_backtrack_depth: usize,
    pub max_deliberations_without_progress: usize,
}
```

```mermaid
graph TD
    FIX[Fix propuesto por LLM] --> S1{Â¿TamaÃ±o â‰¤ 50 lÃ­neas?}
    S1 -->|No| REJ1[Rechazado:<br/>fix demasiado grande]
    S1 -->|SÃ­| S2{Â¿Parsea como AURA vÃ¡lido?}
    S2 -->|No| REJ2[Rechazado:<br/>sintaxis invÃ¡lida]
    S2 -->|SÃ­| S3{Â¿Preserva todos los goals?}
    S3 -->|No| REJ3[Rechazado:<br/>goals modificados]
    S3 -->|SÃ­| S4{Â¿No introduce goals nuevos?}
    S4 -->|No| REJ4[Rechazado:<br/>goals agregados]
    S4 -->|SÃ­| OK[Fix aceptado âœ“<br/>aplicar y re-ejecutar]

    style REJ1 fill:#f8d7da
    style REJ2 fill:#f8d7da
    style REJ3 fill:#f8d7da
    style REJ4 fill:#f8d7da
    style OK fill:#d4edda
```

Esto establece un espacio de adaptaciÃ³n formalmente acotado: el LLM puede modificar el programa, pero solo dentro de las restricciones que el desarrollador ha declarado. Este es un patrÃ³n de diseÃ±o novedoso---**restricciones declaradas por el desarrollador sobre la modificaciÃ³n automatizada de programas**---que no tiene precedente directo en la literatura de APR o sistemas auto-adaptativos.

### 3.5 Sistema de checkpoints

El `CheckpointManager` mantiene snapshots nombrados del estado de la VM:

```rust
pub struct VMCheckpoint {
    pub name: String,
    pub variables: HashMap<String, Value>,
    pub step_count: u64,
    pub timestamp: Instant,
}
```

Los checkpoints se crean implÃ­citamente (ante disparadores de `observe`, antes de llamadas a funciones) y pueden restaurarse con ajustes:

```mermaid
sequenceDiagram
    participant VM as VM
    participant CP as CheckpointManager
    participant CR as CognitiveRuntime

    VM->>CP: save("obtener_usuarios", variables, paso=3)
    Note over CP: { usuarios: [...], contador: 3 }

    VM->>VM: ejecuciÃ³n continÃºa...
    VM->>CR: check_goals()
    Note over CR: goal "todos los usuarios deben estar activos" â†’ false

    CR-->>VM: Backtrack{ checkpoint: "obtener_usuarios",<br/>adjustments: [("usuarios", filtrados)] }

    VM->>CP: restore("obtener_usuarios")
    CP-->>VM: variables restauradas al paso 3
    VM->>VM: aplica ajustes: usuarios = lista_filtrada
    VM->>VM: continÃºa ejecuciÃ³n desde paso 3
```

Esto combina ideas de memoria transaccional de software (Shavit & Touitou 1995; Harris et al. 2005), backtracking cronolÃ³gico de Prolog, y manejo de fallos de planes BDI, pero la sÃ­ntesis---backtracking con ajustes sugeridos por LLM en un ciclo de ejecuciÃ³n cognitiva---es nueva.

### 3.6 El AgentCognitiveRuntime

La implementaciÃ³n real conecta el trait `CognitiveRuntime` a un `AgentProvider` (soportando mÃºltiples backends de LLM):

```rust
pub struct AgentCognitiveRuntime<P: AgentProvider> {
    provider: P,
    tokio_handle: Handle,        // puente async-sync
    goals: Vec<GoalDef>,
    invariants: Vec<String>,
    source_code: String,
    observation_buffer: Vec<ObservationEvent>,
    reasoning_trace: Vec<ReasoningEpisode>,
    available_checkpoints: Vec<String>,
    max_deliberations: usize,
    deliberation_count: usize,
    safety_config: CognitiveSafetyConfig,
    consecutive_backtracks: usize,
    deliberations_without_progress: usize,
}
```

Decisiones de diseÃ±o clave:

- **Puente async-sync**: la VM es sÃ­ncrona; el `AgentProvider` es async. `tokio_handle.block_on()` tiende el puente, manteniendo simple la implementaciÃ³n de la VM.
- **AgrupaciÃ³n de observaciones**: los eventos se acumulan en `observation_buffer` y se drenan despuÃ©s de cada deliberaciÃ³n, proporcionando al LLM contexto acumulativo.
- **Memoria episÃ³dica**: `reasoning_trace: Vec<ReasoningEpisode>` registra cada episodio de deliberaciÃ³n, incluido en solicitudes subsiguientes para que el LLM pueda aprender de la historia reciente.
- **Fail-open**: si el provider falla (error de red, timeout), el runtime retorna `Continue` en lugar de crashear. La capa cognitiva nunca hace al programa *menos* confiable.

### 3.7 El runner de ejecuciÃ³n cognitiva

La funciÃ³n `run_cognitive()` orquesta el ciclo de reintentos:

```rust
pub fn run_cognitive(
    source: &str,
    cognitive: Box<dyn CognitiveRuntime>,
    max_retries: usize,
) -> Result<CognitiveRunResult, RuntimeError>
```

```mermaid
graph TD
    START[CÃ³digo fuente] --> PARSE[Parsear fuente actual]
    PARSE --> CREATE[Crear VM con CognitiveRuntime]
    CREATE --> RUN[Ejecutar programa]

    RUN --> CHECK{Â¿Hay pending_fixes?}
    CHECK -->|SÃ­| VAL[Validar fix vÃ­a validate_fix]
    VAL --> APPLY[Aplicar fix al fuente]
    APPLY --> RETRY{Â¿Quedan reintentos?}
    RETRY -->|SÃ­| PARSE
    RETRY -->|No| ERR[Retornar error]

    CHECK -->|No| RESULT{Â¿Resultado OK?}
    RESULT -->|SÃ­| OK[Retornar CognitiveRunResult<br/>valor + fixes aplicados + reintentos]
    RESULT -->|No| RETRY

    style OK fill:#d4edda
    style ERR fill:#f8d7da
```

Para cada intento:
1. Parsear el cÃ³digo fuente actual
2. Crear la VM con runtime cognitivo (primer intento) o `NullCognitiveRuntime` (reintentos)
3. Cargar y ejecutar el programa
4. Si existen `pending_fixes`, validar cada fix vÃ­a `validate_fix()`, aplicar el vÃ¡lido, y reintentar
5. Si la ejecuciÃ³n tiene Ã©xito sin fixes pendientes, retornar el resultado
6. Si se agotaron los reintentos, retornar el error

Crucialmente, las decisiones `Backtrack` se manejan *dentro* de una sola ejecuciÃ³n (son restauraciones de estado en lÃ­nea), mientras que las decisiones `Fix` requieren re-parseo y re-ejecuciÃ³n. Esta adaptaciÃ³n de doble nivel---backtrack en lÃ­nea para correcciones rÃ¡pidas, re-ejecuciÃ³n completa para cambios estructurales---proporciona una flexibilidad mayor que la de sistemas de estrategia Ãºnica.

---

## 4. Modelo formal: EjecuciÃ³n como selecciÃ³n de trayectoria restringida

Esta secciÃ³n presenta el centro teÃ³rico de AURA. No describimos una arquitectura de software---definimos un *modelo de computaciÃ³n*. El argumento procede de lo constitutivo a lo relacional: primero establecemos quÃ© *es* ejecutar un programa AURA (Secciones 4.2-4.5), luego quÃ© *significa* que una ejecuciÃ³n sea correcta (Secciones 4.6-4.7), luego quÃ© *propiedades* tiene el modelo (Secciones 4.8-4.10), y finalmente cÃ³mo se *relaciona* con modelos existentes (Secciones 4.11-4.14).

La tesis formal que esta secciÃ³n demuestra:

> Un programa AURA no define una funciÃ³n de entradas a salidas, sino un espacio de historias vÃ¡lidas restringido por semÃ¡ntica declarativa, donde la ejecuciÃ³n es la selecciÃ³n progresiva de una trayectoria consistente bajo incertidumbre.

### 4.1 Seis preguntas fundacionales

Para afirmar que AURA propone un *modelo de computaciÃ³n*---y no simplemente una arquitectura interesante---debemos responder seis preguntas con precisiÃ³n formal. Cada pregunta distingue a AURA de los modelos existentes; juntas, cierran ontolÃ³gicamente quÃ© es ejecutar en AURA.

1. **Â¿QuÃ© es un programa?** No es cÃ³digo. Es una tripleta de implementaciÃ³n, intenciones y restricciones. â†’ DefiniciÃ³n 1 (SecciÃ³n 4.2).

2. **Â¿QuÃ© es un estado?** No es un par (entorno, expresiÃ³n). Es una 7-tupla que incluye historia, checkpoints, goals activos e invariantes activos. â†’ DefiniciÃ³n 2 (SecciÃ³n 4.2).

3. **Â¿QuÃ© es un paso?** Es una transiciÃ³n determinista o una transiciÃ³n mediada por orÃ¡culo, cada una con reglas de inferencia explÃ­citas. â†’ Definiciones 5, 13 (Secciones 4.3, 4.4).

4. **Â¿QuÃ© significa terminar?** No es alcanzar un valor. Es alcanzar una configuraciÃ³n donde el valor producido es consistente con todos los goals activos, o donde el orÃ¡culo ha decidido detenerse. â†’ Definiciones 16-17 (SecciÃ³n 4.5).

5. **Â¿QuÃ© significa ser correcto?** La validez estructural (la ejecuciÃ³n respeta transiciones legales e invariantes) no implica correcciÃ³n semÃ¡ntica (la intenciÃ³n del desarrollador fue satisfecha). Hay una jerarquÃ­a de tres niveles. â†’ Definiciones 21-23, Teorema 3 (SecciÃ³n 4.7).

6. **Â¿QuÃ© no puede expresar un modelo clÃ¡sico?** Un programa AURA con orÃ¡culo activo tiene mÃºltiples ejecuciones vÃ¡lidas; el modelo clÃ¡sico de Turing asigna exactamente una. La denotaciÃ³n de un programa AURA es un *conjunto de trayectorias*, no un valor. â†’ Teorema 2, Teoremas 6-8 (Secciones 4.6, 4.12).

Las seis respuestas convergen en la tesis: ejecutar un programa AURA es seleccionar una trayectoria en un espacio restringido, no evaluar una funciÃ³n.

### 4.2 Programa, estado cognitivo y espacio de configuraciones

**DefiniciÃ³n 1 (Programa AURA).** Un programa AURA es una tripleta *P = (C, G, I)* donde:

- *C* es el cÃ³digo: el AST que define las transiciones deterministas (funciones, expresiones, let-bindings). En la implementaciÃ³n, *C* es el resultado de `Parser::parse()` sobre el cÃ³digo fuente.
- *G = {gâ‚, ..., gâ‚˜}* es el conjunto de goals, cada uno con descripciÃ³n y expresiÃ³n check opcional: *gâ±¼ = (descâ±¼, checkâ±¼?)*. ImplementaciÃ³n: `Vec<GoalDef>` donde `GoalDef = {description: String, check: Option<Expr>}`.
- *I = {iâ‚, ..., iâ‚–}* es el conjunto de invariantes: expresiones que definen restricciones duras. ImplementaciÃ³n: `Vec<Expr>` almacenado en `VM.invariants`.

Nota fundamental: **el programa no es el cÃ³digo**. El programa es la tripleta. El cÃ³digo *C* es la implementaciÃ³n; los goals *G* e invariantes *I* son la especificaciÃ³n. Esta separaciÃ³n es la que permite que el orÃ¡culo modifique *C* sin "cambiar el programa"---siempre que *G* e *I* se preserven.

**DefiniciÃ³n 2 (Estado cognitivo).** El estado cognitivo de la VM es una 7-tupla:

```
Î£ = (H, Î“, Îº, G_act, I_act, Î©, U)
```

donde:

- *H*: Var â†’ Val es el heap---el mapeo de variables a valores. ImplementaciÃ³n: `Environment.variables: HashMap<String, Value>`.
- *Î“*: Name â†’ FuncDef es el entorno de funciones. ImplementaciÃ³n: `Environment.functions: HashMap<String, FuncDef>`.
- *Îº*: List(Frame) es la pila de continuaciones (stack de llamadas). ImplementaciÃ³n: el patrÃ³n de save/restore en `VM::call_function()`, donde cada llamada crea un nuevo `Environment` con `parent: Option<Box<Environment>>`.
- *G_act âŠ† G* son los goals activos---el subconjunto de goals del programa con expresiones check que se evalÃºan periÃ³dicamente. ImplementaciÃ³n: `VM.goals: Vec<GoalDef>`.
- *I_act âŠ† I* son los invariantes activos. ImplementaciÃ³n: `VM.invariants: Vec<Expr>`.
- *Î©*: Seq(ObservationEvent) es la historia de observaciones---la secuencia ordenada de eventos cognitivos observados durante la ejecuciÃ³n. ImplementaciÃ³n: `AgentCognitiveRuntime.observation_buffer: Vec<ObservationEvent>`.
- *U*: Name â‡€ Î£' es el mapa parcial de checkpoints---snapshots nombrados de estados anteriores. ImplementaciÃ³n: `CheckpointManager.checkpoints: HashMap<String, VMCheckpoint>` donde `VMCheckpoint = {name, variables, step_count, timestamp}`.

Cada componente de Î£ tiene un mapeo directo a un campo del struct `VM` o del struct `AgentCognitiveRuntime`. Esto no es una coincidencia: la formalizaciÃ³n *describe* la implementaciÃ³n, no la idealiza.

**DefiniciÃ³n 3 (ConfiguraciÃ³n).** Una configuraciÃ³n de ejecuciÃ³n es una 4-tupla:

```
ğ’ = (P, Î£, e, n)
```

donde *P = (C, G, I)* es el programa, *Î£* es el estado cognitivo (DefiniciÃ³n 2), *e* es la expresiÃ³n actualmente bajo evaluaciÃ³n, y *n âˆˆ â„•* es el contador de pasos (implementaciÃ³n: `VM.step_count: u64`).

**DefiniciÃ³n 4 (Espacio de configuraciones).** Para un programa *P*, el espacio de configuraciones es:

```
Conf(P) = { ğ’ | ğ’â‚€ â†’* ğ’ }
```

donde *ğ’â‚€ = (P, Î£â‚€, eâ‚€, 0)* es la configuraciÃ³n inicial (estado vacÃ­o, expresiÃ³n raÃ­z del programa, paso cero) y *â†’** es la clausura reflexivo-transitiva de la relaciÃ³n de transiciÃ³n (Definiciones 5 y 13). *Conf(P)* contiene todas las configuraciones alcanzables desde el inicio.

### 4.3 SemÃ¡ntica operacional: transiciones deterministas

**DefiniciÃ³n 5 (TransiciÃ³n determinista).** La relaciÃ³n â†’_d âŠ‚ Conf Ã— Conf define las transiciones que no requieren orÃ¡culo. Cada regla corresponde a un brazo del `match expr` en `VM::eval()`:

```
                              x âˆˆ dom(H)
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [VAR]
    (P, Î£, x, n) â†’_d (P, Î£, H(x), n+1)


    (P, Î£, eâ‚, n) â†’_d (P, Î£â‚, vâ‚, nâ‚)    Î£â‚‚ = Î£â‚[H â†¦ Hâ‚[x â†¦ vâ‚]]
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [LET]
    (P, Î£, let x = eâ‚, n) â†’_d (P, Î£â‚‚, vâ‚, nâ‚)


    Î“(f) = (params, body)    |args| = |params|
    (P, Î£[Îº â†¦ ÎºÂ·Frame(H)], body[params â†¦ args], n) â†’_d* (P, Î£', v, n')
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [CALL]
    (P, Î£, f(args), n) â†’_d (P, Î£'[Îº â†¦ Îº], v, n')


    (P, Î£, e_cond, n) â†’_d (P, Î£â‚, v_cond, nâ‚)    v_cond â‰  false, nil
    (P, Î£â‚, e_then, nâ‚) â†’_d (P, Î£â‚‚, v, nâ‚‚)
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [IF-TRUE]
    (P, Î£, if e_cond then e_then else e_else, n) â†’_d (P, Î£â‚‚, v, nâ‚‚)


    vâ‚, vâ‚‚ âˆˆ Val    v = vâ‚ âŠ• vâ‚‚    (âŠ• determinado por el operador)
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [BINOP]
    (P, Î£, vâ‚ âŠ• vâ‚‚, n) â†’_d (P, Î£, v, n+1)
```

Estas reglas son representativas, no exhaustivas. AURA incluye reglas adicionales para records, listas, `print`, pattern matching, y otras construcciones. Lo relevante es que el fragmento determinista es un lenguaje funcional estÃ¡ndar con semÃ¡ntica convencional.

**ProposiciÃ³n 1 (Determinismo del fragmento puro).** *Si ğ’ â†’_d ğ’â‚ y ğ’ â†’_d ğ’â‚‚, entonces ğ’â‚ = ğ’â‚‚.*

*DemostraciÃ³n.* Cada regla de â†’_d tiene premisas mutuamente excluyentes (determinadas por la forma sintÃ¡ctica de *e*): [VAR] requiere que *e* sea un identificador, [LET] que sea un let-binding, [CALL] que sea una aplicaciÃ³n, etc. Para una configuraciÃ³n dada, a lo sumo una regla aplica. El resultado de cada regla es una funciÃ³n determinista de sus premisas: la aritmÃ©tica es determinista, la bÃºsqueda en *H* es determinista, la selecciÃ³n de rama en [IF-TRUE]/[IF-FALSE] es determinista. Por tanto â†’_d es una funciÃ³n parcial sobre Conf. âˆ

**DefiniciÃ³n 6 (FunciÃ³n de evaluaciÃ³n determinista).** La funciÃ³n de evaluaciÃ³n determinista es:

```
eval_d : Expr Ã— Î£ â†’ (Val Ã— Î£) âˆª {âŠ¥}
eval_d(e, Î£) = (v, Î£')  si (P, Î£, e, n) â†’_d* (P, Î£', v, n')
             = âŠ¥         si e se queda stuck
```

`eval_d` estÃ¡ bien definida por ProposiciÃ³n 1: dado que â†’_d es una funciÃ³n parcial, la secuencia de reducciones es Ãºnica, y el resultado (si existe) es Ãºnico. ImplementaciÃ³n: la rama determinista de `VM::eval()` que no invoca al runtime cognitivo.

**DefiniciÃ³n 7 (EvaluaciÃ³n con orÃ¡culo --- set-valued).** La evaluaciÃ³n con orÃ¡culo es una funciÃ³n de conjuntos:

```
Eval : Expr Ã— Î£ Ã— O Ã— S â†’ ğ’«(Val Ã— Î£)
Eval(e, Î£, O, S) = { (v, Î£') | âˆƒÏ„ âˆˆ âŸ¦PâŸ§_S : Ï„ pasa por (_, Î£, e, _)
                     y la siguiente configuraciÃ³n-valor es (_, Î£', v, _) }
```

Mientras `eval_d` retorna a lo sumo un resultado, `Eval` retorna un *conjunto* de resultados posibles: distintos orÃ¡culos pueden producir distintos valores para la misma expresiÃ³n stuck.

**ObservaciÃ³n (Composicionalidad).** `eval_d` es composicional: `eval_d(let x = eâ‚ in eâ‚‚, Î£) = eval_d(eâ‚‚, Î£'[x â†¦ v])` donde `(v, Î£') = eval_d(eâ‚, Î£)`. Esto se sigue directamente de la regla [LET] y del determinismo de â†’_d. En cambio, `Eval` **no es composicional**: el orÃ¡culo puede intervenir entre sub-expresiones, alterando el estado de formas no predecibles desde la semÃ¡ntica de las sub-expresiones solas. Formalmente, `Eval(let x = eâ‚ in eâ‚‚, Î£, O, S) âŠ‰ â‹ƒ_{(v,Î£') âˆˆ Eval(eâ‚,Î£,O,S)} Eval(eâ‚‚, Î£'[x â†¦ v], O, S)` en general, ya que el orÃ¡culo acumula historia (Î©) y puede tomar decisiones diferentes segÃºn el contexto de evaluaciÃ³n. Esta ruptura de composicionalidad es una **consecuencia fundamental** del modelo, no un defecto: es lo que distingue a AURA de un lenguaje funcional con efectos.

### 4.4 El orÃ¡culo y las transiciones no deterministas

**DefiniciÃ³n 8 (Disparador de deliberaciÃ³n).** Un disparador de deliberaciÃ³n clasifica la causa que activa al orÃ¡culo. Es un elemento del tipo suma:

```
Trigger = ExpectFailed(failure)
        | ExplicitReason(observations, question)
        | TechnicalError(error)
        | GoalMisalignment(goal_desc, check_result)
```

ImplementaciÃ³n directa: el enum `DeliberationTrigger` en `vm/cognitive.rs` con exactamente estas cuatro variantes.

**DefiniciÃ³n 9 (Ãlgebra de intervenciÃ³n).** El Ã¡lgebra de intervenciÃ³n es el conjunto:

```
Î” = { Continue,
      Override(v)           donde v âˆˆ Val,
      Fix(C', expl)         donde C' es un AST y expl es una explicaciÃ³n,
      Backtrack(cp, adj)    donde cp es un nombre de checkpoint y adj âŠ† Var Ã— Val,
      Halt(err)             donde err es un error }
```

ImplementaciÃ³n directa: el enum `CognitiveDecision` en `vm/cognitive.rs`. Î” es finito en estructura (cinco formas) pero infinito en contenido (los valores *v*, el cÃ³digo *C'*, y los ajustes *adj* son arbitrarios). Esto captura la intuiciÃ³n de que el orÃ¡culo estÃ¡ *estructuralmente restringido* pero *generativamente libre*.

**DefiniciÃ³n 10 (OrÃ¡culo).** Un orÃ¡culo es una funciÃ³n:

```
O : Conf Ã— Trigger â†’ Î”
```

No se requiere que *O* sea determinista, total, ni computable. Un LLM es una realizaciÃ³n de *O* (no determinista, parcial en la prÃ¡ctica, no computable en el sentido clÃ¡sico). El `NullCognitiveRuntime` es otra realizaciÃ³n: *O_null(ğ’, t) = Continue* para todo *ğ’* y *t*.

La separaciÃ³n entre el orÃ¡culo como interfaz formal y sus realizaciones concretas es deliberada. El modelo define quÃ© ejecuciones son *vÃ¡lidas* independientemente de *cÃ³mo* el orÃ¡culo elige; la calidad de la elecciÃ³n es un problema de ingenierÃ­a, no de semÃ¡ntica. Esto es anÃ¡logo a la semÃ¡ntica no determinista en lenguajes concurrentes: la semÃ¡ntica define quÃ© interleavings son vÃ¡lidos sin especificar quÃ© scheduler los produce.

**DefiniciÃ³n 11 (ConfiguraciÃ³n stuck).** Una configuraciÃ³n *ğ’ = (P, Î£, e, n)* estÃ¡ stuck si la expresiÃ³n activa no es un valor y ninguna regla determinista aplica:

```
stuck(ğ’) âŸº Â¬is_value(e) âˆ§ Â¬âˆƒğ’'. ğ’ â†’_d ğ’'
```

La primera condiciÃ³n excluye la terminaciÃ³n normal (DefiniciÃ³n 16): si *e* es un valor, la evaluaciÃ³n terminÃ³ exitosamente. La segunda exige que ninguna regla de â†’_d (DefiniciÃ³n 5) sea aplicable. ImplementaciÃ³n: en `VM::eval()`, una configuraciÃ³n stuck corresponde a un brazo del match que retorna `Err(RuntimeError)`, o a la evaluaciÃ³n de `reason`/`expect` que no puede resolverse determinÃ­sticamente.

**DefiniciÃ³n 12 (ClasificaciÃ³n de trigger).** La funciÃ³n de clasificaciÃ³n de trigger asigna a cada configuraciÃ³n stuck su causa:

```
trigger : {ğ’ | stuck(ğ’)} â†’ Trigger

trigger(ğ’) = ExpectFailed(f)           si e = expect(cond, desc) âˆ§ eval_d(cond, Î£) = (false, _)
           | GoalMisalignment(g, r)    si âˆƒg âˆˆ G_act: g.check â‰  âŠ¥ âˆ§ eval_d(g.check, Î£) = (false, _)
           | ExplicitReason(Î©, q)      si e = reason(q)
           | TechnicalError(err)       en otro caso
```

Las tres primeras variantes se verifican en el orden mostrado; `TechnicalError` es el caso residual. ImplementaciÃ³n directa: los brazos de `VM::eval()` que construyen un `DeliberationTrigger` e invocan `deliberate()`. La clasificaciÃ³n determina el contexto que el orÃ¡culo recibe y, por tanto, influye en la calidad de la intervenciÃ³n.

**DefiniciÃ³n 13 (Transiciones guiadas por orÃ¡culo).** La relaciÃ³n â†’_o âŠ‚ Conf Ã— Conf define las transiciones mediadas por el orÃ¡culo. La premisa comÃºn es que *ğ’* estÃ¡ stuck (DefiniciÃ³n 11) y el trigger estÃ¡ clasificado (DefiniciÃ³n 12):

```
    stuck(ğ’)    O(ğ’, trigger(ğ’)) = Continue
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [STEP-CONTINUE]
    ğ’ â†’_o ğ’       (la configuraciÃ³n no cambia; la ejecuciÃ³n sigue con la siguiente expresiÃ³n)


    stuck(ğ’)    O(ğ’, trigger(ğ’)) = Override(v')
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [STEP-OVERRIDE]
    (P, Î£, e, n) â†’_o (P, Î£, v', n+1)


    stuck(ğ’)    O(ğ’, trigger(ğ’)) = Backtrack(cp, adj)    cp âˆˆ dom(U)
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [STEP-BACKTRACK]
    (P, Î£, e, n) â†’_o (P, Î£[H â†¦ U(cp).variables âŠ• adj, n â†¦ U(cp).step_count], e_resume, n')


    stuck(ğ’)    O(ğ’, trigger(ğ’)) = Fix(C', expl)    validate(C', G, S) = OK
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [STEP-FIX]
    (P, Î£, e, n) â†’_o ((C', G, I), Î£â‚€, eâ‚€', 0)


    stuck(ğ’)    O(ğ’, trigger(ğ’)) = Halt(err)
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [STEP-HALT]
    (P, Î£, e, n) â†’_o (P, Î£[halted â†¦ err], âŠ¥, n)
```

Observaciones cruciales:

- **STEP-FIX preserva la especificaciÃ³n**: el programa resultante es *(C', G, I)*---mismo *G*, mismo *I*, diferente *C*. La validaciÃ³n `validate(C', G, S)` (implementaciÃ³n: `validate_fix()`) garantiza que *C'* sea parseable, que todos los goals en *G* aparezcan en *C'*, que no se agreguen goals nuevos, y que el tamaÃ±o no exceda los lÃ­mites de *S*.
- **STEP-BACKTRACK restaura con ajustes**: el estado se restaura al checkpoint *cp* pero las variables en *adj* se sobreescriben. Esto no es backtracking puro (Prolog) sino backtracking con hipÃ³tesis (el orÃ¡culo propone: "si esta variable hubiera tenido este valor...").
- **STEP-CONTINUE no es trivial**: el orÃ¡culo puede decidir explÃ­citamente que la ejecuciÃ³n debe continuar pese al stuck---por ejemplo, cuando un `expect` falla pero el orÃ¡culo juzga que no requiere intervenciÃ³n.

**DefiniciÃ³n 14 (Admisibilidad).** Una decisiÃ³n *Î´ âˆˆ Î”* es admisible en el contexto de una configuraciÃ³n *ğ’* y restricciones *S* si satisface las condiciones de seguridad correspondientes a su variante:

```
admissible(Continue, ğ’, S)          = true
admissible(Override(v), ğ’, S)       = âˆ€iâ‚– âˆˆ I_act: eval_d(iâ‚–, Î£[result(e) â†¦ v]) âˆ‰ {false}
admissible(Fix(C', expl), ğ’, S)    = validate(C', G, S) = OK
admissible(Backtrack(cp, adj), ğ’, S) = cp âˆˆ dom(U)
                                       âˆ§ consecutive_backtracks < max_backtrack_depth
                                       âˆ§ âˆ€iâ‚– âˆˆ I_act: eval_d(iâ‚–, U(cp).Î£ âŠ• adj) âˆ‰ {false}
admissible(Halt(err), ğ’, S)        = true
```

Continue y Halt son siempre admisibles: el primero no modifica nada, el segundo detiene la ejecuciÃ³n. Override es admisible si el valor inyectado no viola invariantes activos. Fix es admisible si pasa la validaciÃ³n completa (`validate_fix()`). Backtrack es admisible si el checkpoint existe, no se excede la profundidad mÃ¡xima de backtracks consecutivos, y el estado restaurado con ajustes no viola invariantes.

**ProposiciÃ³n 2 (Exhaustividad del Ã¡lgebra).** *Toda transiciÃ³n â†’_o es instancia de exactamente una de las cinco reglas.*

*DemostraciÃ³n.* Las cinco reglas tienen premisas mutuamente excluyentes determinadas por la forma de *Î´ = O(ğ’, trigger(ğ’))*: Î´ es exactamente una variante del tipo suma Î” (DefiniciÃ³n 9). Toda variante de Î” tiene una regla correspondiente. Por tanto la particiÃ³n es exhaustiva y disjunta. âˆ

**ProposiciÃ³n 3 (Soundness de admisibilidad).** *Si `admissible(Î´, ğ’, S)` y ğ’ â†’_o ğ’' vÃ­a Î´, entonces âˆ€iâ‚– âˆˆ I_act: eval_d(iâ‚–, H') âˆ‰ {false}, donde H' es el heap de la configuraciÃ³n resultante ğ’'.*

*DemostraciÃ³n (por anÃ¡lisis de casos).* Para *Î´ = Continue*: la configuraciÃ³n no cambia, por lo que *H' = H* y los invariantes que se satisfacÃ­an antes siguen satisfaciÃ©ndose. Para *Î´ = Override(v)*: la definiciÃ³n de admisibilidad verifica explÃ­citamente los invariantes sobre el nuevo valor. Para *Î´ = Backtrack(cp, adj)*: la admisibilidad verifica invariantes sobre el estado restaurado con ajustes aplicados. Para *Î´ = Fix(C', expl)*: la ejecuciÃ³n reinicia con Î£â‚€ (estado inicial vacÃ­o); los invariantes se verificarÃ¡n desde cero durante la nueva ejecuciÃ³n. Para *Î´ = Halt(err)*: la ejecuciÃ³n se detiene; no hay *H'* sobre el que verificar invariantes (la trayectoria ha terminado). âˆ

### 4.5 EjecuciÃ³n, trayectoria y terminaciÃ³n

Esta subsecciÃ³n responde la cuarta pregunta fundacional: *Â¿quÃ© significa terminar?*

**DefiniciÃ³n 15 (Trayectoria).** Una trayectoria de un programa *P* es una secuencia (posiblemente infinita) de configuraciones:

```
Ï„ = ğ’â‚€, ğ’â‚, ğ’â‚‚, ...
```

donde *ğ’â‚€* es la configuraciÃ³n inicial y cada par consecutivo estÃ¡ conectado por una transiciÃ³n: *ğ’áµ¢ â†’_d ğ’áµ¢â‚Šâ‚* o *ğ’áµ¢ â†’_o ğ’áµ¢â‚Šâ‚*. Sea *Traj(P)* el conjunto de todas las trayectorias de *P*.

**DefiniciÃ³n 16 (ConfiguraciÃ³n terminal).** Una configuraciÃ³n *ğ’ = (P, Î£, e, n)* es terminal si:

```
terminal(ğ’) âŸº (is_value(e) âˆ§ âˆ€g âˆˆ G_act : checkâ±¼ â‰  âŠ¥ âŸ¹ eval(checkâ±¼, H) â‰  false)
              âˆ¨ halted(ğ’)
```

La primera disyunciÃ³n dice: *e* es un valor (la evaluaciÃ³n produjo un resultado) y todos los goals activos con expresiÃ³n check estÃ¡n satisfechos. La segunda dice: el orÃ¡culo emitiÃ³ Halt. ImplementaciÃ³n: la VM retorna `Ok(value)` cuando la evaluaciÃ³n termina sin errores pendientes, o `Err(RuntimeError)` cuando se emite Halt.

NÃ³tese que `eval(checkâ±¼, H) â‰  false` no exige `= true`: si la evaluaciÃ³n del check produce âŠ¥ (variable del check aÃºn no definida), la condiciÃ³n se satisface vacuamente. Esto refleja la implementaciÃ³n, donde `eval(check)` puede fallar si las variables del check aÃºn no existen en *H*.

**DefiniciÃ³n 17 (TerminaciÃ³n).** Una trayectoria *Ï„* termina si es finita y su Ãºltima configuraciÃ³n es terminal:

```
terminates(Ï„) âŸº |Ï„| < âˆ âˆ§ terminal(last(Ï„))
```

**DefiniciÃ³n 18 (Restricciones de seguridad).** Las restricciones de seguridad son una 5-tupla:

```
S = (max_retries, max_deliberations, max_backtrack_depth, max_fix_lines, max_no_progress)
```

ImplementaciÃ³n directa: `max_retries` es el parÃ¡metro de `run_cognitive()`, y los restantes son campos de `CognitiveSafetyConfig`:

| Componente de *S* | Campo en la implementaciÃ³n | Default |
|---|---|---|
| max_retries | ParÃ¡metro de `run_cognitive()` | 3 |
| max_deliberations | `AgentCognitiveRuntime.max_deliberations` | 10 |
| max_backtrack_depth | `CognitiveSafetyConfig.max_backtrack_depth` | 5 |
| max_fix_lines | `CognitiveSafetyConfig.max_fix_lines` | 50 |
| max_no_progress | `CognitiveSafetyConfig.max_deliberations_without_progress` | 3 |

**Teorema 1 (TerminaciÃ³n acotada).** *Bajo las restricciones de seguridad S, toda trayectoria de un programa P termina. En particular:*

*(a) El nÃºmero total de pasos deterministas es a lo sumo (max_retries + 1) Â· N_steps, donde N_steps es el nÃºmero mÃ¡ximo de pasos de una ejecuciÃ³n determinista de C.*

*(b) El nÃºmero total de invocaciones al orÃ¡culo es a lo sumo max_retries Â· max_deliberations.*

*(c) El nÃºmero de backtracks consecutivos es a lo sumo max_backtrack_depth.*

*DemostraciÃ³n.* El runner `run_cognitive()` itera `for attempt in 0..=max_retries`---a lo sumo *max_retries + 1* iteraciones (incluyendo el intento inicial). Cada iteraciÃ³n ejecuta el programa completo (a lo sumo *N_steps* pasos deterministas) y puede producir a lo sumo un Fix (que causa re-intento) o cero Fix (que termina la iteraciÃ³n).

Dentro de cada iteraciÃ³n, el `AgentCognitiveRuntime` incrementa `deliberation_count` en cada llamada a `deliberate()` y retorna `Continue` cuando alcanza `max_deliberations`. El `NullCognitiveRuntime` (usado en reintentos posteriores al primero, por el patrÃ³n `.take()` en el runner) no delibera en absoluto. Por tanto el total de deliberaciones estÃ¡ acotado por *max_deliberations* (solo el primer intento delibera).

El campo `consecutive_backtracks` se incrementa en cada `Backtrack` consecutivo y causa `Halt` cuando alcanza `max_backtrack_depth`. El campo `deliberations_without_progress` causa `Halt` cuando alcanza `max_no_progress`.

Combinando: toda rama del ciclo de ejecuciÃ³n estÃ¡ acotada por un decremento lÃ©xico sobre la tupla *(reintentos restantes, deliberaciones restantes, backtracks restantes)*. Esta tupla decrece estrictamente en cada iteraciÃ³n del ciclo. Por tanto el ciclo termina. âˆ

### 4.6 EjecuciÃ³n vÃ¡lida y espacio de trayectorias

Esta subsecciÃ³n y la siguiente contienen la tesis central del modelo.

**DefiniciÃ³n 19 (Trayectoria vÃ¡lida).** Una trayectoria *Ï„ = ğ’â‚€, ğ’â‚, ...* es vÃ¡lida respecto a restricciones *S* si cumple tres condiciones:

1. **Legalidad de transiciones**: cada par consecutivo *ğ’áµ¢, ğ’áµ¢â‚Šâ‚* estÃ¡ conectado por una transiciÃ³n â†’_d (DefiniciÃ³n 5) o â†’_o (DefiniciÃ³n 13).

2. **Admisibilidad del orÃ¡culo**: toda decisiÃ³n *Î´* emitida por el orÃ¡culo satisface `admissible(Î´, ğ’, S)` (DefiniciÃ³n 14). Esto subsume tanto la seguridad de invariantes (verificada por caso en la definiciÃ³n de admisibilidad) como la disciplina de restricciones (profundidad de backtrack, validaciÃ³n de fixes).

3. **PreservaciÃ³n de especificaciÃ³n**: toda transiciÃ³n STEP-FIX preserva *G* e *I*. Si el programa antes de la transiciÃ³n es *(C, G, I)* y despuÃ©s es *(C', G', I')*, entonces *G' = G* e *I' = I*.

**DefiniciÃ³n 20 (DenotaciÃ³n de un programa).** La denotaciÃ³n de un programa *P* bajo restricciones *S* es:

```
âŸ¦PâŸ§_S = { Ï„ âˆˆ Traj(P) | valid(Ï„, S) âˆ§ terminates(Ï„) }
```

Es decir: el conjunto de todas las trayectorias vÃ¡lidas y terminantes de *P*. **La denotaciÃ³n de un programa AURA no es un valor---es un conjunto de trayectorias.**

**Teorema 2 (DenotaciÃ³n como conjunto de trayectorias --- tesis central).** *âŸ¦PâŸ§_S âŠ† ğ’«(Traj(P)). Si el orÃ¡culo es activo (O â‰  O_null) y existe al menos un punto stuck en la ejecuciÃ³n, entonces es posible que |âŸ¦PâŸ§_S| > 1: el programa admite mÃºltiples ejecuciones vÃ¡lidas. El orÃ¡culo selecciona entre ellas.*

*DemostraciÃ³n.* La primera parte es directa de la DefiniciÃ³n 20: âŸ¦PâŸ§_S es un subconjunto de Traj(P) y por tanto un elemento de ğ’«(Traj(P)).

Para la segunda parte, construimos un ejemplo explÃ­cito. Sea *P* un programa con un expect que falla, y sean *Oâ‚, Oâ‚‚* dos orÃ¡culos tales que *Oâ‚(ğ’, ExpectFailed(...)) = Override(vâ‚)* y *Oâ‚‚(ğ’, ExpectFailed(...)) = Override(vâ‚‚)* con *vâ‚ â‰  vâ‚‚*. Ambas trayectorias resultantes satisfacen las tres condiciones de validez (DefiniciÃ³n 19): las transiciones son legales (STEP-OVERRIDE), la decisiÃ³n Override es admisible (DefiniciÃ³n 14: no viola invariantes), y la especificaciÃ³n se preserva (Override no modifica *P*). Por tanto *Ï„â‚, Ï„â‚‚ âˆˆ âŸ¦PâŸ§_S* y *Ï„â‚ â‰  Ï„â‚‚*. âˆ

Este teorema es el resultado central: la denotaciÃ³n de un programa AURA es genuinamente no determinista cuando el orÃ¡culo estÃ¡ activo. Esto distingue formalmente a AURA de los lenguajes funcionales (denotaciÃ³n es un valor), de los lenguajes concurrentes (el no determinismo proviene del scheduling, no de un orÃ¡culo semÃ¡ntico), y de los lenguajes lÃ³gicos (el no determinismo proviene de la unificaciÃ³n, no de un orÃ¡culo generativo).

**ProposiciÃ³n 4 (Monotonicidad).** *Sea S una restricciÃ³n de seguridad y S' una relajaciÃ³n de S (mayores lÃ­mites). Entonces âŸ¦PâŸ§_S âŠ† âŸ¦PâŸ§_S'. AnÃ¡logamente, sea I' âŠ‚ I un subconjunto estricto de invariantes; entonces âŸ¦(C,G,I)âŸ§_S âŠ† âŸ¦(C,G,I')âŸ§_S.*

*DemostraciÃ³n.* Relajar restricciones solo puede hacer que mÃ¡s trayectorias satisfagan las condiciones de validez: la condiciÃ³n 1 (seguridad de invariantes) se debilita con menos invariantes, la condiciÃ³n 3 (disciplina del orÃ¡culo) se debilita con lÃ­mites mayores. Toda trayectoria vÃ¡lida bajo restricciones mÃ¡s fuertes sigue siendo vÃ¡lida bajo restricciones mÃ¡s dÃ©biles. âˆ

**ProposiciÃ³n 5 (ConexiÃ³n con power domains).** *âŸ¦PâŸ§_S se interpreta naturalmente en el sentido del power domain de Hoare (Plotkin 1976): P "puede producir" una trayectoria vÃ¡lida si existe al menos un orÃ¡culo O tal que la trayectoria resultante pertenece a âŸ¦PâŸ§_S.*

*ObservaciÃ³n.* El power domain de Smyth (P "debe producir" para todo orÃ¡culo) no se satisface en general: orÃ¡culos patolÃ³gicos (ej., siempre Halt) producen trayectorias degeneradas. El power domain de Plotkin (may + must) requerirÃ­a restricciones adicionales sobre el orÃ¡culo que el modelo actual no impone. La semÃ¡ntica de Hoare---existencial sobre orÃ¡culos---es la interpretaciÃ³n correcta para AURA: un programa es "ejecutable" si *existe* un orÃ¡culo que produce una trayectoria vÃ¡lida.

### 4.7 CorrecciÃ³n como pertenencia a trayectoria

Esta subsecciÃ³n responde la quinta pregunta fundacional: *Â¿quÃ© significa ser correcto?*

**DefiniciÃ³n 21 (CorrecciÃ³n estructural).** Una trayectoria *Ï„* es estructuralmente correcta si pertenece a la denotaciÃ³n del programa:

```
correct_struct(Ï„) âŸº Ï„ âˆˆ âŸ¦PâŸ§_S
```

Es decir: la trayectoria es vÃ¡lida (DefiniciÃ³n 19) y termina (DefiniciÃ³n 17). CorrecciÃ³n estructural es el nivel mÃ­nimo: la ejecuciÃ³n respetÃ³ las reglas del modelo.

**DefiniciÃ³n 22 (CorrecciÃ³n semÃ¡ntica --- intenciÃ³n satisfecha).** Una trayectoria *Ï„* satisface la intenciÃ³n declarada si, ademÃ¡s de ser estructuralmente correcta, todos los goals con check evalÃºan a true en la configuraciÃ³n final:

```
correct_sat(Ï„) âŸº terminal(last(Ï„)) âˆ§ âˆ€g âˆˆ G : (g.check â‰  âŠ¥ âŸ¹ eval(g.check, H_final) = true)
```

La diferencia con correcciÃ³n estructural es que aquÃ­ exigimos que todos los checks evalÃºen a *true*, no solo que no evalÃºen a *false*. Un goal cuyo check produce âŠ¥ en la configuraciÃ³n final no satisface esta definiciÃ³n.

**DefiniciÃ³n 23 (CorrecciÃ³n semÃ¡ntica --- intenciÃ³n preservada).** Una trayectoria *Ï„* preserva la intenciÃ³n si, ademÃ¡s de satisfacerla, toda intervenciÃ³n del orÃ¡culo es coherente con la semÃ¡ntica intencional de los goals:

```
correct_pres(Ï„) âŸº correct_sat(Ï„) âˆ§ âˆ€Î´áµ¢ âˆˆ interventions(Ï„) : intent_coherent(Î´áµ¢, G)
```

**Problema abierto.** La funciÃ³n `intent_coherent` no estÃ¡ definida formalmente en este trabajo. Su definiciÃ³n requiere un lenguaje de especificaciÃ³n de intenciones mÃ¡s rico que expresiones booleanas. Candidatos:

- *LÃ³gica de intenciones de Cohen-Levesque* (Cohen & Levesque 1990): formalizarÃ­a goals como actitudes mentales con axiomas de persistencia y compromiso.
- *HyperLTL*: formalizarÃ­a propiedades sobre *conjuntos* de trayectorias (ej., "toda trayectoria que satisface el goal lo hace por la misma razÃ³n").
- *LÃ³gica deÃ³ntica*: distinguirÃ­a entre lo que el orÃ¡culo *puede* hacer (permitido por Î”) y lo que *debe* hacer (coherente con la intenciÃ³n).

Ejemplo concreto de la brecha: dado `goal "mantener usuarios activos" check usuarios != nil`, un orÃ¡culo que elimina todos los usuarios satisface el check (lista vacÃ­a â‰  nil en AURA) pero no preserva la intenciÃ³n. La DefiniciÃ³n 22 no detecta esto; la DefiniciÃ³n 23 lo detectarÃ­a con un `intent_coherent` adecuado.

Sin definir `intent_coherent`, establecemos **condiciones necesarias** que cualquier formalizaciÃ³n futura debe cumplir:

**Desiderata para `intent_coherent`.**

```
(D1) Consistencia:     intent_coherent(Î´, G) âŸ¹ admissible(Î´, ğ’, S)
(D2) No-trivialidad:   âˆƒÎ´: admissible(Î´, ğ’, S) âˆ§ Â¬intent_coherent(Î´, G)
(D3) MonotonÃ­a en G:   G' âŠ† G âˆ§ intent_coherent(Î´, G) âŸ¹ intent_coherent(Î´, G')
(D4) Frame:            si Î´ no modifica variables mencionadas en g.check
                       para ningÃºn g âˆˆ G, entonces intent_coherent(Î´, G)
```

(D1) dice que coherencia implica admisibilidad---es estrictamente mÃ¡s fuerte. (D2) dice que la relaciÃ³n no es trivial: existen intervenciones admisibles que no son coherentes (el ejemplo de "eliminar usuarios" es admisible pero no coherente). (D3) dice que quitar goals no puede hacer incoherente algo coherente: si una intervenciÃ³n es coherente con un conjunto grande de goals, lo es con cualquier subconjunto. (D4) da una condiciÃ³n suficiente parcial: si la intervenciÃ³n no toca las variables mencionadas en los checks de los goals, es coherente---un principio de *frame* que acota el problema.

Estas desiderata son verificables contra el ejemplo: la intervenciÃ³n "eliminar usuarios" viola (D2) (es admisible pero incoherente) y no satisface (D4) (modifica la variable `usuarios` mencionada en el check).

**Teorema 3 (JerarquÃ­a de correcciÃ³n).** *correct_pres(Ï„) âŸ¹ correct_sat(Ï„) âŸ¹ correct_struct(Ï„). Los conversos son falsos.*

*DemostraciÃ³n.* La primera implicaciÃ³n es directa: correct_pres incluye correct_sat como conjunciÃ³n. La segunda es directa: correct_sat implica terminal(last(Ï„)) (y por tanto terminates(Ï„)) y el cumplimiento de goals fortalece la condiciÃ³n de terminaciÃ³n de correct_struct.

Para los conversos, construimos contraejemplos:

- *correct_struct â‡ correct_sat*: una trayectoria que termina con `halted(ğ’)` (el orÃ¡culo emitiÃ³ Halt) es estructuralmente correcta (terminal por la segunda disyunciÃ³n de DefiniciÃ³n 16) pero no satisface correct_sat si algÃºn goal check evalÃºa a âŠ¥ o false.

- *correct_sat â‡ correct_pres*: el ejemplo de "mantener usuarios activos" arriba---la trayectoria satisface todos los checks pero la intervenciÃ³n del orÃ¡culo (eliminar usuarios) no es coherente con la intenciÃ³n. âˆ

### 4.8 ContinuaciÃ³n negociada y equivalencia observacional

**DefiniciÃ³n 24 (ContinuaciÃ³n negociada).** Cuando una configuraciÃ³n *ğ’* estÃ¡ stuck (la expresiÃ³n *e* no puede reducir vÃ­a â†’_d), la continuaciÃ³n se determina por la funciÃ³n:

```
negotiate : (Î£_stuck, Trigger, O, G, I, S) â†’ Î” | REJECT
```

El protocolo de negociaciÃ³n procede en tres fases:

1. **ClasificaciÃ³n**: la VM identifica el trigger âˆˆ {TechnicalError, ExpectFailed, GoalMisalignment, ExplicitReason}.
2. **Propuesta**: el orÃ¡culo propone *Î´ = O(ğ’, trigger)*.
3. **ValidaciÃ³n**: las restricciones verifican la legalidad de *Î´*:
   - Si *Î´ = Fix(C', expl)*: `validate(C', G, S) = OK` â†’ aceptar; `= Err` â†’ REJECT.
   - Si *Î´ = Backtrack(cp, adj)*: *cp âˆˆ dom(U)* â†’ aceptar; *cp âˆ‰ dom(U)* â†’ REJECT.
   - Si *Î´ âˆˆ {Continue, Override(v), Halt(err)}*: aceptar.

ImplementaciÃ³n: en `AgentCognitiveRuntime::deliberate()`, la propuesta del LLM se parsea como `CognitiveDecision`, y en `run_cognitive()`, los Fix se validan vÃ­a `validate_fix()` antes de aplicarse.

La negociaciÃ³n involucra tres participantes---semÃ¡ntica (que declara stuck), orÃ¡culo (que propone Î´), y restricciones (que validan Î´)---y la continuaciÃ³n es *emergente* de su interacciÃ³n, no una propiedad de ninguno por separado.

**DefiniciÃ³n 25 (Viabilidad).** Una configuraciÃ³n stuck es viable si el orÃ¡culo puede proponer al menos una decisiÃ³n admisible:

```
viable(ğ’, O, S) âŸº stuck(ğ’) âˆ§ âˆƒÎ´ âˆˆ Î”: O(ğ’, trigger(ğ’)) = Î´ âˆ§ admissible(Î´, ğ’, S)
```

La viabilidad conecta tres conceptos previamente independientes: stuck (DefiniciÃ³n 11), trigger (DefiniciÃ³n 12), y admisibilidad (DefiniciÃ³n 14). Una configuraciÃ³n stuck que no es viable es un callejÃ³n sin salida del que el orÃ¡culo no puede salir dentro de las restricciones.

**ProposiciÃ³n 6 (Viabilidad del orÃ¡culo nulo).** *Para toda configuraciÃ³n stuck ğ’, se cumple `viable(ğ’, O_null, S)`.*

*DemostraciÃ³n.* *O_null(ğ’, t) = Continue* para todo *ğ’* y *t* (DefiniciÃ³n 10). Por DefiniciÃ³n 14, `admissible(Continue, ğ’, S) = true` siempre. Por tanto existe *Î´ = Continue* tal que *O_null(ğ’, trigger(ğ’)) = Î´* y *admissible(Î´, ğ’, S)*. âˆ

**ProposiciÃ³n 7 (No-viabilidad implica terminaciÃ³n).** *Si `stuck(ğ’) âˆ§ Â¬viable(ğ’, O, S)`, entonces la trayectoria termina. MÃ¡s aÃºn: toda trayectoria en âŸ¦PâŸ§_S solo pasa por configuraciones stuck que fueron viables.*

*DemostraciÃ³n (esbozo).* Si una configuraciÃ³n stuck no es viable, el orÃ¡culo no puede proponer una decisiÃ³n admisible. La ejecuciÃ³n no puede avanzar vÃ­a â†’_d (por definiciÃ³n de stuck) ni vÃ­a â†’_o (por falta de decisiÃ³n admisible). Pero los lÃ­mites de *S* (DefiniciÃ³n 18)---`max_deliberations` y `max_no_progress`---fuerzan Halt despuÃ©s de un nÃºmero finito de intentos fallidos. Por tanto la trayectoria termina. RecÃ­procamente, si una trayectoria pertenece a âŸ¦PâŸ§_S, toda transiciÃ³n â†’_o en ella aplicÃ³ una decisiÃ³n admisible (por la condiciÃ³n 2 de validez, DefiniciÃ³n 19), lo que implica que cada configuraciÃ³n stuck fue viable. âˆ

**DefiniciÃ³n 26 (Equivalencia observacional).** Dos trayectorias *Ï„â‚, Ï„â‚‚ âˆˆ âŸ¦PâŸ§_S* son observacionalmente equivalentes si producen el mismo resultado observable:

```
Ï„â‚ ~_obs Ï„â‚‚ âŸº terminal(last(Ï„â‚)) = terminal(last(Ï„â‚‚))
             âˆ§ âˆ€g âˆˆ G : eval(g.check, H_final(Ï„â‚)) = eval(g.check, H_final(Ï„â‚‚))
```

Es decir: misma configuraciÃ³n terminal y mismos resultados de goal checks. Dos trayectorias que llegan al mismo resultado por caminos diferentes (una vÃ­a Override, otra vÃ­a Backtrack) son observacionalmente equivalentes.

**ProposiciÃ³n 8 (No trivialidad de las clases de equivalencia).** *Cuando el orÃ¡culo es activo y existe al menos un punto stuck, las clases de equivalencia [Ï„]_{~obs} son generalmente no triviales: contienen mÃºltiples trayectorias distintas que producen el mismo resultado observable.*

*DemostraciÃ³n (constructiva).* Sea *P* un programa con un expect fallido. Sea *Oâ‚* un orÃ¡culo que elige Override(v) y *Oâ‚‚* un orÃ¡culo que elige Backtrack(cp, adj) seguido de evaluaciÃ³n exitosa que produce *v*. Las trayectorias *Ï„â‚* (un paso de override) y *Ï„â‚‚* (backtrack + re-evaluaciÃ³n) son distintas (diferentes longitudes, diferentes transiciones) pero producen la misma configuraciÃ³n terminal con el mismo valor *v* y los mismos resultados de goal checks. Por tanto *Ï„â‚ ~_obs Ï„â‚‚* y *[Ï„â‚]_{~obs}* contiene al menos dos elementos. âˆ

### 4.9 PreservaciÃ³n de especificaciÃ³n

**Teorema 4 (PreservaciÃ³n de especificaciÃ³n bajo toda intervenciÃ³n).** *Para toda transiciÃ³n â†’_o, si el programa antes de la transiciÃ³n es P = (C, G, I) y despuÃ©s es P' = (C', G', I'), entonces G' = G e I' = I.*

*DemostraciÃ³n (por anÃ¡lisis de casos sobre Î”).*

- *Î´ = Continue*: la configuraciÃ³n no cambia (regla STEP-CONTINUE). Trivialmente P' = P.
- *Î´ = Override(v)*: solo cambia el resultado de la expresiÃ³n evaluada. El programa P no se modifica. P' = P.
- *Î´ = Backtrack(cp, adj)*: solo cambia el estado Î£ (variables restauradas + ajustes). El programa P no se modifica. P' = P.
- *Î´ = Halt(err)*: solo se marca la configuraciÃ³n como halted. El programa P no se modifica. P' = P.
- *Î´ = Fix(C', expl)*: el programa cambia a (C', G, I). La premisa de STEP-FIX exige `validate(C', G, S) = OK`. La funciÃ³n `validate_fix()` verifica: (a) C' es parseable como AURA vÃ¡lido, (b) todos los goals en G aparecen en C', (c) no se agregan goals nuevos, (d) el tamaÃ±o no excede max_fix_lines. Por tanto G' = G. Los invariantes I no se mencionan en C' (son expresiones evaluadas en runtime, no declaraciones parseadas por validate_fix); I se preserva por construcciÃ³n del runner que copia I al nuevo programa. I' = I.

En los cinco casos, G' = G e I' = I. âˆ

### 4.10 Las primitivas como consecuencias necesarias del modelo

**Teorema 5 (Necesidad de las primitivas --- argumento de degeneraciÃ³n).** *Para cada primitiva p âˆˆ {goal/invariant, observe, expect, reason, backtrack}, eliminar p del modelo produce un modelo degenerado equivalente a uno existente:*

| Primitiva eliminada | Modelo degenerado | Equivalente existente |
|---|---|---|
| goal, invariant | âŸ¦PâŸ§_S = Traj(P) (todas las trayectorias son vÃ¡lidas) | Retry con heurÃ­stica IA (ChatRepair con reintentos) |
| observe | O recibe ğ’ sin Î© (historia vacÃ­a) | ReparaciÃ³n post-mortem (el orÃ¡culo no ve el contexto) |
| expect | Trigger = {TechnicalError, ExplicitReason, GoalMisalignment} | Exception handler con retry (solo errores fatales activan el orÃ¡culo) |
| reason | Trigger = {TechnicalError, ExpectFailed, GoalMisalignment} | Sistema reactivo (el programa no puede solicitar guÃ­a proactiva) |
| backtrack | Î” = {Continue, Override(v), Fix(C', expl), Halt(err)} | APR restart-from-scratch (sin navegaciÃ³n del grafo de estados) |

*DemostraciÃ³n (por degeneraciÃ³n).*

- *Sin goals/invariantes*: la condiciÃ³n 2 de validez (DefiniciÃ³n 19) se debilita (la admisibilidad no verifica invariantes porque no hay). La condiciÃ³n 3 se satisface vacuamente (no hay especificaciÃ³n que preservar). âŸ¦PâŸ§_S contiene *todas* las trayectorias que terminan---el orÃ¡culo no tiene guÃ­a. Esto es indistinguible de un sistema de retry donde un LLM sugiere parches sin criterio de Ã©xito.

- *Sin observe*: el componente Î© del estado (DefiniciÃ³n 2) es siempre vacÃ­o. El orÃ¡culo recibe la configuraciÃ³n sin historia de observaciones. Las decisiones del orÃ¡culo son uniformemente desinformadas sobre la evoluciÃ³n del estado. Esto es indistinguible de la reparaciÃ³n post-mortem: el orÃ¡culo ve el error pero no el camino que llevÃ³ a Ã©l.

- *Sin expect*: el disparador ExpectFailed no existe. El orÃ¡culo solo se activa ante errores fatales (TechnicalError), desalineaciÃ³n de goals (GoalMisalignment), o consultas explÃ­citas (ExplicitReason). Se pierde la capacidad de correcciÃ³n proactiva---detectar que algo no va bien *antes* de que crashee. Esto colapsa al patrÃ³n try/catch con retry.

- *Sin reason*: el disparador ExplicitReason no existe. El programa no puede solicitar guÃ­a del orÃ¡culo; solo la recibe pasivamente ante errores. Se pierde la agencia programÃ¡tica---el programa como solicitante activo de deliberaciÃ³n. Esto colapsa a un sistema puramente reactivo.

- *Sin backtrack*: Î” pierde la variante Backtrack. Las Ãºnicas correcciones posibles son Override (puntual, sin cambio de estado) y Fix (reinicio total con nuevo cÃ³digo). No hay exploraciÃ³n de trayectorias alternativas. Esto es exactamente lo que hacen las herramientas APR existentes: reiniciar desde cero con un parche. âˆ

**ProposiciÃ³n 9 (Suficiencia).** *Las cinco primitivas (goal/invariant, observe, expect, reason, backtrack) junto con la evaluaciÃ³n determinista (â†’_d) bastan para realizar toda Ï„ âˆˆ âŸ¦PâŸ§_S.*

*DemostraciÃ³n (esbozo).* Toda trayectoria en âŸ¦PâŸ§_S es una secuencia de transiciones â†’_d y â†’_o. Las transiciones â†’_d son realizadas por la evaluaciÃ³n determinista. Las transiciones â†’_o requieren: (1) detectar que *e* estÃ¡ stuck---realizado por la evaluaciÃ³n determinista que produce error; (2) clasificar el trigger---realizado por expect (ExpectFailed), reason (ExplicitReason), goals (GoalMisalignment), o el evaluador (TechnicalError); (3) aplicar la decisiÃ³n---Override y Fix no requieren primitivas adicionales, Backtrack requiere checkpoints (creados por observe). Por tanto las cinco primitivas + evaluaciÃ³n determinista cubren toda la maquinaria necesaria. âˆ

### 4.11 Backtrack y la estructura de grafo

**DefiniciÃ³n 27 (Grafo de ejecuciÃ³n).** El grafo de ejecuciÃ³n de una trayectoria *Ï„* es *G(Ï„) = (V, E)* donde:

- *V = {ğ’áµ¢ | ğ’áµ¢ âˆˆ Ï„}* es el conjunto de configuraciones visitadas.
- *E = {(ğ’áµ¢, ğ’áµ¢â‚Šâ‚, label) | ğ’áµ¢, ğ’áµ¢â‚Šâ‚ consecutivos en Ï„}* donde *label âˆˆ {det, continue, override, backtrack, fix, halt}* indica el tipo de transiciÃ³n.

Cuando Ï„ no contiene transiciones Backtrack, G(Ï„) es un camino (grafo lineal). Cuando Ï„ contiene Backtrack, G(Ï„) es un Ã¡rbol: cada Backtrack crea una bifurcaciÃ³n desde un nodo anterior.

**DefiniciÃ³n 28 (Anchura de exploraciÃ³n).** La anchura de exploraciÃ³n de una trayectoria es:

```
width(Ï„) = max_{cp âˆˆ dom(U)} |{Î´áµ¢ âˆˆ Ï„ | Î´áµ¢ = Backtrack(cp, _)}|
```

Es decir: el nÃºmero mÃ¡ximo de veces que se retrocede al *mismo* checkpoint con ajustes diferentes. Mide cuÃ¡ntas alternativas se exploraron desde un mismo punto de decisiÃ³n.

**Propiedad (No-monotonicidad).** Con Backtrack, la ejecuciÃ³n es no-monotÃ³nica: el step_count puede disminuir (restaurar a un checkpoint anterior). Formalmente, si *ğ’áµ¢ â†’_o ğ’áµ¢â‚Šâ‚* vÃ­a STEP-BACKTRACK, entonces *náµ¢â‚Šâ‚ â‰¤ náµ¢*. Esto viola un supuesto fundamental de los modelos de ejecuciÃ³n convencionales.

**Propiedad (Ajustes como hipÃ³tesis).** Los ajustes *adj* en Backtrack(cp, adj) son hipÃ³tesis contrafactuales que el orÃ¡culo propone: "si las variables hubieran tenido estos valores en este punto, la trayectoria habrÃ­a sido diferente". Esto conecta con el razonamiento contrafactual (Pearl 2000) y con la bÃºsqueda heurÃ­stica en planificaciÃ³n.

**Propiedad (ExploraciÃ³n acotada).** A diferencia de Prolog (backtracking exhaustivo) o model checking (exploraciÃ³n exhaustiva), la exploraciÃ³n de AURA es: guiada por orÃ¡culo (no exhaustiva), acotada por *max_backtrack_depth* (DefiniciÃ³n 18), e informada por la historia Î© (el orÃ¡culo ve backtracks anteriores y puede evitar repetirlos).

### 4.12 RelaciÃ³n con modelos clÃ¡sicos: reducciones formales

Esta subsecciÃ³n responde la sexta pregunta fundacional: *Â¿quÃ© no puede expresar un modelo clÃ¡sico?*

**Teorema 6 (ReducciÃ³n a Turing).** *Cuando O = O_null (el NullCognitiveRuntime), AURA es equivalente a un lenguaje funcional convencional con semÃ¡ntica determinista.*

*DemostraciÃ³n.* Con *O = O_null*, toda invocaciÃ³n del orÃ¡culo retorna Continue. No se producen transiciones â†’_o (STEP-CONTINUE no modifica la configuraciÃ³n). Toda trayectoria consiste exclusivamente de transiciones â†’_d. Por ProposiciÃ³n 1, â†’_d es determinista. La trayectoria es Ãºnica. Los goals se evalÃºan pero no causan deliberaciÃ³n (Continue no altera el estado). Los invariantes se verifican pero no restringen la adaptaciÃ³n (no hay adaptaciÃ³n). La ejecuciÃ³n es *eval(main)* en un lenguaje funcional estÃ¡ndar. âˆ

**Teorema 7 (ReducciÃ³n a planificaciÃ³n --- esbozo).** *Cuando C es trivial (un programa que solo declara goals y estados iniciales) y O tiene acceso a una biblioteca de planes, AURA se comporta como un sistema de planificaciÃ³n online.*

*Esbozo de prueba.* La ejecuciÃ³n degenera en: (1) evaluar los goals, (2) detectar que todos estÃ¡n insatisfechos (GoalMisalignment), (3) invocar al orÃ¡culo repetidamente para que proponga Fix que acerquen el estado a la satisfacciÃ³n de goals. El orÃ¡culo actÃºa como planificador; los goals son las condiciones meta; los invariantes son las restricciones del dominio; los Fix son las acciones del plan. Esto mapea directamente a la arquitectura 3T (Gat 1998) y a planificaciÃ³n HTN (Nau et al. 2003).

**Teorema 8 (ReducciÃ³n a sistema reactivo --- esbozo).** *Cuando U = âˆ… (sin checkpoints) y G = âˆ… (sin goals), AURA se comporta como un sistema de control reactivo con exception handling.*

*Esbozo de prueba.* Sin checkpoints, Backtrack no estÃ¡ disponible (la premisa *cp âˆˆ dom(U)* de STEP-BACKTRACK nunca se satisface). Sin goals, GoalMisalignment no se dispara. El orÃ¡culo solo se activa ante TechnicalError (errores fatales) y produce Override (inyecciÃ³n de valor para continuar) o Fix (parcheo y reinicio). Esto es indistinguible de un sistema reactivo con exception handler que hace retry.

**Corolario (AURA generaliza estrictamente los tres modelos).**

| Modelo | RelaciÃ³n con AURA | Componentes desactivados |
|--------|-------------------|--------------------------|
| Turing / lenguaje funcional | Caso sin deliberaciÃ³n | O = O_null |
| PlanificaciÃ³n | DeliberaciÃ³n sin ejecuciÃ³n sustantiva | C trivial |
| Control reactivo | EjecuciÃ³n sin historia ni intenciones | U = âˆ…, G = âˆ… |
| **AURA** | **PlanificaciÃ³n local continua durante ejecuciÃ³n** | Ninguno |

AURA se posiciona en una intersecciÃ³n que ningÃºn modelo individual cubre: ejecuciÃ³n determinista (Turing) + intenciones declarativas (planificaciÃ³n) + adaptaciÃ³n en runtime (reactivo) + navegaciÃ³n de estados (backtracking) + orÃ¡culo generativo (LLM). La formalizaciÃ³n muestra que cada modelo clÃ¡sico es un *caso degenerado* de AURA, no un competidor.

### 4.13 Esbozo: tipos cognitivos y computabilidad

*Esta subsecciÃ³n presenta direcciones de investigaciÃ³n, no resultados establecidos.*

La denotaciÃ³n âŸ¦PâŸ§_S (DefiniciÃ³n 20) no distingue *cÃ³mo* se produce un valor---si por evaluaciÃ³n determinista, por inyecciÃ³n del orÃ¡culo, o por backtracking. Un sistema de tipos cognitivos capturarÃ­a esta distinciÃ³n:

```
Tipos de continuaciÃ³n:
  Pure(v)                    â€” valor producido determinÃ­sticamente
  Repair(v, explanation)     â€” valor producido tras reparaciÃ³n de cÃ³digo
  Replan(v, checkpoint)      â€” valor producido tras backtracking
  Rollback(v, adj)           â€” valor producido tras backtracking con ajustes
  Delegate(v, confidence)    â€” valor inyectado por el orÃ¡culo
```

Una funciÃ³n con efecto cognitivo tendrÃ­a tipo *f : A â†’ B ! {Repair, Rollback}*, indicando que puede requerir intervenciÃ³n del orÃ¡culo. El `NullCognitiveRuntime` garantizarÃ­a *f : A â†’ B ! âˆ…*---evaluaciÃ³n pura.

La incertidumbre tipada extenderÃ­a los valores:

```
  Certain(v)                        â€” producido determinÃ­sticamente
  OracleProvided(v, confidence)     â€” inyectado por el orÃ¡culo con grado de confianza
  Hypothetical(v, checkpoint)       â€” producido en una rama exploratoria
```

**Conjetura 1 (Programas open-world).** *Existe una clase de problemas---"programas open-world" donde las entradas son ambiguas, incompletas, o requieren interpretaciÃ³n semÃ¡ntica---donde AURA con orÃ¡culo activo provee una representaciÃ³n directa que los modelos clÃ¡sicos solo pueden simular con overhead no acotado.*

### 4.14 Esbozo: complejidad computacional cognitiva

*Esta subsecciÃ³n presenta direcciones de investigaciÃ³n, no resultados establecidos.*

El costo de una trayectoria *Ï„* puede descomponerse en cuatro dimensiones:

```
Cost(Ï„) = (T_det, T_oracle, N_backtracks, N_fixes)
```

donde *T_det* = nÃºmero de pasos deterministas, *T_oracle* = nÃºmero de invocaciones al orÃ¡culo, *N_backtracks* = nÃºmero de backtracks, *N_fixes* = nÃºmero de reescrituras de cÃ³digo.

El overhead cognitivo de un programa es:

```
overhead_cog(P) = (Cost_cognitive - Cost_deterministic) / Cost_deterministic
```

donde *Cost_deterministic* es el costo con *O = O_null* y *Cost_cognitive* es el costo con orÃ¡culo activo.

Los datos empÃ­ricos de la SecciÃ³n 7 muestran que el overhead es *O(1)*---constante (~900 Î¼s) e independiente del tamaÃ±o del programa. Esto es consistente con el diseÃ±o: el overhead proviene de la inicializaciÃ³n del runtime cognitivo (constante), no de la evaluaciÃ³n de expresiones (que es la misma con o sin orÃ¡culo).

Una teorÃ­a de complejidad cognitiva completa requerirÃ­a definir clases de complejidad parametrizadas por el costo del orÃ¡culo, anÃ¡logas a las clases relativizadas de la teorÃ­a de computabilidad (Rogers 1967). Por ejemplo: AURA-P = clase de programas AURA que terminan en tiempo polinomial cuando *T_oracle* estÃ¡ acotado polinomialmente. Esto queda como trabajo futuro.

---

## 5. Posicionamiento frente al estado del arte

### 5.1 ComparaciÃ³n integral

*Tabla 3: AURA posicionado contra sistemas representativos de cada lÃ­nea de investigaciÃ³n*

| DimensiÃ³n | GenProg | ChatRepair | LangChain | Rainbow | Jason | AURA v2.0 |
|---|---|---|---|---|---|---|
| **Naturaleza** | Herramienta APR | Herramienta de reparaciÃ³n LLM | OrquestaciÃ³n LLM | Framework auto-adaptativo | Lenguaje BDI | **Lenguaje cognitivo** |
| **CuÃ¡ndo ocurre la reparaciÃ³n** | Post-mortem | Post-mortem | N/A | Runtime (externo) | Fallo de plan | **A mitad de ejecuciÃ³n (en la VM)** |
| **Acceso al estado de runtime** | Ninguno | Ninguno | Ninguno | MÃ©tricas arquitectÃ³nicas | Base de creencias | **Completo: variables, goals, checkpoints** |
| **OrÃ¡culo de reparaciÃ³n** | Suite de tests | Suite de tests + LLM | N/A | Estrategias predefinidas | Biblioteca de planes | **Goals + expects + invariantes + LLM** |
| **InyecciÃ³n de valores** | No | No | No | No | ActualizaciÃ³n de creencias | **SÃ­ (`Override`)** |
| **Backtracking** | No | No | No | No | Pila de intenciones | **SÃ­ (checkpoint + ajustes)** |
| **Parcheo de cÃ³digo** | SÃ­ (fuente) | SÃ­ (fuente) | N/A | SÃ­ (config) | No | **SÃ­ (fuente validado)** |
| **Restricciones de seguridad** | Solo suite de tests | Ninguna | N/A | Por construcciÃ³n | Ninguna | **Invariantes + inmutabilidad de goals** |
| **IntenciÃ³n del desarrollador** | Casos de test | Casos de test | CÃ³digo Python | Restricciones arq. | Goals BDI | **`goal`, `expect`, `invariant`** |
| **IntegraciÃ³n LLM** | Ninguna | API externa | API externa | Ninguna | Ninguna | **Trait de runtime de primera clase** |

### 5.2 La brecha tripartita

AURA cierra una brecha en la intersecciÃ³n de tres preocupaciones previamente separadas:

```mermaid
graph TD
    subgraph " "
        A["ProgramaciÃ³n orientada<br/>a agentes (BDI)"]
        B["ReparaciÃ³n automÃ¡tica<br/>de programas (APR)"]
        C["Sistemas<br/>auto-adaptativos (MAPE-K)"]

        A ---|"goals + intenciones<br/>pero sin LLM"| AB[ ]
        B ---|"reparaciÃ³n con LLM<br/>pero post-mortem"| BC[ ]
        C ---|"monitoreo en runtime<br/>pero capa externa"| CA[ ]

        AB --- AURA
        BC --- AURA
        CA --- AURA

        AURA["ğŸ”· AURA v2.0<br/>Goals evaluados continuamente<br/>+ LLM con estado en vivo<br/>+ invariantes a nivel de lenguaje"]
    end

    style AURA fill:#4a90d9,color:#fff
    style AB fill:none,stroke:none
    style BC fill:none,stroke:none
    style CA fill:none,stroke:none
```

1. **NingÃºn lenguaje actual** proporciona construcciones integradas para expresar la intenciÃ³n del desarrollador (`goal`), expectativas de runtime (`expect`), monitoreo de variables (`observe`), puntos seguros de rollback (checkpoints), y solicitudes explÃ­citas de razonamiento (`reason`) como sintaxis de primera clase.

2. **NingÃºn sistema actual** da a un LLM acceso al estado de ejecuciÃ³n en vivo (valores de variables, camino de ejecuciÃ³n, resultados de evaluaciÃ³n de goals) durante la ejecuciÃ³n del programa, permitiendo decisiones a mitad de ejecuciÃ³n (inyecciÃ³n de valores, parcheo de cÃ³digo, backtracking basado en checkpoints).

3. **NingÃºn sistema actual** impone invariantes de seguridad sobre las adaptaciones generadas por LLM a nivel del lenguaje---donde los invariantes se declaran en la sintaxis del programa, se validan por el parser, y se aplican antes de que cualquier fix propuesto por el LLM sea aplicado.

### 5.3 AfirmaciÃ³n formal de novedad

> AURA implementa un modelo operacional donde ejecutar un programa es buscar una trayectoria vÃ¡lida en un espacio de estados restringido por goals e invariantes declarados por el desarrollador, donde (1) un orÃ¡culo cognitivo (LLM con acceso al estado de ejecuciÃ³n reificado) selecciona trayectorias cuando la ejecuciÃ³n determinista no puede continuar, (2) las intervenciones del orÃ¡culo estÃ¡n estructuralmente tipadas en un Ã¡lgebra de cinco modos (Continue, Override, Fix, Backtrack, Halt), (3) el backtracking con ajustes convierte al programa en un grafo navegable de estados---no una secuencia---, y (4) las restricciones declarativas (goals, invariantes) acotan formalmente el espacio de intervenciÃ³n del orÃ¡culo, distinguiendo a AURA de un sistema con "heurÃ­stica IA externa" y posicionÃ¡ndolo como un modelo operacional no determinista con orÃ¡culo.

### 5.4 Lo que no es novedoso

La honestidad acadÃ©mica requiere identificar sobre quÃ© construye AURA en lugar de inventar:

- La arquitectura BDI (Rao & Georgeff 1991, 1995; Bratman 1987)
- Ciclos auto-adaptativos MAPE-K (Kephart & Chess 2003)
- Mecanismos de checkpoint/rollback (Shavit & Touitou 1995)
- Backtracking cronolÃ³gico (Prolog; Colmerauer & Roussel 1993)
- ReparaciÃ³n de cÃ³digo basada en LLM (Xia & Zhang 2023; Le Goues et al. 2012)
- Hot code reloading (Armstrong 2003)
- Sistemas de mÃ³dulos basados en capacidades (cf. modelo de capacidades de SARL)
- Sistemas de condiciones/restarts (Common Lisp)
- VerificaciÃ³n en runtime (Leucker & Schallhart 2009)
- SemÃ¡ntica operacional (Plotkin 1981)
- ComputaciÃ³n con orÃ¡culos (Turing 1939)

La contribuciÃ³n de AURA no es ninguna de estas piezas individualmente. Es el modelo que emerge de su sÃ­ntesis: una mÃ¡quina abstracta donde la continuidad del programa es negociada entre la semÃ¡ntica determinista del lenguaje y un orÃ¡culo generativo, dentro de un espacio restringido por intenciones declaradas.

---

## 6. Ejemplo desarrollado: Auto-reparaciÃ³n en acciÃ³n

El siguiente programa AURA demuestra la ejecuciÃ³n cognitiva con auto-reparaciÃ³n real. A diferencia de un ejemplo trivial donde todo funciona, este programa tiene un **bug intencional** que el runtime cognitivo detecta y repara.

### 6.0 El escenario: Monitor de sensores IoT

Un sistema lee sensores de temperatura, detecta anomalÃ­as tÃ©rmicas, y genera alertas. El cÃ³digo usa `umbral_temp` para definir el umbral de anomalÃ­a---pero esa variable **nunca se define**. Sin runtime cognitivo, el programa crashea. Con Ã©l, el error se detecta, se delibera un fix, se aplica, y se reintenta.

```aura
+http +json

goal "monitorear todos los sensores"
goal "detectar anomalias termicas" check lecturas != nil

invariant len(lecturas) >= 0

# Simula lecturas de sensores IoT
obtener_lecturas() = [{sensor: "TH-01", temp: 22.5, humedad: 45},
                      {sensor: "TH-02", temp: 38.7, humedad: 30},
                      {sensor: "TH-03", temp: 21.0, humedad: 55}]

# Bug intencional: umbral_temp NO esta definido
es_anomalia(temp) = temp > umbral_temp

# Formatea una alerta para un sensor
formatear_alerta(s) = "ALERTA: sensor {s.sensor} reporta {s.temp}C"

# Punto de entrada principal
main = : lecturas = obtener_lecturas();
         observe lecturas;
         expect len(lecturas) > 0 "sin datos de sensores";
         s1 = first(lecturas);
         s2 = first(tail(lecturas));
         print("Sensor {s1.sensor}: {s1.temp}C");
         print("Sensor {s2.sensor}: {s2.temp}C");
         a2 = es_anomalia(s2.temp);
         expect a2 "se esperaba anomalia en {s2.sensor}";
         alerta = formatear_alerta(s2);
         print(alerta);
         accion = reason "sensor {s2.sensor} en {s2.temp}C, que accion tomar?";
         print("Analisis completo");
         s2.temp
```

### 6.1 EjecuciÃ³n sin runtime cognitivo

```bash
$ aura run examples/cognitive_demo.aura
Sensor TH-01: 22.5C
Sensor TH-02: 38.7C
Runtime error: Variable no definida: umbral_temp
```

El programa imprime los dos primeros sensores, llama a `es_anomalia(38.7)`, que evalÃºa `temp > umbral_temp`. Como `umbral_temp` no estÃ¡ definida, crashea. Comportamiento estÃ¡ndar de cualquier lenguaje.

### 6.2 EjecuciÃ³n con runtime cognitivo

```bash
$ aura run --cognitive --provider mock examples/cognitive_demo.aura
Cognitive mode: provider=mock
Sensor TH-01: 22.5C
Sensor TH-02: 38.7C
Sensor TH-01: 22.5C
Sensor TH-02: 38.7C
ALERTA: sensor TH-02 reporta 38.7C
Analisis completo
38.7
  [1 fix(es) applied, 1 retries]
```

### 6.3 Traza detallada del ciclo cognitivo

Lo que sucede internamente ilustra el modelo de selecciÃ³n de trayectoria (SecciÃ³n 4):

```mermaid
sequenceDiagram
    participant R as Runner
    participant P as Parser
    participant VM as VM
    participant CR as CognitiveRuntime
    participant LLM as MockProvider

    Note over R: Intento 0 (con cognitive)

    R->>P: parsear fuente original
    P->>VM: cargar programa
    VM->>VM: eval: lecturas = obtener_lecturas()
    VM->>CR: observe(ValueChanged{lecturas: Nil â†’ List[3]})
    VM->>CR: check_goals()
    Note over CR: "lecturas != nil" â†’ true âœ“

    VM->>VM: eval: es_anomalia(38.7)
    VM->>VM: eval: temp > umbral_temp
    Note over VM: RuntimeError: Variable no definida: umbral_temp

    VM->>CR: deliberate(TechnicalError{error})
    CR->>LLM: error + fuente + goals + observaciones
    Note over LLM: Detecta "Variable no definida: umbral_temp"
    Note over LLM: umbral â†’ valor 35.0
    Note over LLM: Genera fix: source.replace("umbral_temp", "35.0")
    LLM-->>CR: Fix{new_code, explanation}
    CR-->>VM: Fix â†’ pending_fixes

    Note over R: pending_fixes no vacÃ­o â†’ validar fix
    R->>R: validate_fix(new_code, goals, safety)
    Note over R: âœ“ Parseable, goals preservados, tamaÃ±o OK
    R->>R: current_source = new_code

    Note over R: Intento 1 (sin cognitive, fuente reparado)

    R->>P: parsear fuente reparado
    Note over P: es_anomalia(temp) = temp > 35.0
    P->>VM: cargar programa reparado
    VM->>VM: eval completo sin errores
    Note over VM: 38.7 > 35.0 â†’ true âœ“
    VM-->>R: Ok(38.7)

    Note over R: Retornar CognitiveRunResult<br/>value=38.7, fixes=1, retries=1
```

### 6.4 Lo que demuestra este ejemplo

Este ejemplo no es un escenario artificial---es la demostraciÃ³n concreta del modelo teÃ³rico de la SecciÃ³n 4:

1. **El programa define un espacio de trayectorias, no un comportamiento fijo.** La trayectoria "evaluar `temp > umbral_temp` â†’ crash" no es la Ãºnica posible. El runtime cognitivo encuentra una trayectoria alternativa: reemplazar `umbral_temp` con `35.0` y reintentar.

2. **El error es un punto de bifurcaciÃ³n, no un crash.** En la SecciÃ³n 4.4, definimos que un error activa una transiciÃ³n no determinista (DefiniciÃ³n 13, regla STEP-FIX). AquÃ­ el `RuntimeError` activa `deliberate(TechnicalError)`, que produce un `Fix`---una trayectoria alternativa.

3. **Las restricciones acotan la intervenciÃ³n.** El fix propuesto pasa por `validate_fix()`: debe parsear como AURA vÃ¡lido, preservar los dos goals, y no exceder 50 lÃ­neas. No es una intervenciÃ³n arbitraria---es una selecciÃ³n dentro del espacio restringido.

4. **El orÃ¡culo es semÃ¡nticamente informado.** El MockProvider no elige `35.0` al azar: reconoce que `umbral` es un nombre de variable que implica un valor de umbral numÃ©rico. Un provider real (Claude, Ollama) harÃ­a un razonamiento aÃºn mÃ¡s rico.

### 6.5 Contrafactual: escenario de backtracking

Imaginemos un goal mÃ¡s preciso con backtracking. Si `obtener_lecturas()` retornara un sensor con datos corruptos:

```aura
goal "todos los sensores deben tener lecturas vÃ¡lidas" check for(s in lecturas) : s.temp > 0
```

Cuando la verificaciÃ³n del goal falla:

```mermaid
sequenceDiagram
    participant VM as VM
    participant CP as Checkpoints
    participant CR as CognitiveRuntime
    participant LLM as LLM

    VM->>CP: save("observe_lecturas", vars, step=3)
    VM->>CR: check_goals()
    Note over CR: "for(s in lecturas): s.temp > 0" â†’ false âœ—

    CR->>LLM: GoalMisalignment + contexto + checkpoints
    LLM-->>CR: Backtrack{checkpoint: "observe_lecturas",<br/>adjustments: [("lecturas", lista_filtrada)]}

    CR-->>VM: Backtrack{...}
    VM->>CP: restore("observe_lecturas")
    Note over VM: variables restauradas al paso 3
    VM->>VM: aplica ajuste: lecturas = [solo sensores vÃ¡lidos]
    VM->>VM: continÃºa ejecuciÃ³n con datos corregidos âœ“
```

Esto ilustra la utilidad del backtracking con ajustes: el programa no crashea, no reinicia desde cero, no aplica una estrategia predefinida. El orÃ¡culo interpreta el goal, examina los datos, y propone una correcciÃ³n dirigida---navegando el grafo de estados (SecciÃ³n 4.11) hacia una trayectoria vÃ¡lida.

---

## 7. EvaluaciÃ³n empÃ­rica

Esta secciÃ³n presenta mediciones del prototipo de AURA implementado en Rust, con el objetivo de responder tres preguntas concretas: (1) Â¿cuÃ¡l es el overhead del runtime cognitivo cuando no hay errores?, (2) Â¿cuÃ¡l es el costo de una reparaciÃ³n exitosa?, y (3) Â¿cuÃ¡l es el impacto en recursos del sistema?

Todas las mediciones se realizaron sobre el binario compilado en modo release (`cargo build --release`), ejecutando cada configuraciÃ³n 20 veces y reportando el promedio. El hardware es una mÃ¡quina Linux x86_64 estÃ¡ndar. El provider cognitivo utilizado es `mock`---un provider determinista sin acceso a red que genera fixes basados en patrones del nombre de la variable, eliminando la variabilidad de latencia de red y de modelos externos.

### 7.1 Overhead cognitivo sin errores

Para medir el costo del runtime cognitivo cuando el programa no requiere intervenciÃ³n, se ejecutaron dos programas sin bugs con y sin el flag `--cognitive`:

| Programa | Normal (Î¼s) | Cognitivo (Î¼s) | Overhead |
|----------|-------------|-----------------|----------|
| `simple.aura` (asignaciÃ³n y retorno) | 1,630 | 2,532 | +55% |
| `math.aura` (aritmÃ©tica y comparaciones) | 1,494 | 2,483 | +66% |

El overhead proviene de la inicializaciÃ³n del `CognitiveRuntime`, la creaciÃ³n del `CheckpointManager`, y la evaluaciÃ³n de goals e invariantes en cada paso relevante. Para programas que no activan ningÃºn trigger cognitivo, el overhead es constante (aproximadamente 900 Î¼s) e independiente del tamaÃ±o del programa---lo cual es consistente con el diseÃ±o de inicializaciÃ³n Ãºnica del runtime.

Cuando el modo cognitivo no estÃ¡ activado, el `NullCognitiveRuntime` tiene cero overhead: todas sus operaciones son no-ops que retornan inmediatamente. Esto confirma que las capacidades cognitivas son puramente aditivas.

### 7.2 Costo de reparaciÃ³n cognitiva

Se midieron tres programas con bugs intencionales que el runtime cognitivo detecta y repara:

| Programa | Error | Fix aplicado | Crash (Î¼s) | ReparaciÃ³n (Î¼s) | Overhead |
|----------|-------|-------------|------------|------------------|----------|
| `cognitive_demo.aura` | `umbral_temp` indefinido | `umbral_temp` â†’ `35.0` | 1,475 | 2,603 | +76% |
| `cognitive_func.aura` | `min_puntaje` indefinido | `min_puntaje` â†’ `0` | 1,311 | 2,658 | +103% |
| `cognitive_config.aura` | `timeout_ms` indefinido | `timeout_ms` â†’ `5000` | 1,473 | 2,541 | +73% |

La columna "Crash" mide el tiempo hasta que el programa falla sin modo cognitivo. La columna "ReparaciÃ³n" mide el ciclo completo: ejecuciÃ³n inicial â†’ error â†’ deliberaciÃ³n â†’ generaciÃ³n de fix â†’ validaciÃ³n â†’ re-parseo â†’ segunda ejecuciÃ³n exitosa.

El overhead de reparaciÃ³n (73-103%) incluye dos ejecuciones completas del programa mÃ¡s el ciclo de deliberaciÃ³n. Que el costo total de una reparaciÃ³n exitosa sea menor que 2x el tiempo de crash indica que la deliberaciÃ³n y validaciÃ³n del fix son operaciones baratas en comparaciÃ³n con la ejecuciÃ³n del programa.

### 7.3 Impacto en recursos

**Memoria.** El consumo de memoria residente (RSS) es:

| Modo | RSS (KB) | Delta |
|------|----------|-------|
| Normal | 5,068 | --- |
| Cognitivo | 5,564 | +496 KB |

El incremento de ~500 KB corresponde al `CognitiveRuntime`, el buffer de observaciones, el `CheckpointManager`, y el historial de `ReasoningEpisode`. Para contextos donde el consumo de memoria es crÃ­tico, el modo no cognitivo no incurre en esta asignaciÃ³n.

**TamaÃ±o del binario.** El binario compilado en modo release ocupa 11 MB, incluyendo el runtime cognitivo, los tres providers (mock, Claude API, Ollama), y todas las capacidades del lenguaje (+http, +json, +db, +env).

### 7.4 Suite de tests

La implementaciÃ³n cuenta con 267 tests unitarios y de integraciÃ³n:

| Componente | Tests | Cobertura |
|------------|-------|-----------|
| Lexer | 41 | Todos los tokens, keywords, literales |
| Parser | 78 | AST completo, incluyendo nodos cognitivos |
| VM (core) | 89 | EvaluaciÃ³n, scoping, funciones, records |
| VM (cognitivo) | 36 | DeliberaciÃ³n, fixes, checkpoints, backtrack |
| Agente/bridge | 13 | MockProvider, validaciÃ³n de fixes |
| Capabilities | 10 | HTTP, JSON, DB, env |

La suite completa ejecuta en aproximadamente 1 segundo. Los 36 tests cognitivos cubren los cinco tipos de `CognitiveDecision` (Continue, Override, Fix, Backtrack, Halt), la validaciÃ³n de fixes (preservaciÃ³n de goals, tamaÃ±o mÃ¡ximo, parseabilidad), y los lÃ­mites de `CognitiveSafetyConfig`.

### 7.5 Alcance y limitaciones de la evaluaciÃ³n

Esta evaluaciÃ³n tiene limitaciones que deben hacerse explÃ­citas:

1. **Provider mock vs. provider real.** Los tiempos de reparaciÃ³n medidos usan el `MockProvider` determinista. Con un provider real (Claude API, Ollama), la latencia de deliberaciÃ³n dominarÃ­a el tiempo total (segundos en lugar de microsegundos). Los datos aquÃ­ miden el overhead del *framework* cognitivo, no el costo end-to-end con un LLM.

2. **Programas pequeÃ±os.** Los benchmarks usan programas de menos de 30 lÃ­neas. El overhead cognitivo constante (~900 Î¼s) se volverÃ­a proporcionalmente menor en programas mÃ¡s grandes, pero la validaciÃ³n de fixes (`validate_fix`) podrÃ­a escalar con el tamaÃ±o del cÃ³digo fuente.

3. **Un solo tipo de error.** Los tres ejemplos de reparaciÃ³n involucran el mismo patrÃ³n de error (variable indefinida). La generalizaciÃ³n a otros tipos de error (type mismatches, errores lÃ³gicos, fallos de invariantes) requiere evaluaciÃ³n adicional.

4. **Ausencia de comparaciÃ³n directa.** No existe otro lenguaje con deliberaciÃ³n cognitiva integrada en la semÃ¡ntica de ejecuciÃ³n, lo cual impide una comparaciÃ³n directa. Los sistemas mÃ¡s cercanos (ChatRepair, MAPE-K) operan en niveles diferentes y no son directamente comparables.

---

## 8. DiscusiÃ³n

### 8.1 Enmarcamiento teÃ³rico: del sistema al modelo

La SecciÃ³n 4 presentÃ³ el modelo formal de ejecuciÃ³n como selecciÃ³n de trayectoria restringida, estructurado alrededor de seis preguntas fundacionales (SecciÃ³n 4.1). AquÃ­ evaluamos quÃ© logra esa formalizaciÃ³n y quÃ© queda pendiente.

**Lo que logra la formalizaciÃ³n.** Las Definiciones 1-28 (Secciones 4.2-4.12) responden seis preguntas que cualquier modelo de computaciÃ³n debe abordar:

1. *Â¿QuÃ© es un programa?* (Def. 1): una tripleta P = (C, G, I) de implementaciÃ³n, intenciones y restricciones.
2. *Â¿QuÃ© es un estado?* (Def. 2): una 7-tupla que incluye heap, funciones, continuaciones, goals activos, invariantes, historia de observaciones y checkpoints.
3. *Â¿QuÃ© es un paso?* (Defs. 5, 13): una transiciÃ³n determinista o una transiciÃ³n mediada por orÃ¡culo, con reglas de inferencia explÃ­citas.
4. *Â¿QuÃ© significa terminar?* (Defs. 16-17): alcanzar una configuraciÃ³n terminal donde el valor es consistente con los goals o el orÃ¡culo decidiÃ³ detenerse.
5. *Â¿QuÃ© significa ser correcto?* (Defs. 21-23, Teorema 3): una jerarquÃ­a de tres niveles---correcciÃ³n estructural, intenciÃ³n satisfecha, intenciÃ³n preservada.
6. *Â¿QuÃ© no puede expresar un modelo clÃ¡sico?* (Teoremas 6-8): AURA generaliza estrictamente los modelos de Turing, planificaciÃ³n y control reactivo.

La denotaciÃ³n de un programa (DefiniciÃ³n 20, Teorema 2) como conjunto de trayectorias vÃ¡lidas es el resultado central: eleva a AURA de *sistema interesante* a *modelo de computaciÃ³n*. AURA no compite con DSPy, LMQL, o LangChain (herramientas para *usar* LLMs). Compite con la pregunta de quÃ© significa ejecutar un programa en presencia de incertidumbre.

**Lo que no logra (aÃºn).** La formalizaciÃ³n define *validez estructural* y *correcciÃ³n semÃ¡ntica por satisfacciÃ³n* (DefiniciÃ³n 22), pero no *correcciÃ³n semÃ¡ntica por preservaciÃ³n de intenciÃ³n* (DefiniciÃ³n 23). La brecha entre intenciÃ³n satisfecha e intenciÃ³n preservada (SecciÃ³n 4.7) permanece abierta como problema formal deliberado. Cerrarla requiere un lenguaje de especificaciÃ³n de intenciones mÃ¡s rico---conectando con la lÃ³gica de intenciones (Cohen & Levesque 1990), HyperLTL, o lÃ³gica deÃ³ntica.

**Mapeo a frameworks formales existentes:**

| Framework formal | Mapeo en AURA | ExtensiÃ³n de AURA |
|---|---|---|
| Efectos algebraicos (Plotkin & Pretnar 2009) | Primitivas cognitivas = efectos; `CognitiveRuntime` = handler | Handler generativo (LLM) vs. estÃ¡tico |
| MAPE-K (Kephart & Chess 2003) | `observe`=M, `Trigger`=A, LLM=P, `Decision`=E, `Episode`=K | Embebido en la VM, no capa externa |
| Condiciones/restarts (Common Lisp) | Errors + expects = condiciones; `CognitiveDecision` = restarts | Restarts generados dinÃ¡micamente, no predefinidos |
| SemÃ¡ntica de Prolog | Backtrack = backtracking cronolÃ³gico | Con ajustes a variables + orÃ¡culo para elegir alternativas |
| PlanificaciÃ³n HTN (Nau et al. 2003) | Programa = plan; deliberaciÃ³n = re-planificaciÃ³n | Re-planificaciÃ³n informada por estado de ejecuciÃ³n completo |
| Model checking | Espacio de estados + propiedades (goals/invariants) | ExploraciÃ³n guiada por orÃ¡culo, no exhaustiva |

```mermaid
graph LR
    subgraph "MAPE-K mapeado en AURA"
        M["Monitorear<br/><code>observe()</code>"] --> A["Analizar<br/><code>DeliberationTrigger</code>"]
        A --> P["Planificar<br/>LLM delibera"]
        P --> E["Ejecutar<br/><code>CognitiveDecision</code>"]
        E -.-> M
        K["Conocimiento<br/><code>ReasoningEpisode</code><br/><code>HealingMemory</code>"]
        K <-.-> M
        K <-.-> A
        K <-.-> P
        K <-.-> E
    end
```

### 8.2 El runtime como arquitectura cognitiva

Al mapearse a la teorÃ­a de arquitecturas cognitivas, el runtime de AURA implementa los componentes esenciales identificados por Newell (1990) y arquitecturas subsiguientes:

- **PercepciÃ³n**: detecciÃ³n de eventos vÃ­a `observe()`
- **Memoria de trabajo**: buffer de observaciones + contexto de ejecuciÃ³n
- **Memoria a largo plazo**: `HealingMemory` con persistencia de `ReasoningEpisode`
- **DeliberaciÃ³n**: invocaciÃ³n del LLM con contexto estructurado
- **SelecciÃ³n de acciÃ³n**: enum `CognitiveDecision`
- **Aprendizaje**: el historial de episodios informa deliberaciones subsiguientes
- **MetacogniciÃ³n**: lÃ­mites de `CognitiveSafetyConfig` sobre el comportamiento de razonamiento

Esto sugiere que AURA es, hasta donde sabemos, **el primer runtime de lenguaje de programaciÃ³n que es en sÃ­ mismo una arquitectura cognitiva**---en lugar de un lenguaje usado para implementar una.

### 8.3 Limitaciones

**IntenciÃ³n preservada vs. intenciÃ³n satisfecha (el problema central).** Como se discutiÃ³ en la SecciÃ³n 4.7 (Definiciones 22-23), AURA hoy solo garantiza *intenciÃ³n satisfecha*: que los goals evalÃºen a `true` despuÃ©s de una intervenciÃ³n. Pero no garantiza *intenciÃ³n preservada*: que la intervenciÃ³n sea coherente con lo que el desarrollador quiso decir. Un goal "mantener usuarios activos" podrÃ­a satisfacerse degeneradamente eliminando usuarios inactivos, cuando el desarrollador querÃ­a notificarles. Este es el problema conceptual mÃ¡s importante que enfrenta el modelo, y resolverlo probablemente requiere un lenguaje de especificaciÃ³n de intenciones mÃ¡s rico que expresiones booleanas---conectando con la semÃ¡ntica de intenciones de Cohen & Levesque (1990) o verificaciÃ³n de propiedades temporales (LTL/CTL).

**Coherencia semÃ¡ntica de intervenciones.** Relacionado con lo anterior: hoy el sistema valida que una intervenciÃ³n sea *segura* (invariants OK, goals preservados, parseable), pero no que sea *semÃ¡nticamente coherente*. El LLM puede producir una intervenciÃ³n que satisface todas las restricciones formales pero es absurda en contexto. La distinciÃ³n entre "decisiÃ³n semÃ¡nticamente vÃ¡lida" y "heurÃ­stica conveniente" requiere un modelo formal de validez de intervenciÃ³n que hoy no existe.

**Latencia.** La deliberaciÃ³n con LLM aÃ±ade segundos de latencia por invocaciÃ³n. AURA mitiga esto a travÃ©s del `NullCognitiveRuntime` (cero overhead cuando las caracterÃ­sticas cognitivas estÃ¡n inactivas) y observaciones agrupadas, pero las aplicaciones en tiempo real pueden necesitar lÃ­mites de latencia mÃ¡s estrictos.

**Determinismo.** Las respuestas del LLM son no determinÃ­sticas. Dos ejecuciones del mismo programa pueden seguir trayectorias diferentes en el espacio de estados (SecciÃ³n 4.6, Teorema 2). AURA registra la traza de `ReasoningEpisode` para anÃ¡lisis de reproducibilidad, pero las garantÃ­as formales de convergencia requieren trabajo futuro.

**CorrecciÃ³n de fixes generados por LLM.** La funciÃ³n `validate_fix()` verifica sintaxis, preservaciÃ³n de goals, y tamaÃ±o---pero no correcciÃ³n semÃ¡ntica. Un fix que parsea correctamente y preserva goals aÃºn puede introducir errores lÃ³gicos. La verificaciÃ³n formal de parches generados por LLM sigue siendo un problema de investigaciÃ³n abierto.

**Costo.** Cada deliberaciÃ³n incurre en costos de API del LLM. Los lÃ­mites `max_deliberations` y `max_deliberations_without_progress` acotan esto, pero las estrategias de deliberaciÃ³n conscientes del costo son trabajo futuro.

### 8.4 Direcciones futuras

- **Modelo formal de validez de intervenciÃ³n**: definir formalmente cuÃ¡ndo una intervenciÃ³n del orÃ¡culo es *semÃ¡nticamente vÃ¡lida* y no solo *segura*. Conectar con la lÃ³gica de intenciones (Cohen & Levesque 1990) y verificaciÃ³n temporal.
- **SemÃ¡ntica operacional completa**: extender las reglas representativas de la SecciÃ³n 4.3 a una semÃ¡ntica operacional de pasos pequeÃ±os exhaustiva que cubra todas las formas sintÃ¡cticas de AURA.
- **Convergencia de trayectorias**: demostrar formalmente que, bajo ciertas condiciones sobre el orÃ¡culo y las restricciones, la bÃºsqueda de trayectoria siempre termina (o acotar la probabilidad de no-terminaciÃ³n).
- **Runtime cognitivo multi-agente**: mÃºltiples orÃ¡culos con diferentes especializaciones, formalizados como un sistema de handlers compuestos.
- **AdaptaciÃ³n verificada**: usar mÃ©todos formales para demostrar que las adaptaciones dentro del espacio acotado por invariantes preservan propiedades especificadas (model checking restringido).
- **DeliberaciÃ³n consciente del costo**: estrategias que balanceen el costo de llamadas al orÃ¡culo contra el beneficio esperado (formalizable como problema de decisiÃ³n parcialmente observable).
- **CogniciÃ³n colaborativa**: modos humano-en-el-ciclo donde el runtime presenta las trayectorias candidatas en lugar de seleccionar autÃ³nomamente.

---

## 9. ConclusiÃ³n

Este trabajo se estructurÃ³ alrededor de seis preguntas fundacionales (SecciÃ³n 4.1) que cualquier modelo de computaciÃ³n debe responder. Las respuestas formales---28 definiciones, 8 teoremas, 9 proposiciones, 4 desiderata, y 1 conjetura (Secciones 4.2-4.14)---convergen en una tesis:

> Un programa AURA no define una funciÃ³n de entradas a salidas, sino un espacio de historias vÃ¡lidas restringido por semÃ¡ntica declarativa, donde la ejecuciÃ³n es la selecciÃ³n progresiva de una trayectoria consistente bajo incertidumbre.

Los resultados centrales son:

1. **La denotaciÃ³n de un programa es un conjunto de trayectorias** (DefiniciÃ³n 20, Teorema 2). Cuando el orÃ¡culo estÃ¡ activo, el programa admite mÃºltiples ejecuciones vÃ¡lidas; el orÃ¡culo selecciona entre ellas.

2. **La correcciÃ³n tiene tres niveles** (Definiciones 21-23, Teorema 3): correcciÃ³n estructural (la ejecuciÃ³n respetÃ³ las reglas), intenciÃ³n satisfecha (los goals evalÃºan a true), e intenciÃ³n preservada (las intervenciones son coherentes con la semÃ¡ntica intencional). Los conversos son falsos---cada nivel es estrictamente mÃ¡s fuerte.

3. **Toda intervenciÃ³n preserva la especificaciÃ³n** (Teorema 4): goals e invariantes se mantienen invariantes bajo toda transiciÃ³n â†’_o, incluyendo Fix que reescribe el cÃ³digo.

4. **Las primitivas son consecuencias necesarias** (Teorema 5): eliminar cualquiera degrada el modelo a uno existente. Son el vocabulario mÃ­nimo para que la ejecuciÃ³n-como-selecciÃ³n-de-trayectoria sea un modelo distinto.

5. **AURA generaliza estrictamente tres modelos clÃ¡sicos** (Teoremas 6-8): Turing (sin orÃ¡culo), planificaciÃ³n (sin ejecuciÃ³n sustantiva), y control reactivo (sin historia ni intenciones).

**AURA no es un lenguaje con features de agentes. Es una mÃ¡quina abstracta donde la continuidad del programa es negociada** entre semÃ¡ntica determinista, orÃ¡culo generativo, y restricciones declarativas. Las primitivas del lenguaje (`goal`, `observe`, `expect`, `reason`, `invariant`) no son features que decidimos agregar---son las consecuencias mÃ­nimas necesarias del modelo (SecciÃ³n 4.10): sin cualquiera de ellas, el modelo degenera a algo que ya existe.

La implementaciÃ³n en Rust (267 tests, modo cognitivo funcional con auto-reparaciÃ³n demostrada) prueba que el modelo es realizable. La evaluaciÃ³n empÃ­rica (SecciÃ³n 7) muestra que el overhead cognitivo sin errores es constante (~900 Î¼s), que una reparaciÃ³n exitosa cuesta menos de 2x el tiempo de crash, y que el `NullCognitiveRuntime` asegura cero overhead para programas no cognitivos.

Los problemas abiertos son significativos y productivos: la brecha entre intenciÃ³n satisfecha e intenciÃ³n preservada (DefiniciÃ³n 23, SecciÃ³n 4.7), los tipos cognitivos y la conjetura de programas open-world (SecciÃ³n 4.13), y la complejidad computacional cognitiva (SecciÃ³n 4.14). Estos son preguntas sobre modelos de computaciÃ³n, no sobre arquitectura de software---lo cual confirma que AURA opera en el espacio teÃ³rico correcto.

Si un programa ya no es una funciÃ³n sino una trayectoria en un espacio restringido, y si el orÃ¡culo que guÃ­a esa trayectoria es un modelo de lenguaje grande que entiende la semÃ¡ntica del cÃ³digo que ejecuta---entonces la frontera entre "programar" y "especificar intenciones para que una mÃ¡quina las navegue" se desdibuja de maneras que la teorÃ­a de lenguajes de programaciÃ³n aÃºn no ha explorado. AURA hace esa exploraciÃ³n concreta, formal, y tratable.

---

## Referencias

### Lenguajes de programaciÃ³n orientados a agentes

[1] Shoham, Y. (1993). "Agent-Oriented Programming." *Artificial Intelligence*, 60(1):51-92.

[2] Rao, A.S. (1996). "AgentSpeak(L): BDI Agents Speak Out in a Logical Computable Language." *MAAMAW'96*, LNCS 1038, Springer, 42-55.

[3] Bordini, R.H., Hubner, J.F., & Wooldridge, M. (2007). *Programming Multi-Agent Systems in AgentSpeak using Jason*. Wiley.

[4] Hindriks, K.V. (2009). "Programming Rational Agents in GOAL." In *Multi-Agent Programming*, Springer, 119-157.

[5] Dastani, M. (2008). "2APL: A Practical Agent Programming Language." *Autonomous Agents and Multi-Agent Systems*, 16(3):214-248.

[6] Rodriguez, S., Gaud, N., & Galland, S. (2014). "SARL: A General-Purpose Agent-Oriented Programming Language." *WI-IAT 2014*, IEEE/WIC/ACM.

[7] Pokahr, A., Braubach, L., & Lamersdorf, W. (2005). "Jadex: A BDI Reasoning Engine." In *Multi-Agent Programming*, Springer, 149-174.

### TeorÃ­a BDI

[8] Bratman, M.E. (1987). *Intention, Plans, and Practical Reason*. Harvard University Press.

[9] Rao, A.S. & Georgeff, M.P. (1991). "Modeling Rational Agents within a BDI-Architecture." *KR'91*, Morgan Kaufmann, 473-484.

[10] Rao, A.S. & Georgeff, M.P. (1995). "BDI Agents: From Theory to Practice." *ICMAS'95*, AAAI Press, 312-319.

[11] Sardina, S. & Padgham, L. (2011). "A BDI Agent Programming Language with Failure Handling, Declarative Goals, and Planning." *Autonomous Agents and Multi-Agent Systems*, 23(1):18-70.

[12] Cohen, P.R. & Levesque, H.J. (1990). "Intention is Choice with Commitment." *Artificial Intelligence*, 42(2-3):213-261.

### ReparaciÃ³n automÃ¡tica de programas

[13] Le Goues, C., Nguyen, T.V., Forrest, S., & Weimer, W. (2012). "GenProg: A Generic Method for Automatic Software Repair." *IEEE TSE*, 38(1):54-72.

[14] Nguyen, H.D.T., Qi, D., Roychoudhury, A., & Chandra, S. (2013). "SemFix: Program Repair via Semantic Analysis." *ICSE 2013*, 772-781.

[15] Mechtaev, S., Yi, J., & Roychoudhury, A. (2016). "Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis." *ICSE 2016*, 1071-1082.

[16] Long, F. & Rinard, M. (2016). "Automatic Patch Generation by Learning Correct Code." *POPL 2016*, 298-312.

[17] Xia, C.S. & Zhang, L. (2022). "Less Training, More Repairing Please: Revisiting Automated Program Repair via Zero-Shot Learning." *ESEC/FSE 2022*, 959-971.

[18] Xia, C.S. & Zhang, L. (2023). "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT." *ISSTA 2024*. arXiv:2304.00385.

[19] Monperrus, M. (2018). "Automatic Software Repair: A Bibliography." *ACM Computing Surveys*, 51(1):1-24.

### Sistemas auto-adaptativos

[20] Kephart, J.O. & Chess, D.M. (2003). "The Vision of Autonomic Computing." *IEEE Computer*, 36(1):41-50.

[21] Garlan, D., Cheng, S.-W., Huang, A.-C., Schmerl, B., & Steenkiste, P. (2004). "Rainbow: Architecture-Based Self-Adaptation with Reusable Infrastructure." *IEEE Computer*, 37(10):46-54.

[22] Weyns, D., Malek, S., & Andersson, J. (2012). "FORMS: Unifying Reference Model for Formal Specification of Distributed Self-Adaptive Systems." *ACM TAAS*, 7(1).

[23] Weyns, D. (2020). *An Introduction to Self-Adaptive Systems: A Contemporary Software Engineering Perspective*. Wiley/IEEE Press.

### Arquitecturas cognitivas

[24] Laird, J.E., Newell, A., & Rosenbloom, P.S. (1987). "SOAR: An Architecture for General Intelligence." *Artificial Intelligence*, 33(1):1-64.

[25] Newell, A. (1990). *Unified Theories of Cognition*. Harvard University Press.

[26] Anderson, J.R. & Lebiere, C. (1998). *The Atomic Components of Thought*. Lawrence Erlbaum Associates.

[27] Anderson, J.R. et al. (2004). "An Integrated Theory of the Mind." *Psychological Review*, 111(4):1036-1060.

[28] Sun, R. (2016). *Anatomy of the Mind: Exploring Psychological Mechanisms and Processes with the Clarion Cognitive Architecture*. Oxford University Press.

[29] Franklin, S. et al. (2014). "LIDA: A Systems-level Architecture for Cognition, Emotion, and Learning." *IEEE Trans. on Autonomous Mental Development*, 6(1):19-41.

### ReflexiÃ³n, efectos y meta-programaciÃ³n

[30] Smith, B.C. (1984). "Reflection and Semantics in Lisp." *POPL '84*, ACM, 23-35.

[31] Kiczales, G., des Rivieres, J., & Bobrow, D.G. (1991). *The Art of the Metaobject Protocol*. MIT Press.

[32] Kiczales, G. et al. (1997). "Aspect-Oriented Programming." *ECOOP '97*, LNCS 1241, Springer, 220-242.

[33] Plotkin, G.D. & Pretnar, M. (2009). "Handlers of Algebraic Effects." *ESOP 2009*, LNCS 5502, Springer, 80-94.

[34] Bauer, A. & Pretnar, M. (2015). "Programming with Algebraic Effects and Handlers." *Journal of Logical and Algebraic Methods in Programming*, 84(1):108-123.

### Checkpoint, rollback y tolerancia a fallos

[35] Shavit, N. & Touitou, D. (1995). "Software Transactional Memory." *PODC '95*, ACM, 204-213.

[36] Harris, T., Marlow, S., Peyton Jones, S., & Herlihy, M. (2005). "Composable Memory Transactions." *PPoPP '05*, ACM, 48-60.

[37] Armstrong, J. (2003). *Making Reliable Distributed Systems in the Presence of Software Errors*. PhD Thesis, Royal Institute of Technology, Stockholm.

[38] Rinard, M. et al. (2004). "Enhancing Server Availability and Security Through Failure-Oblivious Computing." *OSDI 2004*, USENIX, 303-316.

[39] Perkins, J.H. et al. (2009). "Automatically Patching Errors in Deployed Software." *SOSP 2009*, ACM, 87-102.

### ProgramaciÃ³n integrada con LLM

[40] Beurer-Kellner, L., Fischer, M., & Vechev, M. (2023). "Prompting Is Programming: A Query Language for Large Language Models." *PLDI 2023*, ACM, 1507-1532.

[41] Khattab, O. et al. (2023). "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines." arXiv:2310.03714. *ICLR 2024*.

[42] Zheng, L. et al. (2024). "SGLang: Efficient Execution of Structured Language Model Programs." arXiv:2312.07104.

[43] Yao, S. et al. (2023). "ReAct: Synergizing Reasoning and Acting in Language Models." *ICLR 2023*.

[44] Shinn, N. et al. (2023). "Reflexion: Language Agents with Verbal Reinforcement Learning." *NeurIPS 2023*.

### ProgramaciÃ³n orientada a objetivos y planificaciÃ³n

[45] Fikes, R.E. & Nilsson, N.J. (1971). "STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving." *Artificial Intelligence*, 2(3-4):189-208.

[46] Nilsson, N.J. (1994). "Teleo-Reactive Programs for Agent Control." *JAIR*, 1:139-158.

[47] Nau, D. et al. (2003). "SHOP2: An HTN Planning System." *JAIR*, 20:379-404.

### VerificaciÃ³n en runtime y diseÃ±o por contrato

[48] Meyer, B. (1992). "Applying 'Design by Contract'." *IEEE Computer*, 25(10):40-51.

[49] Leucker, M. & Schallhart, C. (2009). "A Brief Account of Runtime Verification." *Journal of Logic and Algebraic Programming*, 78(5):293-303.

[50] Ernst, M.D. et al. (2007). "The Daikon System for Dynamic Detection of Likely Invariants." *Science of Computer Programming*, 69(1-3):35-45.

### Surveys y trabajo fundacional

[51] Wooldridge, M. & Jennings, N.R. (1995). "Intelligent Agents: Theory and Practice." *Knowledge Engineering Review*, 10(2):115-152.

[52] Wang, L. et al. (2024). "A Survey on Large Language Model Based Autonomous Agents." *Frontiers of Computer Science*.

[53] Schmidhuber, J. (2003). "Goedel Machines: Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements." Technical Report IDSIA-19-03.

[54] Hicks, M. & Nettles, S. (2005). "Dynamic Software Updating." *ACM TOPLAS*, 27(6):1049-1096.

[55] Gat, E. (1998). "On Three-Layer Architectures." In *Artificial Intelligence and Mobile Robots*, MIT Press, 195-210.

### ProgramaciÃ³n lÃ³gica y backtracking

[56] Colmerauer, A. & Roussel, P. (1993). "The Birth of Prolog." *History of Programming Languages II*, ACM, 331-367.

[57] Lloyd, J.W. (1987). *Foundations of Logic Programming* (2nd ed.). Springer-Verlag.

[58] Sterling, L. & Shapiro, E. (1994). *The Art of Prolog* (2nd ed.). MIT Press.

### Reversible computing y exploraciÃ³n de estados

[59] Landauer, R. (1961). "Irreversibility and Heat Generation in the Computing Process." *IBM Journal of Research and Development*, 5(3):183-191.

[60] Bennett, C.H. (1973). "Logical Reversibility of Computation." *IBM Journal of Research and Development*, 17(6):525-532.

[61] Clarke, E.M., Grumberg, O., & Peled, D.A. (1999). *Model Checking*. MIT Press.

### OrÃ¡culos y computabilidad

[62] Turing, A.M. (1939). "Systems of Logic Based on Ordinals." *Proceedings of the London Mathematical Society*, s2-45(1):161-228.

[63] Rogers, H. (1967). *Theory of Recursive Functions and Effective Computability*. McGraw-Hill.

### SemÃ¡ntica operacional

[64] Plotkin, G.D. (1981). "A Structural Approach to Operational Semantics." Technical Report DAIMI FN-19, Aarhus University.

[65] Wright, A.K. & Felleisen, M. (1994). "A Syntactic Approach to Type Soundness." *Information and Computation*, 115(1):38-94.

---

*AURA estÃ¡ implementado en Rust con 244+ tests, incluyendo auto-reparaciÃ³n cognitiva funcional. CÃ³digo fuente disponible en el repositorio del proyecto.*
