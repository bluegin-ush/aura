# AURA: Incorporaci√≥n de deliberaci√≥n cognitiva en la sem√°ntica de lenguajes de programaci√≥n

## Un reporte t√©cnico sobre Agent-Unified Runtime Architecture

---

**Resumen.** Presentamos AURA (Agent-Unified Runtime Architecture), un lenguaje de programaci√≥n que incorpora un ciclo de deliberaci√≥n cognitiva---observar, razonar, ajustar, continuar---directamente en su sem√°ntica de ejecuci√≥n. A diferencia de los enfoques existentes donde los modelos de lenguaje grande (LLMs) operan como herramientas externas de generaci√≥n de c√≥digo (GitHub Copilot, ChatRepair) o donde la auto-reparaci√≥n opera a nivel de sistemas (MAPE-K, Rainbow), AURA introduce *primitivas cognitivas* (`goal`, `observe`, `expect`, `invariant`, `reason`) como construcciones sint√°cticas de primera clase que se parsean en nodos AST con reglas de evaluaci√≥n definidas. Cuando un programa se ejecuta bajo el runtime cognitivo de AURA, la m√°quina virtual puede pausar en puntos arbitrarios de ejecuci√≥n, reificar el contexto completo de ejecuci√≥n (variables, goals, invariantes, historial de observaciones, checkpoints), despachar a un LLM para deliberaci√≥n, y resumir con una de cinco intervenciones estructuralmente tipadas: continuar, inyecci√≥n de valor, parcheo de c√≥digo, backtracking basado en checkpoints con ajustes, o detenci√≥n. Los goals e invariantes declarados por el desarrollador restringen todas las modificaciones generadas por el LLM, creando un espacio de adaptaci√≥n formalmente acotado. Hasta donde sabemos, AURA es el primer lenguaje de programaci√≥n donde (1) la deliberaci√≥n cognitiva es parte de la sem√°ntica operacional, (2) un LLM participa como componente del runtime con acceso al estado de ejecuci√≥n en vivo, y (3) los invariantes declarados por el desarrollador imponen restricciones de seguridad sobre las modificaciones de programa generadas por IA a nivel del lenguaje.

---

## 1. Introducci√≥n

### 1.1 La brecha paradigm√°tica

Tres comunidades de investigaci√≥n han desarrollado independientemente soluciones al problema de construir software que se adapte a condiciones inesperadas:

**Programaci√≥n orientada a agentes** (Shoham 1993; Rao 1996; Bordini et al. 2007) introdujo actitudes mentales---creencias, deseos, intenciones---como primitivas de programaci√≥n. Lenguajes como AgentSpeak/Jason, GOAL y 2APL implementan la arquitectura BDI (Belief-Desire-Intention) con razonamiento expl√≠cito sobre objetivos y manejo de fallos en planes. Sin embargo, estos lenguajes son anteriores a la era de los LLM: su "razonamiento" es b√∫squeda en biblioteca de planes, no deliberaci√≥n abierta.

**Reparaci√≥n autom√°tica de programas** (Le Goues et al. 2012; Xia & Zhang 2023; Long & Rinard 2016) desarroll√≥ t√©cnicas para corregir errores autom√°ticamente, desde reparaci√≥n basada en b√∫squeda (GenProg) hasta reparaci√≥n conversacional con LLM (ChatRepair). Estos sistemas logran resultados impresionantes en benchmarks, pero todos operan *post-mortem*: el programa primero debe fallar, producir un fallo de test o mensaje de error, y luego una herramienta externa propone un parche. Ninguna herramienta APR tiene acceso al estado de ejecuci√≥n en vivo.

**Sistemas auto-adaptativos** (Kephart & Chess 2003; Garlan et al. 2004; Weyns et al. 2012) formalizaron el ciclo MAPE-K (Monitorear-Analizar-Planificar-Ejecutar sobre Conocimiento compartido) para computaci√≥n aut√≥noma. Sistemas como Rainbow detectan violaciones de restricciones arquitect√≥nicas y aplican estrategias de reparaci√≥n predefinidas. Estos operan a nivel de infraestructura, no a nivel de lenguaje de programaci√≥n.

A pesar de d√©cadas de progreso en cada comunidad, persiste una brecha fundamental: **ning√∫n lenguaje de programaci√≥n existente integra deliberaci√≥n cognitiva---la capacidad de pausar la ejecuci√≥n, razonar sobre el estado actual contra intenciones declaradas, y elegir entre intervenciones estructuralmente diversas---en su sem√°ntica de ejecuci√≥n.**

### 1.2 La s√≠ntesis

AURA cierra esta brecha sintetizando ideas de las tres tradiciones en un solo dise√±o de lenguaje:

| Origen | Concepto | Realizaci√≥n en AURA |
|--------|---------|------------------|
| Arquitecturas BDI | Goals como actitudes mentales de primera clase | `goal "descripci√≥n" check expr` --- goals con expresiones de verificaci√≥n evaluadas en runtime |
| Verificaci√≥n en runtime | Monitoreo continuo de propiedades | `observe variable` --- declara puntos de monitoreo en runtime |
| Dise√±o por contrato | Precondiciones e invariantes | `invariant expr` --- restricciones que acotan todas las adaptaciones |
| Ciclo MAPE-K | Ciclo Monitorear-Analizar-Planificar-Ejecutar | `observe` -> `deliberate()` -> `CognitiveDecision` -> aplicar |
| Checkpoint/rollback | Gesti√≥n transaccional de estado | `CheckpointManager` --- snapshots nombrados con restauraci√≥n y ajustes |
| Frameworks de agentes LLM | Razonamiento potenciado por LLM | `reason "pregunta"` --- deliberaci√≥n expl√≠cita con inyecci√≥n de valores |

El resultado es un lenguaje donde el modelo de ejecuci√≥n cambia fundamentalmente:

```mermaid
graph LR
    subgraph "Modelo v1 ‚Äî ejecuci√≥n tradicional"
        A1[parsear] --> B1[ejecutar] --> C1[fallar] --> D1[reparar] --> E1[re-ejecutar]
    end
```

```mermaid
graph LR
    subgraph "Modelo v2 ‚Äî ejecuci√≥n cognitiva"
        A2[parsear] --> B2[ejecutar] --> C2[observar] --> D2[razonar] --> E2[ajustar] --> F2[continuar]
        F2 -.-> C2
    end
```

### 1.3 Contribuciones

Este reporte hace las siguientes afirmaciones, cada una respaldada por evidencia de implementaci√≥n y posicionada contra la literatura relevada:

1. **Goals como expresiones de runtime evaluadas continuamente** (Secci√≥n 3.1). Ning√∫n lenguaje BDI existente trata los goals como expresiones en el lenguaje anfitri√≥n evaluadas durante la ejecuci√≥n. El `GoalDef.check: Option<Expr>` de AURA permite monitoreo continuo de goals a granularidad arbitraria, distinto de los √°tomos simb√≥licos de AgentSpeak, las f√≥rmulas l√≥gicas de GOAL, y los maintain goals basados en callbacks de Jadex.

2. **Deliberaci√≥n cognitiva como sem√°ntica del lenguaje** (Secci√≥n 3.2). Ning√∫n lenguaje existente define la deliberaci√≥n como una operaci√≥n sem√°ntica que puede modificar el estado de ejecuci√≥n, reescribir c√≥digo, o hacer backtrack con ajustes. El trait `CognitiveRuntime` (`observe`, `deliberate`, `check_goals`, `is_active`) es invocado por la VM durante la evaluaci√≥n de expresiones, no como una capa de monitoreo externa.

3. **√Ålgebra de intervenci√≥n de cinco modos** (Secci√≥n 3.3). El enum `CognitiveDecision` define cinco intervenciones estructuralmente tipadas (`Continue`, `Override(Value)`, `Fix{new_code, explanation}`, `Backtrack{checkpoint, adjustments}`, `Halt(error)`), proporcionando un espacio de intervenci√≥n m√°s rico que cualquier sistema de auto-reparaci√≥n existente.

4. **Adaptaci√≥n acotada por invariantes** (Secci√≥n 3.4). Los invariantes y goals declarados por el desarrollador restringen todas las modificaciones generadas por el LLM. La funci√≥n `validate_fix()` verifica que los fixes sean parseables, respeten l√≠mites de tama√±o, preserven todos los goals declarados, y no introduzcan goals nuevos. Este es un patr√≥n de dise√±o novedoso: restricciones declaradas por el desarrollador sobre la modificaci√≥n automatizada de programas.

5. **Abstracci√≥n cognitiva de cero overhead** (Secci√≥n 3.5). Cuando `is_active()` retorna `false` (el `NullCognitiveRuntime`), todas las verificaciones cognitivas son no-ops. Los programas sin caracter√≠sticas cognitivas se ejecutan con rendimiento id√©ntico al de un runtime no cognitivo.

---

## 2. Trabajo relacionado

### 2.1 Lenguajes de programaci√≥n orientados a agentes

**AgentSpeak(L)** (Rao 1996) introdujo el modelo de programaci√≥n BDI dominante: los agentes tienen creencias (hechos tipo Prolog), eventos disparadores activan planes de una biblioteca de planes, y las intenciones son pilas de planes parcialmente ejecutados. **Jason** (Bordini et al. 2007) es la implementaci√≥n m√°s completa, a√±adiendo actos de habla, entornos y abstracciones organizacionales. Los goals en AgentSpeak son √°tomos simb√≥licos (`!achieve_goal`) que disparan selecci√≥n de planes; el fallo causa abandono de intenci√≥n o re-planificaci√≥n dentro de la biblioteca de planes.

**GOAL** (Hindriks 2009) usa goals declarativos expresados como f√≥rmulas l√≥gicas. La base de goals de un agente se actualiza mediante un ciclo de deliberaci√≥n que eval√∫a goals contra creencias. GOAL es el trabajo previo m√°s cercano al modelo de goals activos de AURA, pero sus goals son f√≥rmulas l√≥gicas en un lenguaje de consulta de creencias separado, no expresiones en el lenguaje anfitri√≥n.

**2APL** (Dastani 2008) introduce *reglas de razonamiento pr√°ctico* (PR-rules) que revisan planes cuando las condiciones cambian. Cuando un plan falla, las PR-rules hacen matching con el contexto de fallo y generan planes revisados. Este es el mecanismo de re-planificaci√≥n m√°s sofisticado en la literatura AOPL, pero opera sobre mapeos regla-plan predefinidos, no deliberaci√≥n abierta con LLM.

**Jadex** (Pokahr et al. 2005) a√±ade *maintain goals* al modelo BDI: condiciones que deben permanecer verdaderas, con re-activaci√≥n autom√°tica de planes cuando se violan. Esto es estructuralmente similar al `goal ... check expr` de AURA, pero las condiciones maintain de Jadex son predicados Java registrados como callbacks, no expresiones en el lenguaje del agente mismo.

**SARL** (Rodriguez et al. 2014) introduce un modelo de capacidad/habilidad donde los agentes declaran capacidades requeridas y vinculan implementaciones en runtime. Esto es arquitect√≥nicamente similar al sistema de capacidades de AURA (`+http`, `+json`, `+db`).

**La brecha.** Ning√∫n lenguaje BDI existente trata los goals como expresiones evaluadas continuamente en el sistema de expresiones del lenguaje anfitri√≥n. La Tabla 1 resume la distinci√≥n:

*Tabla 1: Representaci√≥n de goals a trav√©s de lenguajes orientados a agentes*

| Lenguaje | Representaci√≥n del goal | Momento de evaluaci√≥n | Respuesta ante fallo |
|----------|-------------------|-------------------|-----------------|
| AgentSpeak | √Åtomo simb√≥lico (`!g`) | Al dispararse | Abandonar intenci√≥n |
| GOAL | F√≥rmula l√≥gica | Por ciclo de deliberaci√≥n | Re-seleccionar plan |
| Jadex | Predicado Java (callback) | Al callback | Re-activar plan |
| 2APL | F√≥rmula l√≥gica | Por ciclo, PR-rules | Revisi√≥n basada en reglas |
| **AURA** | **Expresi√≥n del lenguaje anfitri√≥n** | **Continua, por paso** | **Deliberaci√≥n cognitiva + backtrack** |

### 2.2 Reparaci√≥n autom√°tica de programas

**GenProg** (Le Goues et al. 2012) fue pionero en la reparaci√≥n automatizada de programas basada en b√∫squeda usando programaci√≥n gen√©tica para evolucionar parches. **SemFix** (Nguyen et al. 2013) y **Angelix** (Mechtaev et al. 2016) introdujeron reparaci√≥n a nivel sem√°ntico usando ejecuci√≥n simb√≥lica y resoluci√≥n de restricciones. **Prophet** (Long & Rinard 2016) aprendi√≥ modelos de correcci√≥n de c√≥digo a partir de parches humanos para rankear candidatos.

La era de los LLM transform√≥ el campo. **ChatRepair** (Xia & Zhang 2023) usa interacci√≥n conversacional con LLM para corregir 162/337 bugs de Defects4J a ~$0.42 por bug. **RepairLLaMA** (Silva et al. 2023) hace fine-tuning de LLMs open-source con adaptadores LoRA para reparaci√≥n. **AlphaRepair** (Xia & Zhang 2022) demostr√≥ que modelos de c√≥digo pre-entrenados pueden realizar reparaci√≥n zero-shot tratando c√≥digo con errores como un problema de modelo de lenguaje enmascarado.

**La limitaci√≥n post-mortem.** Todas las herramientas APR---cl√°sicas y basadas en LLM---comparten una arquitectura fundamental:

```
[Programa falla] ‚Üí [Extraer c√≥digo + error] ‚Üí [Enviar a herramienta] ‚Üí [Obtener parche] ‚Üí [Aplicar] ‚Üí [Re-ejecutar]
```

Ninguna tiene acceso al estado de ejecuci√≥n en vivo. Ninguna puede inyectar valores a mitad de ejecuci√≥n. Ninguna puede hacer backtrack a un checkpoint con ajustes. La herramienta de reparaci√≥n nunca ve qu√© variables ten√≠an qu√© valores en el momento del fallo, qu√© goals pretend√≠a el desarrollador (m√°s all√° de aserciones de test), o el camino de ejecuci√≥n que llev√≥ al error.

### 2.3 Sistemas auto-adaptativos

**Computaci√≥n aut√≥noma** (Kephart & Chess 2003) propuso la arquitectura de referencia MAPE-K: Monitorear (recolectar datos v√≠a sensores), Analizar (determinar si se necesita adaptaci√≥n), Planificar (seleccionar estrategia), Ejecutar (aplicar v√≠a efectores), sobre Conocimiento compartido. **Rainbow** (Garlan et al. 2004) implementa MAPE-K a nivel arquitect√≥nico, monitoreando sistemas en ejecuci√≥n contra restricciones y aplicando estrategias de reparaci√≥n predefinidas.

**FORMS** (Weyns et al. 2012) proporciona un modelo de referencia formal para sistemas auto-adaptativos con sem√°ntica rigurosa para el sistema gestionado, entorno, goals de adaptaci√≥n, y ciclo de retroalimentaci√≥n.

**La limitaci√≥n de capa externa.** Todas las implementaciones MAPE-K a√±aden monitoreo y adaptaci√≥n como una capa arquitect√≥nica externa. El sistema gestionado es una caja negra observada a trav√©s de sondas. Las estrategias de adaptaci√≥n son configuraciones predefinidas, no modificaciones de c√≥digo generadas en runtime. La l√≥gica de adaptaci√≥n est√° separada de la l√≥gica del programa.

### 2.4 Arquitecturas cognitivas

**Soar** (Laird et al. 1987; Newell 1990) implementa un sistema de producci√≥n con sub-goalificaci√≥n universal: cuando ninguna producci√≥n se dispara, un *impasse* activa la creaci√≥n autom√°tica de sub-goals. El mecanismo de *chunking* de Soar aprende nuevas producciones a partir de la resoluci√≥n de sub-goals, creando un ciclo de aprendizaje. **ACT-R** (Anderson & Lebiere 1998; Anderson et al. 2004) modela la cognici√≥n como la interacci√≥n de buffers modulares (visual, motor, memoria declarativa, buffer de goals) mediados por reglas de producci√≥n. **CLARION** (Sun 2016) modela expl√≠citamente la interacci√≥n entre conocimiento impl√≠cito (subsimb√≥lico) y expl√≠cito (simb√≥lico). **LIDA** (Franklin et al. 2014) implementa la Teor√≠a del Espacio de Trabajo Global con un mecanismo de difusi√≥n similar a la consciencia.

**La relevancia.** El runtime cognitivo de AURA implementa un ciclo que mapea directamente a componentes de arquitecturas cognitivas:

| Componente cognitivo | Implementaci√≥n en AURA |
|---|---|
| Percepci√≥n | `observe()` --- detecci√≥n de eventos durante la ejecuci√≥n |
| Memoria de trabajo | Buffer de observaciones + contexto de ejecuci√≥n actual |
| Deliberaci√≥n | `deliberate()` --- invocaci√≥n del LLM con contexto empaquetado |
| Decisi√≥n | Enum `CognitiveDecision` --- cinco tipos de intervenci√≥n |
| Acci√≥n | Hot reload, inyecci√≥n de valor, restauraci√≥n de checkpoint |
| Aprendizaje | Traza de `ReasoningEpisode` + persistencia en `HealingMemory` |
| Metacognici√≥n | `CognitiveSafetyConfig` --- l√≠mites de seguridad sobre el comportamiento de razonamiento |

Esto convierte al runtime de AURA en una arquitectura cognitiva en s√≠ misma, en lugar de un lenguaje usado para *implementar* una arquitectura cognitiva---una distinci√≥n sin precedentes en la literatura.

### 2.5 Arquitecturas reflectivas y de meta-nivel

**3-Lisp de Smith** (Smith 1984) introdujo la reflexi√≥n computacional: un programa que puede inspeccionar y modificar su propia ejecuci√≥n. **CLOS MOP** (Kiczales et al. 1991) proporcion√≥ un protocolo de meta-objetos que permite a los programas personalizar su propio sistema de clases. **Programaci√≥n orientada a aspectos** (Kiczales et al. 1997) introdujo puntos de uni√≥n donde preocupaciones transversales pueden interceptar la ejecuci√≥n.

**Efectos algebraicos** (Plotkin & Pretnar 2009; Bauer & Pretnar 2015) proporcionan el modelo formal m√°s cercano: las computaciones pueden "ceder" efectos a handlers que los inspeccionan y reanudan. El puente cognitivo de AURA puede formalizarse como un handler de efectos algebraicos donde el efecto es "necesito asistencia cognitiva" y el handler es el LLM. La diferencia clave: los handlers de efectos algebraicos se definen est√°ticamente; el "handler" de AURA genera respuestas novedosas din√°micamente.

**El sistema de condiciones/restarts de Common Lisp** es el precedente cl√°sico m√°s cercano a la intervenci√≥n a mitad de ejecuci√≥n de AURA. Cuando un error se√±ala una condici√≥n, los handlers pueden elegir entre restarts predefinidos (ej., `use-value`, `store-value`, `abort`). AURA generaliza esto: en lugar de restarts definidos por el programador, el LLM genera intervenciones novedosas informadas por el contexto de runtime, goals e invariantes.

### 2.6 Sistemas de programaci√≥n integrados con LLM

**LMQL** (Beurer-Kellner et al. 2023) es la comparaci√≥n m√°s relevante como lenguaje de programaci√≥n real (publicado en PLDI) que extiende Python con generaci√≥n restringida de LLM. LMQL compila a m√°scaras a nivel de token para decodificaci√≥n restringida. Sin embargo, se enfoca en restricciones en tiempo de generaci√≥n, no en razonamiento de agentes---no tiene goals, observaci√≥n, auto-reparaci√≥n, ni runtime cognitivo.

**DSPy** (Khattab et al. 2023) introduce especificaciones declarativas de programas LLM con optimizaci√≥n autom√°tica de prompts. **SGLang** (Zheng et al. 2024) optimiza la ejecuci√≥n de programas LLM estructurados con RadixAttention. Ambos est√°n embebidos en Python y se enfocan en la eficiencia de llamadas al LLM, no en adaptaci√≥n en runtime.

**ReAct** (Yao et al. 2023) y **Reflexion** (Shinn et al. 2023) implementan ciclos observar-razonar-actuar en agentes LLM, pero como patrones de prompt, no como sem√°ntica del lenguaje.

*Tabla 2: Sistemas de programaci√≥n integrados con LLM*

| Sistema | ¬øEs un lenguaje? | ¬øLLM como primitiva? | ¬øGoals? | ¬øAuto-reparaci√≥n? | ¬øCiclo en runtime? |
|--------|---------------|-------------------|--------|---------------|---------------|
| LMQL | **S√≠** | S√≠ (generaci√≥n restringida) | No | No | No |
| DSPy | Parcial (DSL en Python) | S√≠ (signatures) | No | Optimizaci√≥n de prompt | No |
| SGLang | Parcial (DSL en Python) | S√≠ (primitivas) | No | No | No |
| LangChain | No (biblioteca) | No (llamada a funci√≥n) | No | No | No |
| ReAct | No (patr√≥n de prompt) | S√≠ (en-prompt) | No | No | S√≠ (ad hoc) |
| **AURA** | **S√≠** | **S√≠** (`reason`) | **S√≠** (`goal check`) | **S√≠** (a nivel de lenguaje) | **S√≠** (integrado en la VM) |

---

## 3. Dise√±o e implementaci√≥n

La siguiente figura muestra la arquitectura general del runtime cognitivo de AURA, desde el c√≥digo fuente hasta la intervenci√≥n del LLM:

```mermaid
graph TB
    subgraph "C√≥digo fuente AURA"
        SRC["goal, observe, expect,<br/>invariant, reason"]
    end

    subgraph "Compilaci√≥n"
        LEX[Lexer<br/>logos] --> PAR[Parser] --> AST[AST<br/>nodos cognitivos]
    end

    subgraph "M√°quina virtual"
        VM[VM<br/>eval + step_count]
        CP[CheckpointManager<br/>snapshots nombrados]
        OBS[observed_vars<br/>HashSet]
        PF[pending_fixes<br/>Vec]
    end

    subgraph "Runtime cognitivo"
        CR["trait CognitiveRuntime<br/>observe() ¬∑ deliberate()<br/>check_goals() ¬∑ is_active()"]
        NULL[NullCognitiveRuntime<br/>cero overhead]
        AGENT["AgentCognitiveRuntime&lt;P&gt;<br/>buffer ¬∑ traza ¬∑ safety"]
    end

    subgraph "Proveedores LLM"
        MOCK[MockProvider]
        CLAUDE[ClaudeProvider]
        OLLAMA[OllamaProvider]
    end

    SRC --> LEX
    AST --> VM
    VM <--> CR
    VM <--> CP
    VM <--> OBS
    VM <--> PF
    CR --- NULL
    CR --- AGENT
    AGENT <--> MOCK
    AGENT <--> CLAUDE
    AGENT <--> OLLAMA
```

### 3.1 Primitivas cognitivas

AURA introduce seis construcciones que forman su vocabulario cognitivo. Estas se parsean en nodos AST---son parte de la gram√°tica del lenguaje, no funciones de biblioteca.

#### 3.1.1 `goal`

```
goal "process user data correctly"
goal "all users must have valid names" check users != nil
```

Los goals son declaraciones de nivel superior (`Definition::Goal(GoalDef)`) con una expresi√≥n `check` opcional. La estructura `GoalDef`:

```rust
pub struct GoalDef {
    pub description: String,
    pub check: Option<Expr>,  // El elemento novedoso
    pub span: Span,
}
```

Cuando `check` est√° presente, el goal es *activo*: la VM eval√∫a la expresi√≥n check despu√©s de cambios en variables observadas, despu√©s de retornos de funciones, y en intervalos de pasos configurables. Si la verificaci√≥n eval√∫a a falso, se eleva un `DeliberationTrigger::GoalMisalignment`, invocando el runtime cognitivo.

La palabra clave `check` se parsea como un *soft keyword* (`Ident("check")`), no como un token reservado---preservando compatibilidad hacia atr√°s con programas que usan "check" como identificador.

#### 3.1.2 `observe`

```
observe users
observe response.status
observe data where valid == true
```

`observe` declara un punto de monitoreo en runtime (`Expr::Observe`). Cuando una variable observada cambia de valor, la VM:
1. Crea un checkpoint impl√≠cito (v√≠a `CheckpointManager`)
2. Notifica al runtime cognitivo v√≠a `observe(ObservationEvent::ValueChanged{...})`
3. Dispara la evaluaci√≥n de goals activos

Sin un runtime cognitivo, `observe` es un no-op que retorna nil.

#### 3.1.3 `expect`

```
expect len(users) > 0 "should have users"
```

`expect` es verificaci√≥n de intenci√≥n (`Expr::Expect`). A diferencia de las aserciones que crashean ante un fallo, los expects se registran como `ExpectationFailure` y, cuando un runtime cognitivo est√° activo, disparan `DeliberationTrigger::ExpectFailed`. El runtime puede entonces decidir continuar, sobreescribir el resultado, generar un fix, o hacer backtrack.

#### 3.1.4 `invariant`

```
invariant len(users) > 0
```

Los invariantes (`Definition::Invariant(Expr)`) declaran restricciones que ninguna adaptaci√≥n puede violar. Sirven como la frontera de seguridad del desarrollador: la funci√≥n `validate_fix()` verifica que los fixes propuestos por el LLM no rompan invariantes antes de ser aplicados.

#### 3.1.5 `reason`

```
strategy = reason "we have {len(users)} users, should we process all or filter?"
```

`reason` es un punto de deliberaci√≥n expl√≠cito (`Expr::Reason`). La ejecuci√≥n pausa, la pregunta y las observaciones recientes se env√≠an al runtime cognitivo, y la decisi√≥n del LLM se convierte en el valor de la expresi√≥n. Esto permite *inyecci√≥n de valores*: el LLM puede retornar un valor que se vincula a una variable y se usa en la computaci√≥n subsiguiente.

Sin un runtime cognitivo, `reason` retorna nil.

#### 3.1.6 `@self_heal`

```
@self_heal(max_attempts: 5, mode: "semantic")
process_data(data) = { ... }
```

Anotaci√≥n a nivel de funci√≥n (`SelfHealConfig`) que marca funciones individuales para reparaci√≥n autom√°tica. Configurable con `max_attempts` y `mode` (technical, semantic, auto).

### 3.2 El trait CognitiveRuntime

El trait `CognitiveRuntime` define la interfaz entre la VM y el agente cognitivo:

```rust
pub trait CognitiveRuntime: Send {
    fn observe(&mut self, event: ObservationEvent);
    fn deliberate(&mut self, trigger: DeliberationTrigger) -> CognitiveDecision;
    fn check_goals(&mut self) -> Vec<CognitiveDecision>;
    fn is_active(&self) -> bool;
    fn set_available_checkpoints(&mut self, checkpoints: Vec<String>) {}
}
```

**Eventos de observaci√≥n** (`ObservationEvent`) incluyen `ValueChanged`, `ExpectEvaluated`, `FunctionReturned`, y `CheckpointCreated`. Estos proporcionan al LLM contexto de runtime rico que ninguna herramienta de reparaci√≥n post-mortem puede acceder.

**Disparadores de deliberaci√≥n** (`DeliberationTrigger`) clasifican qu√© provoc√≥ la deliberaci√≥n: `ExpectFailed`, `ExplicitReason`, `TechnicalError`, o `GoalMisalignment`. Esta clasificaci√≥n ayuda al LLM a entender la naturaleza del problema.

El `NullCognitiveRuntime` implementa todas las operaciones como no-ops con `is_active() = false`, proporcionando cero overhead para ejecuci√≥n no cognitiva.

El siguiente diagrama muestra c√≥mo la VM interact√∫a con el trait durante la evaluaci√≥n de expresiones:

```mermaid
sequenceDiagram
    participant VM as VM (eval)
    participant CR as CognitiveRuntime
    participant LLM as Proveedor LLM

    VM->>VM: eval(Expr::Let) ‚Äî variable observada
    VM->>CR: observe(ValueChanged)
    VM->>CR: check_goals()
    CR-->>VM: [Continue]

    VM->>VM: eval(Expr::Expect) ‚Äî falla
    VM->>CR: observe(ExpectEvaluated)
    VM->>CR: deliberate(ExpectFailed)
    CR->>LLM: request con contexto completo
    LLM-->>CR: respuesta
    CR-->>VM: Override(valor) | Fix{...} | Backtrack{...}

    VM->>VM: eval(Expr::Reason)
    VM->>CR: deliberate(ExplicitReason)
    CR->>LLM: pregunta + observaciones + goals
    LLM-->>CR: decisi√≥n
    CR-->>VM: Override(valor_inyectado)
```

### 3.3 El √°lgebra de intervenci√≥n de cinco modos

`CognitiveDecision` define cinco intervenciones estructuralmente tipadas:

```rust
pub enum CognitiveDecision {
    Continue,
    Override(Value),
    Fix { new_code: String, explanation: String },
    Backtrack { checkpoint: String, adjustments: Vec<(String, Value)> },
    Halt(RuntimeError),
}
```

```mermaid
graph TD
    T[Disparador de deliberaci√≥n] --> LLM[LLM delibera]
    LLM --> C{Decisi√≥n}

    C -->|"sin cambios"| CONT[Continue<br/>La ejecuci√≥n sigue normalmente]
    C -->|"inyectar valor"| OVR["Override(Value)<br/>Reemplaza el resultado de la expresi√≥n<br/>con un valor elegido por el LLM"]
    C -->|"parchear c√≥digo"| FIX["Fix{new_code, explanation}<br/>Reescribe el fuente y re-ejecuta<br/>desde el inicio"]
    C -->|"retroceder"| BT["Backtrack{checkpoint, adjustments}<br/>Restaura la VM al checkpoint nombrado<br/>con ajustes en variables"]
    C -->|"detener"| HALT["Halt(error)<br/>Detiene la ejecuci√≥n<br/>con explicaci√≥n"]

    style CONT fill:#d4edda
    style OVR fill:#cce5ff
    style FIX fill:#fff3cd
    style BT fill:#f8d7da
    style HALT fill:#e2e3e5
```

Comparaci√≥n con modelos de intervenci√≥n existentes:

| Intervenci√≥n | Sem√°ntica | Precedente |
|---|---|---|
| `Continue` | Proceder normalmente | Com√∫n (todos los sistemas) |
| `Override(Value)` | Inyectar un valor de reemplazo en el flujo de ejecuci√≥n | Restart `use-value` de Common Lisp, pero elegido por LLM |
| `Fix{new_code, explanation}` | Reescribir c√≥digo fuente; re-ejecutar desde el inicio | Herramientas APR (GenProg, ChatRepair), pero con contexto de runtime |
| `Backtrack{checkpoint, adjustments}` | Restaurar la VM al checkpoint nombrado, aplicar ajustes a variables, continuar desde ese punto | **Sin precedente** en APR o frameworks LLM |
| `Halt(error)` | Detener ejecuci√≥n con explicaci√≥n | Com√∫n (todos los sistemas) |

La intervenci√≥n `Backtrack` es particularmente novedosa. A diferencia de las herramientas APR que deben re-ejecutar desde cero, y a diferencia de la supervisi√≥n de Erlang que reinicia desde el estado inicial, AURA puede restaurar a cualquier checkpoint nombrado *con ajustes*---el LLM especifica qu√© variables modificar antes de reanudar. Esto permite re-ejecuci√≥n parcial con correcciones informadas, una capacidad sin precedentes en la literatura.

### 3.4 Seguridad: adaptaci√≥n acotada por invariantes

La funci√≥n `validate_fix()` impone restricciones de seguridad antes de que cualquier modificaci√≥n propuesta por el LLM sea aplicada:

1. **Restricci√≥n de tama√±o**: los fixes que exceden `max_fix_lines` (por defecto: 50) son rechazados, previniendo reescrituras completas del programa.
2. **Validez sint√°ctica**: cada fix propuesto debe tokenizarse y parsearse como AURA v√°lido.
3. **Inmutabilidad de goals**: el fix debe preservar todos los goals declarados---sin adiciones, sin eliminaciones, sin modificaciones. Los goals son dominio exclusivo del desarrollador.
4. **Profundidad de backtrack**: `max_backtrack_depth` (por defecto: 5) previene ciclos infinitos de backtrack.
5. **Seguimiento de progreso**: `max_deliberations_without_progress` (por defecto: 3) detiene el razonamiento descontrolado.

```rust
pub struct CognitiveSafetyConfig {
    pub max_fix_lines: usize,
    pub max_backtrack_depth: usize,
    pub max_deliberations_without_progress: usize,
}
```

```mermaid
graph TD
    FIX[Fix propuesto por LLM] --> S1{¬øTama√±o ‚â§ 50 l√≠neas?}
    S1 -->|No| REJ1[Rechazado:<br/>fix demasiado grande]
    S1 -->|S√≠| S2{¬øParsea como AURA v√°lido?}
    S2 -->|No| REJ2[Rechazado:<br/>sintaxis inv√°lida]
    S2 -->|S√≠| S3{¬øPreserva todos los goals?}
    S3 -->|No| REJ3[Rechazado:<br/>goals modificados]
    S3 -->|S√≠| S4{¬øNo introduce goals nuevos?}
    S4 -->|No| REJ4[Rechazado:<br/>goals agregados]
    S4 -->|S√≠| OK[Fix aceptado ‚úì<br/>aplicar y re-ejecutar]

    style REJ1 fill:#f8d7da
    style REJ2 fill:#f8d7da
    style REJ3 fill:#f8d7da
    style REJ4 fill:#f8d7da
    style OK fill:#d4edda
```

Esto establece un espacio de adaptaci√≥n formalmente acotado: el LLM puede modificar el programa, pero solo dentro de las restricciones que el desarrollador ha declarado. Este es un patr√≥n de dise√±o novedoso---**restricciones declaradas por el desarrollador sobre la modificaci√≥n automatizada de programas**---que no tiene precedente directo en la literatura de APR o sistemas auto-adaptativos.

### 3.5 Sistema de checkpoints

El `CheckpointManager` mantiene snapshots nombrados del estado de la VM:

```rust
pub struct VMCheckpoint {
    pub name: String,
    pub variables: HashMap<String, Value>,
    pub step_count: u64,
    pub timestamp: Instant,
}
```

Los checkpoints se crean impl√≠citamente (ante disparadores de `observe`, antes de llamadas a funciones) y pueden restaurarse con ajustes:

```mermaid
sequenceDiagram
    participant VM as VM
    participant CP as CheckpointManager
    participant CR as CognitiveRuntime

    VM->>CP: save("fetch_users", variables, step=3)
    Note over CP: { users: [...], count: 3 }

    VM->>VM: ejecuci√≥n contin√∫a...
    VM->>CR: check_goals()
    Note over CR: goal "all users must be active" ‚Üí false

    CR-->>VM: Backtrack{ checkpoint: "fetch_users",<br/>adjustments: [("users", filtered)] }

    VM->>CP: restore("fetch_users")
    CP-->>VM: variables restauradas al paso 3
    VM->>VM: aplica ajustes: users = filtered_list
    VM->>VM: contin√∫a ejecuci√≥n desde paso 3
```

Esto combina ideas de memoria transaccional de software (Shavit & Touitou 1995; Harris et al. 2005), backtracking cronol√≥gico de Prolog, y manejo de fallos de planes BDI, pero la s√≠ntesis---backtracking con ajustes sugeridos por LLM en un ciclo de ejecuci√≥n cognitiva---es nueva.

### 3.6 El AgentCognitiveRuntime

La implementaci√≥n real conecta el trait `CognitiveRuntime` a un `AgentProvider` (soportando m√∫ltiples backends de LLM):

```rust
pub struct AgentCognitiveRuntime<P: AgentProvider> {
    provider: P,
    tokio_handle: Handle,        // puente async-sync
    goals: Vec<GoalDef>,
    invariants: Vec<String>,
    source_code: String,
    observation_buffer: Vec<ObservationEvent>,
    reasoning_trace: Vec<ReasoningEpisode>,
    available_checkpoints: Vec<String>,
    max_deliberations: usize,
    deliberation_count: usize,
    safety_config: CognitiveSafetyConfig,
    consecutive_backtracks: usize,
    deliberations_without_progress: usize,
}
```

Decisiones de dise√±o clave:

- **Puente async-sync**: la VM es s√≠ncrona; el `AgentProvider` es async. `tokio_handle.block_on()` tiende el puente, manteniendo simple la implementaci√≥n de la VM.
- **Agrupaci√≥n de observaciones**: los eventos se acumulan en `observation_buffer` y se drenan despu√©s de cada deliberaci√≥n, proporcionando al LLM contexto acumulativo.
- **Memoria epis√≥dica**: `reasoning_trace: Vec<ReasoningEpisode>` registra cada episodio de deliberaci√≥n, incluido en solicitudes subsiguientes para que el LLM pueda aprender de la historia reciente.
- **Fail-open**: si el provider falla (error de red, timeout), el runtime retorna `Continue` en lugar de crashear. La capa cognitiva nunca hace al programa *menos* confiable.

### 3.7 El runner de ejecuci√≥n cognitiva

La funci√≥n `run_cognitive()` orquesta el ciclo de reintentos:

```rust
pub fn run_cognitive(
    source: &str,
    cognitive: Box<dyn CognitiveRuntime>,
    max_retries: usize,
) -> Result<CognitiveRunResult, RuntimeError>
```

```mermaid
graph TD
    START[C√≥digo fuente] --> PARSE[Parsear fuente actual]
    PARSE --> CREATE[Crear VM con CognitiveRuntime]
    CREATE --> RUN[Ejecutar programa]

    RUN --> CHECK{¬øHay pending_fixes?}
    CHECK -->|S√≠| VAL[Validar fix v√≠a validate_fix]
    VAL --> APPLY[Aplicar fix al fuente]
    APPLY --> RETRY{¬øQuedan reintentos?}
    RETRY -->|S√≠| PARSE
    RETRY -->|No| ERR[Retornar error]

    CHECK -->|No| RESULT{¬øResultado OK?}
    RESULT -->|S√≠| OK[Retornar CognitiveRunResult<br/>valor + fixes aplicados + reintentos]
    RESULT -->|No| RETRY

    style OK fill:#d4edda
    style ERR fill:#f8d7da
```

Para cada intento:
1. Parsear el c√≥digo fuente actual
2. Crear la VM con runtime cognitivo (primer intento) o `NullCognitiveRuntime` (reintentos)
3. Cargar y ejecutar el programa
4. Si existen `pending_fixes`, validar cada fix v√≠a `validate_fix()`, aplicar el v√°lido, y reintentar
5. Si la ejecuci√≥n tiene √©xito sin fixes pendientes, retornar el resultado
6. Si se agotaron los reintentos, retornar el error

Crucialmente, las decisiones `Backtrack` se manejan *dentro* de una sola ejecuci√≥n (son restauraciones de estado en l√≠nea), mientras que las decisiones `Fix` requieren re-parseo y re-ejecuci√≥n. Esta adaptaci√≥n de doble nivel---backtrack en l√≠nea para correcciones r√°pidas, re-ejecuci√≥n completa para cambios estructurales---proporciona una flexibilidad sin igual en sistemas de estrategia √∫nica.

---

## 4. Posicionamiento frente al estado del arte

### 4.1 Comparaci√≥n integral

*Tabla 3: AURA posicionado contra sistemas representativos de cada l√≠nea de investigaci√≥n*

| Dimensi√≥n | GenProg | ChatRepair | LangChain | Rainbow | Jason | AURA v2.0 |
|---|---|---|---|---|---|---|
| **Naturaleza** | Herramienta APR | Herramienta de reparaci√≥n LLM | Orquestaci√≥n LLM | Framework auto-adaptativo | Lenguaje BDI | **Lenguaje cognitivo** |
| **Cu√°ndo ocurre la reparaci√≥n** | Post-mortem | Post-mortem | N/A | Runtime (externo) | Fallo de plan | **A mitad de ejecuci√≥n (en la VM)** |
| **Acceso al estado de runtime** | Ninguno | Ninguno | Ninguno | M√©tricas arquitect√≥nicas | Base de creencias | **Completo: variables, goals, checkpoints** |
| **Or√°culo de reparaci√≥n** | Suite de tests | Suite de tests + LLM | N/A | Estrategias predefinidas | Biblioteca de planes | **Goals + expects + invariantes + LLM** |
| **Inyecci√≥n de valores** | No | No | No | No | Actualizaci√≥n de creencias | **S√≠ (`Override`)** |
| **Backtracking** | No | No | No | No | Pila de intenciones | **S√≠ (checkpoint + ajustes)** |
| **Parcheo de c√≥digo** | S√≠ (fuente) | S√≠ (fuente) | N/A | S√≠ (config) | No | **S√≠ (fuente validado)** |
| **Restricciones de seguridad** | Solo suite de tests | Ninguna | N/A | Por construcci√≥n | Ninguna | **Invariantes + inmutabilidad de goals** |
| **Intenci√≥n del desarrollador** | Casos de test | Casos de test | C√≥digo Python | Restricciones arq. | Goals BDI | **`goal`, `expect`, `invariant`** |
| **Integraci√≥n LLM** | Ninguna | API externa | API externa | Ninguna | Ninguna | **Trait de runtime de primera clase** |

### 4.2 La brecha tripartita

AURA cierra una brecha en la intersecci√≥n de tres preocupaciones previamente separadas:

```mermaid
graph TD
    subgraph " "
        A["Programaci√≥n orientada<br/>a agentes (BDI)"]
        B["Reparaci√≥n autom√°tica<br/>de programas (APR)"]
        C["Sistemas<br/>auto-adaptativos (MAPE-K)"]

        A ---|"goals + intenciones<br/>pero sin LLM"| AB[ ]
        B ---|"reparaci√≥n con LLM<br/>pero post-mortem"| BC[ ]
        C ---|"monitoreo en runtime<br/>pero capa externa"| CA[ ]

        AB --- AURA
        BC --- AURA
        CA --- AURA

        AURA["üî∑ AURA v2.0<br/>Goals evaluados continuamente<br/>+ LLM con estado en vivo<br/>+ invariantes a nivel de lenguaje"]
    end

    style AURA fill:#4a90d9,color:#fff
    style AB fill:none,stroke:none
    style BC fill:none,stroke:none
    style CA fill:none,stroke:none
```

1. **Ning√∫n lenguaje actual** proporciona construcciones integradas para expresar la intenci√≥n del desarrollador (`goal`), expectativas de runtime (`expect`), monitoreo de variables (`observe`), puntos seguros de rollback (checkpoints), y solicitudes expl√≠citas de razonamiento (`reason`) como sintaxis de primera clase.

2. **Ning√∫n sistema actual** da a un LLM acceso al estado de ejecuci√≥n en vivo (valores de variables, camino de ejecuci√≥n, resultados de evaluaci√≥n de goals) durante la ejecuci√≥n del programa, permitiendo decisiones a mitad de ejecuci√≥n (inyecci√≥n de valores, parcheo de c√≥digo, backtracking basado en checkpoints).

3. **Ning√∫n sistema actual** impone invariantes de seguridad sobre las adaptaciones generadas por LLM a nivel del lenguaje---donde los invariantes se declaran en la sintaxis del programa, se validan por el parser, y se aplican antes de que cualquier fix propuesto por el LLM sea aplicado.

### 4.3 Afirmaci√≥n formal de novedad

> AURA incorpora un ciclo cognitivo (observar-razonar-ajustar-continuar) como un mecanismo de runtime de primera clase dentro de la sem√°ntica de ejecuci√≥n del lenguaje, donde (1) la fase "razonar" invoca un modelo de lenguaje grande externo con contexto de ejecuci√≥n reificado, (2) la fase "ajustar" aplica modificaciones verificadas en tipo al c√≥digo en ejecuci√≥n, y (3) el ciclo opera a granularidad arbitraria---desde expresiones individuales hasta cuerpos de funciones completos---en lugar de solo en fronteras de proceso (Erlang), fronteras de transacci√≥n (STM), o puntos de uni√≥n predefinidos (AOP).

### 4.4 Lo que no es novedoso

La honestidad acad√©mica requiere identificar sobre qu√© construye AURA en lugar de inventar:

- La arquitectura BDI (Rao & Georgeff 1991, 1995; Bratman 1987)
- Ciclos auto-adaptativos MAPE-K (Kephart & Chess 2003)
- Mecanismos de checkpoint/rollback (Shavit & Touitou 1995)
- Reparaci√≥n de c√≥digo basada en LLM (Xia & Zhang 2023; Le Goues et al. 2012)
- Hot code reloading (Armstrong 2003)
- Sistemas de m√≥dulos basados en capacidades (cf. modelo de capacidades de SARL)
- Sistemas de condiciones/restarts (Common Lisp)
- Verificaci√≥n en runtime (Leucker & Schallhart 2009)

La contribuci√≥n de AURA es la *s√≠ntesis*: integrar todo lo anterior en un mecanismo de runtime unificado dise√±ado desde cero para esta interacci√≥n.

---

## 5. Ejemplo desarrollado

El siguiente programa AURA demuestra la ejecuci√≥n cognitiva. Los comentarios anotan lo que el runtime cognitivo hace en cada punto.

```aura
# Demo del runtime cognitivo
# Ejecutar con: aura run --cognitive --provider mock examples/cognitive_demo.aura

+http +json

# El desarrollador declara su intenci√≥n
goal "process user data correctly"
goal "all users must have valid names" check users != nil

# Definici√≥n de tipo con anotaciones de validaci√≥n
@User {
    id :i
    name :s
    email :s
}

# Funci√≥n de fuente de datos
fetch_users() = [{id: 1, name: "Alice", email: "alice@example.com"}, {id: 2, name: "Bob", email: "bob@example.com"}, {id: 3, name: "Charlie", email: "charlie@example.com"}]

# Funci√≥n de formateo
format_user(user) = "User {user.id}: {user.name} <{user.email}>"

main = : users = fetch_users(); observe users; expect len(users) > 0 "should have users"; strategy = reason "we have users, should we process all or filter?"; first_user = first(users); format_user(first_user)
```

### 5.1 Traza de ejecuci√≥n bajo runtime cognitivo

Al ejecutarse con `aura run --cognitive --provider claude examples/cognitive_demo.aura`:

```mermaid
sequenceDiagram
    participant P as Parser
    participant VM as VM
    participant CP as Checkpoints
    participant CR as CognitiveRuntime
    participant LLM as LLM

    P->>VM: cargar programa (goals, funciones, main)
    Note over VM: Goals registrados, CheckpointManager inicializado

    VM->>VM: eval: users = fetch_users()
    Note over VM: users = List[3 registros User]

    VM->>CP: save("observe_users", vars, step=3)
    VM->>CR: observe(ValueChanged{users: Nil ‚Üí List[...]})
    VM->>CR: check_goals()
    Note over CR: "users != nil" ‚Üí true ‚úì

    VM->>VM: eval: expect len(users) > 0
    Note over VM: condici√≥n: true ‚úì
    VM->>CR: observe(ExpectEvaluated{result: true})

    VM->>CR: deliberate(ExplicitReason{question: "..."})
    CR->>LLM: pregunta + goals + observaciones
    LLM-->>CR: Override("process_all")
    CR-->>VM: strategy = "process_all"

    VM->>VM: eval: first_user = first(users)
    VM->>VM: eval: format_user(first_user)
    Note over VM: "User 1: Alice <alice@example.com>"
```

### 5.2 Contrafactual: escenario de desalineaci√≥n de goal

Supongamos que `fetch_users()` retorn√≥ una lista incluyendo un usuario con `name: nil`. El goal `check users != nil` a√∫n pasar√≠a (la lista misma no es nil), pero imaginemos un goal m√°s preciso:

```aura
goal "all names must be non-empty" check for(u in users) : u.name != nil
```

Cuando la verificaci√≥n del goal falla:

```mermaid
sequenceDiagram
    participant VM as VM
    participant CP as Checkpoints
    participant CR as CognitiveRuntime
    participant LLM as LLM

    VM->>CP: save("observe_users", vars, step=3)
    VM->>CR: check_goals()
    Note over CR: "for(u in users): u.name != nil" ‚Üí false ‚úó

    CR->>LLM: GoalMisalignment + contexto + checkpoints
    LLM-->>CR: Backtrack{checkpoint: "observe_users",<br/>adjustments: [("users", lista_filtrada)]}

    CR-->>VM: Backtrack{...}
    VM->>CP: restore("observe_users")
    Note over VM: variables restauradas al paso 3
    VM->>VM: aplica ajuste: users = [solo v√°lidos]
    VM->>VM: contin√∫a ejecuci√≥n con datos corregidos ‚úì
```

Este es el poder del backtracking basado en checkpoints con ajustes informados por LLM: el programa no crashea, no reinicia desde cero, y no aplica una estrategia predefinida fr√°gil. El LLM entiende el goal ("all names must be non-empty"), examina los datos, y propone una correcci√≥n dirigida.

---

## 6. Discusi√≥n

### 6.1 Enmarcamiento te√≥rico

El runtime cognitivo de AURA puede formalizarse a trav√©s de m√∫ltiples lentes te√≥ricos:

**Como un handler de efectos algebraicos** (Plotkin & Pretnar 2009): las primitivas cognitivas (`observe`, `reason`, `expect` ante fallo) son efectos cedidos por la computaci√≥n. El `CognitiveRuntime` es el handler que interpreta estos efectos. La novedad clave: el handler no es una funci√≥n definida est√°ticamente sino un LLM que razona din√°micamente.

**Como una instancia MAPE-K** (Kephart & Chess 2003): `observe` = Monitorear, clasificaci√≥n de `DeliberationTrigger` = Analizar, deliberaci√≥n del LLM = Planificar, aplicaci√≥n de `CognitiveDecision` = Ejecutar, traza de `ReasoningEpisode` = Conocimiento. La novedad: todas las fases est√°n embebidas en el runtime del lenguaje, no superpuestas externamente.

```mermaid
graph LR
    subgraph "MAPE-K mapeado a AURA"
        M["Monitor<br/><code>observe()</code>"] --> A["Analyze<br/><code>DeliberationTrigger</code>"]
        A --> P["Plan<br/>LLM delibera"]
        P --> E["Execute<br/><code>CognitiveDecision</code>"]
        E -.-> M
        K["Knowledge<br/><code>ReasoningEpisode</code><br/><code>HealingMemory</code>"]
        K <-.-> M
        K <-.-> A
        K <-.-> P
        K <-.-> E
    end
```

**Como un sistema generalizado de condiciones/restarts**: las condiciones de Common Lisp se√±alan errores; los restarts proporcionan opciones de recuperaci√≥n. AURA generaliza ambos: cualquier evento de ejecuci√≥n (no solo errores) puede disparar deliberaci√≥n, y las opciones de recuperaci√≥n son generadas din√°micamente por un LLM en lugar de ser predefinidas por el programador.

### 6.2 El runtime como arquitectura cognitiva

Al mapearse a la teor√≠a de arquitecturas cognitivas, el runtime de AURA implementa los componentes esenciales identificados por Newell (1990) y arquitecturas subsiguientes:

- **Percepci√≥n**: detecci√≥n de eventos v√≠a `observe()`
- **Memoria de trabajo**: buffer de observaciones + contexto de ejecuci√≥n
- **Memoria a largo plazo**: `HealingMemory` con persistencia de `ReasoningEpisode`
- **Deliberaci√≥n**: invocaci√≥n del LLM con contexto estructurado
- **Selecci√≥n de acci√≥n**: enum `CognitiveDecision`
- **Aprendizaje**: el historial de episodios informa deliberaciones subsiguientes
- **Metacognici√≥n**: l√≠mites de `CognitiveSafetyConfig` sobre el comportamiento de razonamiento

Esto convierte a AURA, hasta donde sabemos, en **el primer runtime de lenguaje de programaci√≥n que es en s√≠ mismo una arquitectura cognitiva**---en lugar de un lenguaje usado para implementar una.

### 6.3 Limitaciones

**Latencia.** La deliberaci√≥n con LLM a√±ade segundos de latencia por invocaci√≥n. AURA mitiga esto a trav√©s del `NullCognitiveRuntime` (cero overhead cuando las caracter√≠sticas cognitivas est√°n inactivas) y observaciones agrupadas, pero las aplicaciones en tiempo real pueden necesitar l√≠mites de latencia m√°s estrictos.

**Determinismo.** Las respuestas del LLM son no determin√≠sticas. Dos ejecuciones del mismo programa pueden seguir caminos diferentes. AURA registra la traza de `ReasoningEpisode` para an√°lisis de reproducibilidad, pero las garant√≠as formales requieren trabajo futuro.

**Correcci√≥n de fixes generados por LLM.** La funci√≥n `validate_fix()` verifica sintaxis, preservaci√≥n de goals, y tama√±o---pero no correcci√≥n sem√°ntica. Un fix que parsea correctamente y preserva goals a√∫n puede introducir errores. La suite de tests proporciona validaci√≥n adicional, pero la verificaci√≥n formal de parches generados por LLM sigue siendo un problema de investigaci√≥n abierto.

**Costo.** Cada deliberaci√≥n incurre en costos de API del LLM. Los l√≠mites `max_deliberations` y `max_deliberations_without_progress` acotan esto, pero las estrategias de deliberaci√≥n conscientes del costo son trabajo futuro.

### 6.4 Direcciones futuras

- **Sem√°ntica formal**: definir el ciclo cognitivo de AURA en un framework de sem√°ntica operacional, construyendo sobre la literatura de efectos algebraicos.
- **Runtime cognitivo multi-agente**: m√∫ltiples LLMs con diferentes especializaciones (ej., uno para reparaci√≥n de c√≥digo, otro para razonamiento arquitect√≥nico).
- **Adaptaci√≥n verificada**: usar m√©todos formales para demostrar que las adaptaciones dentro del espacio acotado por invariantes preservan propiedades especificadas.
- **Deliberaci√≥n consciente del costo**: estrategias que balanceen el costo de llamadas al LLM contra el beneficio esperado.
- **Cognici√≥n colaborativa**: modos humano-en-el-ciclo donde el runtime presenta opciones en lugar de actuar aut√≥nomamente.

---

## 7. Conclusi√≥n

AURA demuestra que la deliberaci√≥n cognitiva puede incorporarse en la sem√°ntica de lenguajes de programaci√≥n, creando una nueva categor√≠a de dise√±o de lenguajes. Al sintetizar conceptos de programaci√≥n orientada a agentes (goals BDI), sistemas auto-adaptativos (monitoreo MAPE-K), y reparaci√≥n de programas basada en LLM (correcci√≥n conversacional) en construcciones de primera clase del lenguaje con reglas de evaluaci√≥n definidas, AURA abre un espacio de dise√±o que, hasta donde sabemos, no ha sido explorado en la literatura publicada.

La idea clave es arquitect√≥nica: el LLM no es una herramienta externa que procesa archivos fuente, sino un componente del runtime que participa en la ejecuci√≥n---observando cambios en variables, evaluando goals, y tomando decisiones que se aplican inmediatamente dentro del flujo de ejecuci√≥n. Los invariantes y goals declarados por el desarrollador restringen el espacio de adaptaci√≥n, creando un contrato formalmente acotado entre la intenci√≥n humana y el razonamiento de la m√°quina.

Los 244 tests de AURA, la implementaci√≥n completa en Rust, y el modo de ejecuci√≥n cognitiva funcional demuestran que este dise√±o no es meramente te√≥rico sino pr√°cticamente realizable. El `NullCognitiveRuntime` asegura cero overhead para programas no cognitivos, haciendo que las capacidades cognitivas sean puramente aditivas.

Si este paradigma escala a sistemas de producci√≥n, c√≥mo pueden establecerse garant√≠as formales de correcci√≥n para adaptaciones generadas por LLM, y qu√© patrones de experiencia de desarrollador emergen cuando los programas pueden razonar sobre su propia ejecuci√≥n---estas son preguntas que la existencia de AURA hace concretas y tratables.

---

## Referencias

### Lenguajes de programaci√≥n orientados a agentes

[1] Shoham, Y. (1993). "Agent-Oriented Programming." *Artificial Intelligence*, 60(1):51-92.

[2] Rao, A.S. (1996). "AgentSpeak(L): BDI Agents Speak Out in a Logical Computable Language." *MAAMAW'96*, LNCS 1038, Springer, 42-55.

[3] Bordini, R.H., Hubner, J.F., & Wooldridge, M. (2007). *Programming Multi-Agent Systems in AgentSpeak using Jason*. Wiley.

[4] Hindriks, K.V. (2009). "Programming Rational Agents in GOAL." In *Multi-Agent Programming*, Springer, 119-157.

[5] Dastani, M. (2008). "2APL: A Practical Agent Programming Language." *Autonomous Agents and Multi-Agent Systems*, 16(3):214-248.

[6] Rodriguez, S., Gaud, N., & Galland, S. (2014). "SARL: A General-Purpose Agent-Oriented Programming Language." *WI-IAT 2014*, IEEE/WIC/ACM.

[7] Pokahr, A., Braubach, L., & Lamersdorf, W. (2005). "Jadex: A BDI Reasoning Engine." In *Multi-Agent Programming*, Springer, 149-174.

### Teor√≠a BDI

[8] Bratman, M.E. (1987). *Intention, Plans, and Practical Reason*. Harvard University Press.

[9] Rao, A.S. & Georgeff, M.P. (1991). "Modeling Rational Agents within a BDI-Architecture." *KR'91*, Morgan Kaufmann, 473-484.

[10] Rao, A.S. & Georgeff, M.P. (1995). "BDI Agents: From Theory to Practice." *ICMAS'95*, AAAI Press, 312-319.

[11] Sardina, S. & Padgham, L. (2011). "A BDI Agent Programming Language with Failure Handling, Declarative Goals, and Planning." *Autonomous Agents and Multi-Agent Systems*, 23(1):18-70.

[12] Cohen, P.R. & Levesque, H.J. (1990). "Intention is Choice with Commitment." *Artificial Intelligence*, 42(2-3):213-261.

### Reparaci√≥n autom√°tica de programas

[13] Le Goues, C., Nguyen, T.V., Forrest, S., & Weimer, W. (2012). "GenProg: A Generic Method for Automatic Software Repair." *IEEE TSE*, 38(1):54-72.

[14] Nguyen, H.D.T., Qi, D., Roychoudhury, A., & Chandra, S. (2013). "SemFix: Program Repair via Semantic Analysis." *ICSE 2013*, 772-781.

[15] Mechtaev, S., Yi, J., & Roychoudhury, A. (2016). "Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis." *ICSE 2016*, 1071-1082.

[16] Long, F. & Rinard, M. (2016). "Automatic Patch Generation by Learning Correct Code." *POPL 2016*, 298-312.

[17] Xia, C.S. & Zhang, L. (2022). "Less Training, More Repairing Please: Revisiting Automated Program Repair via Zero-Shot Learning." *ESEC/FSE 2022*, 959-971.

[18] Xia, C.S. & Zhang, L. (2023). "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT." *ISSTA 2024*. arXiv:2304.00385.

[19] Monperrus, M. (2018). "Automatic Software Repair: A Bibliography." *ACM Computing Surveys*, 51(1):1-24.

### Sistemas auto-adaptativos

[20] Kephart, J.O. & Chess, D.M. (2003). "The Vision of Autonomic Computing." *IEEE Computer*, 36(1):41-50.

[21] Garlan, D., Cheng, S.-W., Huang, A.-C., Schmerl, B., & Steenkiste, P. (2004). "Rainbow: Architecture-Based Self-Adaptation with Reusable Infrastructure." *IEEE Computer*, 37(10):46-54.

[22] Weyns, D., Malek, S., & Andersson, J. (2012). "FORMS: Unifying Reference Model for Formal Specification of Distributed Self-Adaptive Systems." *ACM TAAS*, 7(1).

[23] Weyns, D. (2020). *An Introduction to Self-Adaptive Systems: A Contemporary Software Engineering Perspective*. Wiley/IEEE Press.

### Arquitecturas cognitivas

[24] Laird, J.E., Newell, A., & Rosenbloom, P.S. (1987). "SOAR: An Architecture for General Intelligence." *Artificial Intelligence*, 33(1):1-64.

[25] Newell, A. (1990). *Unified Theories of Cognition*. Harvard University Press.

[26] Anderson, J.R. & Lebiere, C. (1998). *The Atomic Components of Thought*. Lawrence Erlbaum Associates.

[27] Anderson, J.R. et al. (2004). "An Integrated Theory of the Mind." *Psychological Review*, 111(4):1036-1060.

[28] Sun, R. (2016). *Anatomy of the Mind: Exploring Psychological Mechanisms and Processes with the Clarion Cognitive Architecture*. Oxford University Press.

[29] Franklin, S. et al. (2014). "LIDA: A Systems-level Architecture for Cognition, Emotion, and Learning." *IEEE Trans. on Autonomous Mental Development*, 6(1):19-41.

### Reflexi√≥n, efectos y meta-programaci√≥n

[30] Smith, B.C. (1984). "Reflection and Semantics in Lisp." *POPL '84*, ACM, 23-35.

[31] Kiczales, G., des Rivieres, J., & Bobrow, D.G. (1991). *The Art of the Metaobject Protocol*. MIT Press.

[32] Kiczales, G. et al. (1997). "Aspect-Oriented Programming." *ECOOP '97*, LNCS 1241, Springer, 220-242.

[33] Plotkin, G.D. & Pretnar, M. (2009). "Handlers of Algebraic Effects." *ESOP 2009*, LNCS 5502, Springer, 80-94.

[34] Bauer, A. & Pretnar, M. (2015). "Programming with Algebraic Effects and Handlers." *Journal of Logical and Algebraic Methods in Programming*, 84(1):108-123.

### Checkpoint, rollback y tolerancia a fallos

[35] Shavit, N. & Touitou, D. (1995). "Software Transactional Memory." *PODC '95*, ACM, 204-213.

[36] Harris, T., Marlow, S., Peyton Jones, S., & Herlihy, M. (2005). "Composable Memory Transactions." *PPoPP '05*, ACM, 48-60.

[37] Armstrong, J. (2003). *Making Reliable Distributed Systems in the Presence of Software Errors*. PhD Thesis, Royal Institute of Technology, Stockholm.

[38] Rinard, M. et al. (2004). "Enhancing Server Availability and Security Through Failure-Oblivious Computing." *OSDI 2004*, USENIX, 303-316.

[39] Perkins, J.H. et al. (2009). "Automatically Patching Errors in Deployed Software." *SOSP 2009*, ACM, 87-102.

### Programaci√≥n integrada con LLM

[40] Beurer-Kellner, L., Fischer, M., & Vechev, M. (2023). "Prompting Is Programming: A Query Language for Large Language Models." *PLDI 2023*, ACM, 1507-1532.

[41] Khattab, O. et al. (2023). "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines." arXiv:2310.03714. *ICLR 2024*.

[42] Zheng, L. et al. (2024). "SGLang: Efficient Execution of Structured Language Model Programs." arXiv:2312.07104.

[43] Yao, S. et al. (2023). "ReAct: Synergizing Reasoning and Acting in Language Models." *ICLR 2023*.

[44] Shinn, N. et al. (2023). "Reflexion: Language Agents with Verbal Reinforcement Learning." *NeurIPS 2023*.

### Programaci√≥n orientada a objetivos y planificaci√≥n

[45] Fikes, R.E. & Nilsson, N.J. (1971). "STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving." *Artificial Intelligence*, 2(3-4):189-208.

[46] Nilsson, N.J. (1994). "Teleo-Reactive Programs for Agent Control." *JAIR*, 1:139-158.

[47] Nau, D. et al. (2003). "SHOP2: An HTN Planning System." *JAIR*, 20:379-404.

### Verificaci√≥n en runtime y dise√±o por contrato

[48] Meyer, B. (1992). "Applying 'Design by Contract'." *IEEE Computer*, 25(10):40-51.

[49] Leucker, M. & Schallhart, C. (2009). "A Brief Account of Runtime Verification." *Journal of Logic and Algebraic Programming*, 78(5):293-303.

[50] Ernst, M.D. et al. (2007). "The Daikon System for Dynamic Detection of Likely Invariants." *Science of Computer Programming*, 69(1-3):35-45.

### Surveys y trabajo fundacional

[51] Wooldridge, M. & Jennings, N.R. (1995). "Intelligent Agents: Theory and Practice." *Knowledge Engineering Review*, 10(2):115-152.

[52] Wang, L. et al. (2024). "A Survey on Large Language Model Based Autonomous Agents." *Frontiers of Computer Science*.

[53] Schmidhuber, J. (2003). "Goedel Machines: Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements." Technical Report IDSIA-19-03.

[54] Hicks, M. & Nettles, S. (2005). "Dynamic Software Updating." *ACM TOPLAS*, 27(6):1049-1096.

[55] Gat, E. (1998). "On Three-Layer Architectures." In *Artificial Intelligence and Mobile Robots*, MIT Press, 195-210.

---

*AURA est√° implementado en Rust con 244 tests. C√≥digo fuente disponible en el repositorio del proyecto.*
